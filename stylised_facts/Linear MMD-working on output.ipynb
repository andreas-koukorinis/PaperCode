{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4833769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import freqopttest.util as util\n",
    "import freqopttest.data as data\n",
    "import freqopttest.kernel as kernel\n",
    "import freqopttest.tst as tst\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "import freqopttest.glo as glo\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pickle\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfadffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24878600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_file(test_results_one_dict, test_results_two_dict, shift, window, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    test_results_one_file = os.path.join(output_dir, f\"test_results_one_shift_{shift}_window_{window}.pkl\")\n",
    "    test_results_two_file = os.path.join(output_dir, f\"test_results_two_shift_{shift}_window_{window}.pkl\")\n",
    "    \n",
    "    with open(test_results_one_file, 'wb') as f:\n",
    "        pickle.dump(test_results_one_dict, f)\n",
    "        \n",
    "    with open(test_results_two_file, 'wb') as f:\n",
    "        pickle.dump(test_results_two_dict, f)\n",
    "\n",
    "\n",
    "def load_results(output_dir):\n",
    "    results_files = [f for f in os.listdir(output_dir) if f.endswith('.csv')]\n",
    "    \n",
    "    results = []\n",
    "    for file in results_files:\n",
    "        filepath = os.path.join(output_dir, file)\n",
    "        df = pd.read_csv(filepath)\n",
    "        df['filename'] = file\n",
    "        results.append(df)\n",
    "    \n",
    "    return pd.concat(results, ignore_index=True)\n",
    "\n",
    "\n",
    "def generate_latex_summary_table(dataframe):\n",
    "    latex_table = dataframe.to_latex(index=False)\n",
    "    return latex_table\n",
    "\n",
    "def analyze_and_save(shift, window, analyzer, output_dir):\n",
    "    try:\n",
    "        test_results_one_dict, test_results_two_dict = analyzer.analyze(shift=shift, window=window)\n",
    "        save_results_to_file(test_results_one_dict, test_results_two_dict, shift, window, output_dir)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error occurred during analyze for shift {shift} and window {window}:\", e)\n",
    "        \n",
    "def load_and_concatenate_results_csv(output_dir):\n",
    "    output_dir = Path(output_dir)\n",
    "    csv_files = sorted(output_dir.glob(\"*.csv\"))\n",
    "    dfs = []\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        dfs.append(df)\n",
    "    concatenated_df = pd.concat(dfs, axis=0)\n",
    "    return concatenated_df\n",
    "\n",
    "def load_and_concatenate_results(output_dir):\n",
    "    output_dir = Path(output_dir)\n",
    "    pkl_files = sorted(output_dir.glob(\"*.pkl\"))\n",
    "    dfs = []\n",
    "    for pkl_file in pkl_files:\n",
    "        with open(pkl_file, 'rb') as f:\n",
    "            df = pickle.load(f)\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            dfs.append(df)\n",
    "    concatenated_df = pd.concat(dfs, axis=0)\n",
    "    return concatenated_df\n",
    "\n",
    "def load_and_concatenate_results_default_dict(output_dir):\n",
    "    output_dir = Path(output_dir)\n",
    "    pkl_files = sorted(output_dir.glob(\"*.pkl\"))\n",
    "    dfs = []\n",
    "    for pkl_file in pkl_files:\n",
    "        with open(pkl_file, 'rb') as f:\n",
    "            results_dict = pickle.load(f)\n",
    "        for key in results_dict:\n",
    "            if isinstance(key, int):\n",
    "                key = (key,)\n",
    "            results_dict[key][\"shift\"] = key[0]\n",
    "            results_dict[key][\"window\"] = key[1]\n",
    "        df = pd.DataFrame.from_dict(results_dict, orient=\"index\")\n",
    "        dfs.append(df)\n",
    "    concatenated_df = pd.concat(dfs, axis=0)\n",
    "    return concatenated_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eca10e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kgauss(sigma2, default_sigma2=None):\n",
    "    \"\"\"\n",
    "    Create a KGauss instance with the given sigma2 value, or use the default_sigma2 value if provided.\n",
    "\n",
    "    :param sigma2: float, the sigma2 value to use for creating the KGauss instance.\n",
    "    :param default_sigma2: float, optional, the default sigma2 value to use if the provided sigma2 is invalid.\n",
    "    :return: KGauss, the created KGauss instance.\n",
    "    :raise ValueError: if both sigma2 and default_sigma2 are invalid.\n",
    "    \"\"\"\n",
    "    if sigma2 > 0:\n",
    "        return kernel.KGauss(sigma2)\n",
    "    elif default_sigma2 is not None and default_sigma2 > 0:\n",
    "        print(\"Using default sigma2 value:\", default_sigma2)\n",
    "        return kernel.KGauss(default_sigma2)\n",
    "    else:\n",
    "        raise ValueError(\"Both sigma2 and default_sigma2 are invalid. Please provide a positive value for either.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac557a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LinearMMDAnalyzer:\n",
    "    \"\"\"\n",
    "    A class to analyze Linear MMD tests on symbol data and save the results.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, symbol='', mfdfaDataFrames='', \n",
    "                 LinearMMDOutputFiles='', bar_choice='', variable=''):\n",
    "        \"\"\"\n",
    "        Initialize the LinearMMDAnalyzer with symbol, input, and output paths.\n",
    "\n",
    "        :param symbol: str, the symbol to analyze.\n",
    "        :param mfdfaDataFrames: str, the base directory containing the symbol data.\n",
    "        :param LinearMMDOutputFiles: str, the base directory where the output files will be saved.\n",
    "        :param bar_choice: str, the bar choice to process (e.g., 'dollar').\n",
    "        :param variable: str, the variable to analyze.\n",
    "        \"\"\"\n",
    "        self.symbol = symbol\n",
    "        self.mfdfaDataFrames = mfdfaDataFrames\n",
    "        self.LinearMMDOutputFiles = LinearMMDOutputFiles\n",
    "        self.symbol_mfdfa_Frames_loc = os.path.join(self.mfdfaDataFrames, self.symbol)\n",
    "        self.bar_choice = bar_choice\n",
    "        self.variable = variable\n",
    "        self.symbol_mfdfa_Frames_results_bar_choice = os.path.join(self.symbol_mfdfa_Frames_loc,'results',self.bar_choice)\n",
    "        \n",
    "    def get_var_files(self):\n",
    "        return [f for f in os.listdir(self.symbol_mfdfa_Frames_results_bar_choice) if str(self.variable) in f]\n",
    "\n",
    "    def get_unpickled_file(self):\n",
    "        \"\"\"\n",
    "        the unpickled file containing the symbol/variable data.\n",
    "            :return: probably default dict.\n",
    "        \"\"\"\n",
    "        for file in list(self.get_var_files()):\n",
    "            return pd.read_pickle(os.path.join(self.symbol_mfdfa_Frames_results_bar_choice, file))\n",
    "        \n",
    "\n",
    "\n",
    "    def analyze(self, shift=1, window=5, kgauss_instance=None):\n",
    "        \"\"\"\n",
    "        Perform the Linear MMD analysis on the symbol data and return the results.\n",
    "\n",
    "        :param shift: int, the shift parameter for the analysis.\n",
    "        :param window: int, the window parameter for the analysis.\n",
    "        :param kgauss_instance: KGauss, an optional instance of the KGauss class to use for the kernel.\n",
    "        :return: tuple, a tuple containing the test results dictionaries.\n",
    "        \"\"\"\n",
    "        unpickled_Df = self.get_unpickled_file()\n",
    "        length = unpickled_Df.shape[1]\n",
    "        test_results_one_dict = defaultdict(dict)\n",
    "        test_results_two_dict = defaultdict(dict)\n",
    "        for start_point in range(0, (length - window - shift)):\n",
    "            end_point = start_point + shift\n",
    "            X = np.array(unpickled_Df.iloc[:, start_point:end_point])\n",
    "            Y = np.array(unpickled_Df.iloc[:, end_point + shift:end_point + 2 * shift])\n",
    "            Z = np.array(unpickled_Df.iloc[:, start_point + window:end_point + window])\n",
    "\n",
    "            data_sample = data.TSTData(X, Y)  # data to train the model\n",
    "            test_data_one = data_sample = data.TSTData(X, Z)\n",
    "            test_data_two = data_sample = data.TSTData(Y, Z)\n",
    "\n",
    "            tr, te = data_sample.split_tr_te(tr_proportion=0.9, seed=100)\n",
    "\n",
    "            # choose the best kernel that maximizes the test power\n",
    "            med = util.meddistance(tr.stack_xy())\n",
    "            widths = [(med * f) for f in 2.0 ** np.linspace(-1, 4, 25)]\n",
    "\n",
    "            if kgauss_instance is None:\n",
    "\n",
    "                list_kernels = [create_kgauss(w ** 2, default_sigma2=1) for w in widths]\n",
    "                \n",
    "            else:\n",
    "                list_kernels = [kgauss_instance for _ in range(len(widths))]\n",
    "            \n",
    "            besti, powers = tst.LinearMMDTest.grid_search_kernel(tr, list_kernels, alpha=0.01)\n",
    "\n",
    "            # The actual test\n",
    "            best_ker = list_kernels[besti]\n",
    "            lin_mmd_test = tst.LinearMMDTest(best_ker, alpha=0.01)\n",
    "\n",
    "            # Test 1 Results Dict\n",
    "            test_results_one_dict[start_point]['widths'] = widths\n",
    "            test_results_one_dict[start_point]['med'] = med\n",
    "            test_results_one_dict[start_point]['widths'] = widths\n",
    "            test_results_one_dict[start_point]['besti'] = besti\n",
    "            test_results_one_dict[start_point]['powers'] = powers\n",
    "\n",
    "            test_results_one_dict[start_point]['med_on_test_data'] = util.meddistance(test_data_one.stack_xy())\n",
    "            test_results_one_dict[start_point]['test_result'] = lin_mmd_test.perform_test(test_data_one)\n",
    "            test_results_one_dict[start_point]['test_variance'] = lin_mmd_test.variance(X, Z, best_ker)  # test variance\n",
    "            test_results_one_dict[start_point]['two_moments'] = lin_mmd_test.two_moments(X, Z, best_ker)  # test variance\n",
    "            test_results_one_dict[start_point]['compute_unbiased_linear_estimator'] = lin_mmd_test.compute_stat(\n",
    "                test_data_one)\n",
    "\n",
    "            # Test 2 Results Dict\n",
    "            test_results_two_dict[start_point]['test_result'] = lin_mmd_test.perform_test(test_data_two)\n",
    "            test_results_two_dict[start_point]['test_variance'] = lin_mmd_test.variance(Y, Z, best_ker)\n",
    "            test_results_two_dict[start_point]['med_on_test_data'] = util.meddistance(\n",
    "                test_data_two.stack_xy())  # test variance\n",
    "            test_results_two_dict[start_point]['two_moments'] = lin_mmd_test.two_moments(Y, Z, best_ker)  # test variance\n",
    "            test_results_two_dict[start_point]['compute_unbiased_linear_estimator'] = lin_mmd_test.compute_stat(\n",
    "                test_data_two)\n",
    "\n",
    "        return test_results_one_dict, test_results_two_dict\n",
    "    def analyze_adaptive_two(self, kgauss_instance=None):\n",
    "        unpickled_Df = self.get_unpickled_file()\n",
    "        num_shifts = int(unpickled_Df.shape[1] / 10)\n",
    "        num_windows = int(unpickled_Df.shape[1] / 20)\n",
    "\n",
    "        test_results_dicts = {}\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            future_to_pair = {}\n",
    "            for shift in range(1, num_shifts + 1):\n",
    "                for window in range(1, num_windows + 1):\n",
    "                    future = executor.submit(self.analyze, shift, window, kgauss_instance)\n",
    "                    future_to_pair[future] = (shift, window)\n",
    "\n",
    "            for future in concurrent.futures.as_completed(future_to_pair):\n",
    "                shift, window = future_to_pair[future]\n",
    "                try:\n",
    "                    test_results_one_dict, test_results_two_dict = future.result()\n",
    "                    self.save_results(test_results_one_dict, test_results_two_dict, shift, window)\n",
    "                    test_results_dicts[(shift, window)] = (test_results_one_dict, test_results_two_dict)\n",
    "                    \n",
    "                except ValueError:\n",
    "                    # Handle the error here, e.g. by skipping this pair or applying some other fallback option\n",
    "                    pass\n",
    "\n",
    "        return test_results_dicts\n",
    "    def analyze_adaptive(self, kgauss_instance=None):\n",
    "        \"\"\"\n",
    "        Perform the Linear MMD analysis on the symbol data with adaptive shifts and windows.\n",
    "\n",
    "        :param kgauss_instance: KGauss, an optional instance of the KGauss class to use for the kernel.\n",
    "        :return: tuple, a tuple containing the test results dictionaries.\n",
    "        \"\"\"\n",
    "        unpickled_Df = self.get_unpickled_file()\n",
    "        length = unpickled_Df.shape[1]\n",
    "        max_shift = length // 3\n",
    "        max_window = length // 3\n",
    "\n",
    "        shifts_windows = [(shift, window) for shift in range(1, max_shift + 1) for window in range(1, max_window + 1)]\n",
    "\n",
    "        test_results_dicts = []\n",
    "\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(self.analyze, shift, window, kgauss_instance) for shift, window in shifts_windows]\n",
    "\n",
    "            for future in futures:\n",
    "                test_results_one_dict, test_results_two_dict, shift, window = future.result()\n",
    "                self.save_results(test_results_one_dict, test_results_two_dict, shift, window)\n",
    "                test_results_dicts.append((test_results_one_dict, test_results_two_dict, shift, window))\n",
    "\n",
    "        self.test_results_dicts = test_results_dicts\n",
    "\n",
    "        # Create a unique filename based on the shifts_windows parameter\n",
    "        hash_str = str(shifts_windows).encode('utf-8')\n",
    "        filename = hashlib.sha256(hash_str).hexdigest()[:16] + \".pkl\"\n",
    "\n",
    "        # Save the test_results_dicts to a pickle file\n",
    "        with open(os.path.join(self.LinearMMDOutputFiles, filename), \"wb\") as f:\n",
    "            pickle.dump(test_results_dicts, f)\n",
    "        return test_results_dicts\n",
    "\n",
    "    def analyze_multiple_variables(self, variables, shift=1, window=5):\n",
    "        \"\"\"\n",
    "        Perform the Linear MMD analysis on multiple variables and return the results.\n",
    "\n",
    "        :param variables: list, a list of variables to analyze.\n",
    "        :param shift: int, the shift parameter for the analysis.\n",
    "        :param window: int, the window parameter for the analysis.\n",
    "        :return: list, a list of dictionaries containing the test results for each variable.\n",
    "        \"\"\"\n",
    "        results_list = []\n",
    "\n",
    "        for variable in variables:\n",
    "            self.variable = variable\n",
    "            test_results_one_dict, test_results_two_dict = self.analyze(shift=shift, window=window)\n",
    "            results_dict = {\n",
    "                'variable': variable,\n",
    "                'test_results_one_dict': test_results_one_dict,\n",
    "                'test_results_two_dict': test_results_two_dict\n",
    "            }\n",
    "            results_list.append(results_dict)\n",
    "\n",
    "        return results_list\n",
    "\n",
    "    def save_results(self, test_results_one_dict, test_results_two_dict, shift, window):\n",
    "        \"\"\"\n",
    "        Save the test results to pickle files.\n",
    "\n",
    "        :param test_results_one_dict: dict, the first test results dictionary.\n",
    "        :param test_results_two_dict: dict, the second test results dictionary.\n",
    "        :param shift: int, the shift parameter for the analysis.\n",
    "        :param window: int, the window parameter for the analysis.\n",
    "        \"\"\"\n",
    "        pickle_out_dict_one = os.path.join(self.LinearMMDOutputFiles, \"\".join(\n",
    "            (str(self.symbol) + \"_\" + str(self.variable) + \"_shift_\" + str(shift) + \"_wind_\" + str(window) + \"_\" + str(\n",
    "                'linear_test') + \"_ONE.pkl\")))\n",
    "        pickle.dump(test_results_one_dict, open(pickle_out_dict_one, 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        pickle_out_dict_two = os.path.join(self.LinearMMDOutputFiles, \"\".join(\n",
    "            (str(self.symbol) + \"_\" + str(self.variable) + \"_shift_\" + str(shift) + \"_wind_\" + str(window) + \"_\" + str(\n",
    "                'linear_test') + \"_TWO.pkl\")))\n",
    "        pickle.dump(test_results_two_dict, open(pickle_out_dict_two, 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de627155",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "experimentOne = '/media/ak/T71/August11th2022Experiments/experimentOne'\n",
    "mfdfaDataFrames = '/media/ak/T71/August11th2022Experiments/mfdfaDataFrames'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dcbc36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ak/T71/August11th2022Experiments/mfdfaDataFrames/TY1 True\n",
      "/media/ak/T71/August11th2022Experiments/mfdfaDataFrames/TY1/results/tick\n"
     ]
    }
   ],
   "source": [
    "symbol = 'TY1'\n",
    "symbolPath = os.path.join(mfdfaDataFrames, symbol)\n",
    "\n",
    "bar_choice = 'tick'\n",
    "variable ='alpha'\n",
    "print(symbolPath, os.path.isdir(symbolPath))\n",
    "\n",
    "\n",
    "symbol_barChoicePath = os.path.join(symbolPath, 'results', bar_choice)\n",
    "print(symbol_barChoicePath)\n",
    "\n",
    "files = [f for f in os.listdir(symbol_barChoicePath) if str(bar_choice) in f]\n",
    "variables = ['n_F', 'list_H', 'list_H_intercept', 'tau', 'alpha', 'mfSpect']\n",
    "\n",
    "LinearMMDOutputFiles = os.path.join(experimentOne, 'LinearMMDOutputFiles')\n",
    "if not os.path.exists(LinearMMDOutputFiles):\n",
    "    os.makedirs(LinearMMDOutputFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e218e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "analyzer = LinearMMDAnalyzer(symbol=symbol, mfdfaDataFrames=mfdfaDataFrames,\n",
    "                             LinearMMDOutputFiles=LinearMMDOutputFiles,\n",
    "                             bar_choice=bar_choice, variable=variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a67bbc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default sigma2 value: 1\n"
     ]
    }
   ],
   "source": [
    "gaussian_kernel = create_kgauss(sigma2=-1, default_sigma2=1)\n",
    "test_results_one_dict, test_results_two_dict = analyzer.analyze(kgauss_instance=gaussian_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8563d80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results_one_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8c42ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the defaultdict to a regular dictionary\n",
    "regular_dict = dict(test_results_one_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43174bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant data and create a DataFrame\n",
    "data = []\n",
    "for key, value in regular_dict.items():\n",
    "    row = [\n",
    "        key,\n",
    "        value[\"med\"],\n",
    "        value[\"med_on_test_data\"],\n",
    "        value[\"test_result\"][\"alpha\"],\n",
    "        value[\"test_result\"][\"pvalue\"],\n",
    "        value[\"test_result\"][\"test_stat\"],\n",
    "        value[\"test_result\"][\"h0_rejected\"],\n",
    "    ]\n",
    "    data.append(row)\n",
    "\n",
    "# Define column names\n",
    "columns = [\n",
    "    \"Key\",\n",
    "    \"Med\",\n",
    "    \"Med_on_test_data\",\n",
    "    \"Alpha\",\n",
    "    \"P-value\",\n",
    "    \"Test_stat\",\n",
    "    \"H0_rejected\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5290ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bd702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=plt.hist(df['Test_stat'], bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d233f6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.Test_stat.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da690df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df[\"H0_rejected\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54a33a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "defdict =pd.read_pickle('/media/ak/T71/August11th2022Experiments/experimentOne/LinearMMDOutputFiles/ebd8ad32be85c03e.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7f4ae87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([(1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (3, 5), (3, 6), (3, 7), (4, 7)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defdict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9d78ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:newTimeSeries] *",
   "language": "python",
   "name": "conda-env-newTimeSeries-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
