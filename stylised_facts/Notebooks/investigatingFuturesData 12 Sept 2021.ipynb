{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e377717",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stylised_facts_data_utilities'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ddd4b694bcdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/directory/tothe/handshakefile/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstylised_facts_data_utilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateLOB\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcreateLOB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stylised_facts_data_utilities'"
     ]
    }
   ],
   "source": [
    "import time \n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "from collections import defaultdict\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/ak/Documents/PaperCode/stylised_facts')\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "###\n",
    "\n",
    "# Plot settings\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "sb.set()\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/directory/tothe/handshakefile/')\n",
    "\n",
    "import stylised_facts_data_utilities.createLOB as createLOB\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "import scipy\n",
    "from scipy.stats import norm, ttest_ind\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6ac153",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pyinform as pyinf\n",
    "import pingouin as pig\n",
    "from scipy.stats import entropy\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.special import kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef73b53",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2499c23",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataFolder = '/media/ak/WorkDrive/Data'\n",
    "figures_destination = '/home/ak/Documents/Research/Papers/figures/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703c7a77",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "quotes =[f for f in os.listdir(dataFolder) if str('_quotes') in f]\n",
    "trades =[f for f in os.listdir(dataFolder) if str('_trades') in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c018f9aa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "symbols = ['VIX',  'FB1',  'TU1',  'G_1',  'RX1',  'OE1',  'TY1',  'FV1',\n",
    " 'JB1',  'RX1',  'DU1',  'KE1',  'US1',  'YM1', 'XM1',  'VXX'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf5fcb5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def quantile_plot(x, **kwargs):\n",
    "    quantiles , xr = stats.probplot(x, fit= True)\n",
    "    plt.scatter(xr, quantiles, **kwargs)\n",
    "def returns(s):\n",
    "    arr = np.diff(np.log(s))\n",
    "    return (pd.Series(arr, index=s.index[1:]))\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def plot_autocorr(bar_types,bar_returns):\n",
    "    f,axes=plt.subplots(len(bar_types),figsize=(20,14))\n",
    "\n",
    "    for i, (bar, typ) in enumerate(zip(bar_returns, bar_types)):\n",
    "        sm.graphics.tsa.plot_acf(bar, lags=120, ax=axes[i],\n",
    "                                 alpha=0.05, unbiased=True, fft=True,\n",
    "                                 zero=False,\n",
    "                                 title=f'{typ} AutoCorr')\n",
    "        \n",
    "    file_name = 'multiclocks_autocorrel.png'\n",
    "    plt.savefig(os.path.join(figures_destination,file_name))\n",
    "    plt.legend()\n",
    "    plt.tight_layout() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cb09fc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class volatilityEstimation(object):\n",
    "    \n",
    "    def __init__(self, df, tick_window=20, clean=True):\n",
    "        self.df = df\n",
    "        self.tick_window = tick_window\n",
    "        self.mu_1 =  np.sqrt((2 / np.pi))\n",
    "        self.mu_43 = 2**(2 / 3) * scipy.special.gamma(7 / 6) ** scipy.special.gamma(1 / 2) ** (-1)\n",
    "        self.clean = True #set to true to remove nans at the beginning of the period\n",
    "        self.trading_seconds_btc = 86400\n",
    "        self.trading_seconds_futures = 23400\n",
    "                      \n",
    "        self.column_open ='micro_price_open'\n",
    "        self.column_high = 'micro_price_high'\n",
    "        self.column_low = 'micro_price_low'\n",
    "        self.column_close = 'micro_price_close'\n",
    "\n",
    "        self.log_hl = (df[str(self.column_high)] / df[str(self.column_low)]).apply(np.log)\n",
    "        self.log_co = (df[str(self.column_close)] / df[str(self.column_open)]).apply(np.log)\n",
    "        # median sampling frequency\n",
    "        \n",
    "        self.z = pd.DataFrame([((x.hour * 60 + x.minute) * 60 + x.second) for x in self.df['TimeStamp_open']]).diff().fillna(0).astype('float64')\n",
    "        self.clean_arrival_rates = np.asarray(self.z[(self.z>self.z.quantile(0.003)) & ( self.z < self.z.quantile(0.97) ) ].dropna()).astype('float64')\n",
    "        # clean arrival rates after i remove all the outliers\n",
    "        self.mean_sampling_frequency = np.mean(np.asarray(self.clean_arrival_rates))\n",
    "        \n",
    " \n",
    "    def arrival_rates(self):\n",
    "        \n",
    "        z = pd.DataFrame([((x.hour * 60 + x.minute) * 60 + x.second) for x in self.df['TimeStamp_open']]).diff().fillna(0).astype('float64')\n",
    "        clean_arrival_rates = np.asarray(z[(z>z.quantile(0.003)) & (z<z.quantile(0.97)) ].dropna()).astype('float64')\n",
    "                                         # clean arrival rates after i remove all the outliers\n",
    "        \n",
    "        return clean_arrival_rates\n",
    "        \n",
    "    \n",
    "    def M_parameter(self):\n",
    "        \"\"\"\n",
    "        normalised sampling frequency for adjustments\n",
    "        \"\"\"\n",
    "       \n",
    "        M_btc = self.trading_seconds_btc / self.mean_sampling_frequency\n",
    "        M_futures = self.trading_seconds_futures / self.mean_sampling_frequency        \n",
    "        \n",
    "        return [M_btc, M_futures]\n",
    "        \n",
    "    \n",
    "    def garmanKlass(self, trading_periods):\n",
    "        \n",
    "        rs = 0.5 * self.log_hl ** 2 - (2 * np.log(2) - 1) * self.log_co ** 2\n",
    "\n",
    "        def f(v):\n",
    "            return (trading_periods * v.mean()) ** 0.5\n",
    "\n",
    "        result = rs.rolling(window=self.tick_window, center=False).apply(func=f)\n",
    "\n",
    "        if  self.clean:\n",
    "            return result.dropna()\n",
    "        else:\n",
    "            return result\n",
    "    \n",
    "    def parkinson(self, trading_periods, clean = True):\n",
    "\n",
    "        rs = (1.0 / (4.0 * np.log(2.0))) *(self.df[str(self.column_high)] / self.df[str(self.column_close)]).apply(np.log) ** 2.0\n",
    "\n",
    "        def f(v):\n",
    "            return trading_periods * v.mean() ** 0.5\n",
    "\n",
    "        result = rs.rolling(window=self.tick_window, center=False).apply(func=f)\n",
    "\n",
    "        if self.clean:\n",
    "            return result.dropna()\n",
    "        else:\n",
    "            return result\n",
    "    \n",
    "    def bipower_variation(self, rollingWindow=5):\n",
    "        '''\n",
    "        Bipower Variation (BV) is the sum of the product of absolute time series returns\n",
    "        :param column: price column\n",
    "        :return: returns bivariate variation\n",
    "        Barnhorf - Nielse & Shephard 2004 & 2006\n",
    "        '''\n",
    "        bv = self.mu_1 ** (-2) * ((self.log_hl.abs() *self.log_hl.shift(1).abs()).fillna(0).rolling(rollingWindow).sum())\n",
    "        \n",
    "        if self.clean:\n",
    "            return bv.dropna()\n",
    "        else:\n",
    "            return bv\n",
    "\n",
    "    \n",
    "    def tripower_quarticity(self, rollingWindow=5, sampling_param = 0):\n",
    "        \"\"\"\n",
    "        using M for BTC here - change sampling param to 1 for everything else\n",
    "        \"\"\"\n",
    "        tpq = self.M_parameter()[sampling_param] * self.mu_43 ** (-3) * (( self.log_hl.abs()**(4 /3) * self.log_hl.shift(1).abs() **(4 / 3) * self.log_hl.shift(2).abs() **(4 / 3) ).rolling(rollingWindow).sum().fillna(0))\n",
    "        \n",
    "        if self.clean:\n",
    "            \n",
    "            return tpq.dropna()\n",
    "        else:\n",
    "            return tpq\n",
    "\n",
    "    \n",
    "    def realised_variance(self, rollingWindow=5):\n",
    "        \"\"\"\n",
    "        realised variance - andersen & bollerselv -1998\n",
    "        \"\"\"\n",
    "\n",
    "        realvar = (self.log_hl **2).rolling(rollingWindow).sum().fillna(0)\n",
    "        \n",
    "        if self.clean:\n",
    "            return realvar.dropna()\n",
    "        else:\n",
    "            return realvar\n",
    "    \n",
    "    def realised_absolute_variation(self, rollingWindow=5, sampling_param = 0):\n",
    "        \"\"\"\n",
    "        realised absolute variation - Forsberg & Ghysels 2007\n",
    "        using M for BTC here - change sampling param to 1 for everything else\n",
    "        \"\"\"\n",
    "        \n",
    "        realabsovar = self.mu_1 **(-2) *self.M_parameter[sampling_param] **(-0.5) * self.log_hl.rolling(rollingWindow).sum().fillna(0)\n",
    "        \n",
    "        return realabsovar\n",
    "    \n",
    "    def realised_skewness_kurtosis(self, rollingWindow = 15, sampling_param = 0):\n",
    "        \n",
    "        \"\"\"\n",
    "        set the rolling window to 10-15\n",
    "        sampling param for BTC ~ 0\n",
    "        \"\"\"\n",
    "        \n",
    "        rm3 = (self.log_hl **3).rolling(rollingWindow).sum().fillna(0)\n",
    "        rm4 = (self.log_hl **4).rolling(rollingWindow).sum().fillna(0)\n",
    "        \n",
    "        rs = np.sqrt(self.M_parameter()[sampling_param]) * (rm3 / self.realised_variance(rollingWindow = rollingWindow)) **(3/2)\n",
    "        rk = self.M_parameter()[sampling_param] * (rm4 / self.realised_variance(rollingWindow = rollingWindow)) ** 2\n",
    "        \n",
    "        return rs, rk\n",
    "    \n",
    "    def jumps_test(self, rollingWindow, sampling_param = 0):\n",
    "        \"\"\"\n",
    "        Jump test by Huang and Tauchen 2005\n",
    "        adopted by repo: RealisedQuantities\n",
    "        \"\"\"\n",
    "        \n",
    "        j1 = (np.log(self.realised_variance(rollingWindow= rollingWindow)) - np.log(self.bipower_variation(rollingWindow = rollingWindow))) \n",
    "        j2 = (((self.mu_1 **-4) +2* (self.mu_1 **-2) -5 ) / \n",
    "              (self.M_parameter()[0]*self.tripower_quarticity(rollingWindow = rollingWindow, sampling_param=sampling_param)\n",
    "                *(self.bipower_variation(rollingWindow = rollingWindow)**-2)))\n",
    "        \n",
    "        j = j1 / (j2 ** 0.5)          \n",
    "        \n",
    "        \n",
    "        return (j.abs() >= stats.norm.ppf(0.995))*1 # can also look at 0.999)*\n",
    "    \n",
    "    def relative_jump_measure(self, rollingWindow, sampling_param = 0):\n",
    "\n",
    "        nominator = (self.realised_variance(rollingWindow= rollingWindow) \n",
    "                     - self.bipower_variation(rollingWindow = rollingWindow))\n",
    "        \n",
    "        measure_to_return = nominator / self.bipower_variation(rollingWindow = rollingWindow)\n",
    "        \n",
    "        return measure_to_return\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "    def negative_semivariance(self, x):\n",
    "        \"\"\"\n",
    "        input: returns\n",
    "        negative realised semivariance\n",
    "        Barndorff- Nielsen\n",
    "        \"\"\"\n",
    "        return x.apply(lambda x: (x**2))*(x<0).sum()\n",
    "\n",
    "    def positive_semivariance(self, x):\n",
    "        \"\"\"\n",
    "        input: returns\n",
    "        positive realised semivariance\n",
    "        Barndorff- Nielsen\n",
    "        \"\"\"\n",
    "        return x.apply(lambda x: (x**2))*(x>0).sum()\n",
    "    \n",
    "    def signed_jump_variation(self, x):\n",
    "        \n",
    "        sjv = self.positive_semivariance(x) -self.negative_semivariance(x)\n",
    "        sjv_p = sjv*(sjv>0)\n",
    "        sjv_n = sjv*(sjv<0)\n",
    "        \n",
    "        return sjv_p, sjv_n\n",
    "    \n",
    "    def continuous_discontinuous_quadratic_variance(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        seperate continuous and discontinuous parts of quadratic variation\n",
    "        \n",
    "        \"\"\"\n",
    "        pass                                                                                                                                 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class descriptiveStats(object):\n",
    "       \n",
    "        def __init__(self, df):\n",
    "        \n",
    "            self.df = df\n",
    "            self.column_open ='micro_price_open'\n",
    "            self.column_high = 'micro_price_high'\n",
    "            self.column_low = 'micro_price_low'\n",
    "            self.column_close = 'micro_price_close'\n",
    "            self.log_hl = (df[str(self.column_high)] / df[str(self.column_low)]).apply(np.log)\n",
    "            self.log_co = (df[str(self.column_close)] / df[str(self.column_open)]).apply(np.log)\n",
    "        \n",
    "        def base_descriptive_stats(self):\n",
    "\n",
    "            desc_stats =dict()\n",
    "            desc_stats['median_hl'] = np.median(self.log_hl)\n",
    "            desc_stats['mean_co'] = np.median(self.log_co)\n",
    "            desc_stats['quantile_95_hl'] = self.log_hl.quantile(0.95)\n",
    "            desc_stats['quantile_95_co'] = self.log_co.quantile(0.95)\n",
    "            desc_stats['q3_75_hl'] = self.log_hl.quantile(0.75)\n",
    "            desc_stats['q3_75_co'] = self.log_co.quantile(0.75)\n",
    "            desc_stats['q1_25_hl'] = self.log_hl.quantile(0.25)\n",
    "            desc_stats['q1_25_co'] = self.log_co.quantile(0.25)\n",
    "            desc_stats['outlier_lower_limit_hl'] = desc_stats['q1_25_hl'] - 1.5*(desc_stats['q3_75_hl'] - desc_stats['q1_25_hl'])\n",
    "            desc_stats['outlier_upper_limit_hl'] = desc_stats['q3_75_hl'] + 1.5*(desc_stats['q3_75_hl'] - desc_stats['q1_25_hl'])\n",
    "            desc_stats['lower_limit_outliers'] = self.log_hl[self.log_hl > desc_stats['outlier_lower_limit_hl']].count()\n",
    "            desc_stats['upper_limit_outliers'] = self.log_hl[self.log_hl > desc_stats['outlier_upper_limit_hl']].count()\n",
    "            desc_stats['total_outliers'] = desc_stats['lower_limit_outliers'] + desc_stats['upper_limit_outliers']\n",
    "            \n",
    "            return desc_stats\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ee4019",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    loads data and constructs full LOB and calendar bars for each one\n",
    "    \"\"\"\n",
    "    def __init__(self, data_location, \n",
    "                 symbol,\n",
    "                 dates, \n",
    "                 use_columns, \n",
    "                 calendar_resample,\n",
    "                 trade_volume_width, \n",
    "                 ticks_width, \n",
    "                 usd_volume_width):\n",
    "        \n",
    "        self._data_location = data_location\n",
    "        self._symbol= symbol\n",
    "        self._input_dates = dates\n",
    "        self._use_columns = use_columns\n",
    "        self._calendar_resample = calendar_resample\n",
    "        self._ticks_width = ticks_width\n",
    "        self._trade_volume_width = trade_volume_width\n",
    "        self._usd_volume_width = usd_volume_width\n",
    "        self._dates = []\n",
    "        self._bars_dict = {}\n",
    "        self.calendar_bars = []\n",
    "        self.volume_bars = []\n",
    "        self.tick_bars = []\n",
    "        self.usd_volume_bars = []\n",
    "        self._logger = logger('DataLoader')\n",
    "#         self._symbols = sorted(['VIX',  'FB1',  'TU1',  'G_1',  'RX1',  'OE1',  'TY1',  'FV1',  'JB1',  'RX1',  'DU1',  'KE1',  'US1',  'YM1', 'XM1',  'VXX'] )\n",
    "        self._data_folder = data_location\n",
    "        \n",
    "        self._quotes_string = \"\".join((self._symbol, '_Comdty_quotes' ))\n",
    "        self._trades_string = \"\".join((self._symbol, '_Comdty_trades' ))\n",
    "        \n",
    "#         self._quotes_files =sorted([ f for f in os.listdir(self._data_folder) if str('_quotes') in f])\n",
    "#         self._trades_files = sorted([f for f in os.listdir(self._data_folder) if str('_trades') in f])\n",
    "        \n",
    "        self._quotes_Files = os.path.join(self._data_folder, self._quotes_string)\n",
    "        self._trades_Files = os.path.join(self._data_folder, self._trades_string)\n",
    "    \n",
    "    def load_and_format_data(self):\n",
    "        mergedFile = dict()\n",
    "        for date in self._input_dates:\n",
    "            dateToLoad = os.path.join(date + '.csv')\n",
    "            self._logger.info(f\"Loading data for:{date}\")\n",
    "            \n",
    "            quotesDateFile = os.path.join(self._quotes_Files, dateToLoad)\n",
    "            print(quotesDateFile)\n",
    "            tradesDateFile = os.path.join(self._trades_Files, dateToLoad)\n",
    "            print(tradesDateFile)\n",
    "            \n",
    "            try:\n",
    "                qt_tmp_df = pd.read_csv(quotesDateFile, usecols=self._use_columns)\n",
    "                qt_tmp_df['TradeTime'] = pd.to_datetime(qt_tmp_df.time).values\n",
    "                              \n",
    "                quotes_df =pd.merge_asof(qt_tmp_df[qt_tmp_df['type'] =='BID'].dropna().fillna(\"ffill\").sort_values('TradeTime')\n",
    "                                         , qt_tmp_df[qt_tmp_df['type'] =='ASK'].dropna().fillna(\"ffill\").sort_values('TradeTime'), \n",
    "                                         on='TradeTime', allow_exact_matches=True)\n",
    "                \n",
    "                quotes_df = quotes_df.rename( columns={'value_x': 'BestBid', 'value_y': 'BestAsk',\n",
    "                 'size_x': 'BidSize', 'size_y': 'AskSize', 'time_x': 'QuoteTime' })\n",
    "                              \n",
    "                tr_tmp_df = pd.read_csv(tradesDateFile, usecols=self._use_columns)\n",
    "                tr_tmp_df['TradeTime'] = pd.to_datetime(tr_tmp_df.time).values\n",
    "                trades_df= tr_tmp_df.dropna().fillna(\"ffill\").sort_values('TradeTime')\n",
    "                trades_df = trades_df.rename( columns={'value': 'TradePrice', \n",
    "                                                       'size': 'TradeSize'})\n",
    "                trades_columns = ['TradeSize', 'type','TradePrice','TradeTime']\n",
    "                \n",
    "                LOB =pd.merge_asof(quotes_df, trades_df[trades_columns].sort_values('TradeTime'), on='TradeTime', allow_exact_matches=True)\n",
    "                \n",
    "                LOB.BidSize = LOB['BidSize'].replace(0, 1)\n",
    "                LOB.AskSize = LOB['AskSize'].replace(0, 1)\n",
    "                LOB['TimeStamp'] = pd.to_datetime(LOB.TradeTime).dt.time\n",
    "                LOB['TradeVolume'] = LOB['TradeSize'].fillna(0)\n",
    "                LOB['total_traded_volume'] = LOB.TradeVolume\n",
    "                LOB['milliSeconds'] = [(((x.hour * 60 + x.minute) * 60 + x.second) * 1000) for x in LOB['TimeStamp']]\n",
    "                LOB['dollar_traded_volume'] = pd.Series(LOB.TradePrice * LOB.TradeVolume).fillna(0)\n",
    "                LOB['timeStampIdx'] = pd.DatetimeIndex(LOB.time_y)\n",
    "                LOB['micro_price'] = (LOB.BestAsk * LOB.AskSize + LOB.BestBid * LOB.BidSize) / (LOB.AskSize + LOB.BidSize)  #\n",
    "                \n",
    "                mergedFile[date] = LOB\n",
    "                \n",
    "            except FileNotFoundError:\n",
    "                self._logger.info(f\"Data for day {date} does not exist. Skipping this day\")\n",
    "        return mergedFile\n",
    "    \n",
    "    def apply_micro_structure_features(self, df):\n",
    "        \n",
    "        self._logger.info(\"Applying micro-structure features\")\n",
    "        \n",
    "     \n",
    "        df['weighted_average_bid_price'] = pd.DataFrame(df.filter(like='bid_price', axis=1).values).mul(\n",
    "        pd.DataFrame(df.filter(like='bid_size', axis=1).values)).sum(axis=1) / pd.DataFrame(df.filter(like='bid_size', axis=1).values).sum(axis=1)\n",
    "        \n",
    "        df['weighted_average_ask_price'] = pd.DataFrame(df.filter(like='ask_price', axis=1).values).mul(\n",
    "            pd.DataFrame(df.filter(like='ask_size', axis=1).values)).sum(\n",
    "            axis=1) / pd.DataFrame(df.filter(like='ask_size', axis=1).values).sum(axis=1)\n",
    "        df['weighted_activity_spread'] = df['weighted_average_ask_price'] - df['weighted_average_bid_price']\n",
    "        df['total_size'] = (pd.DataFrame(df.filter(like='ask_size', axis=1).values).sum(axis=1) +\n",
    "                            pd.DataFrame(df.filter(like='bid_size', axis=1).values).sum(axis=1))\n",
    "        df['micro_price'] = ((pd.DataFrame(df.filter(like='bid_price', axis=1).values).mul(\n",
    "            pd.DataFrame(df.filter(like='bid_size', axis=1).values)).sum(axis=1) +\n",
    "                              pd.DataFrame(df.filter(like='ask_price', axis=1).values).mul(\n",
    "                                  pd.DataFrame(df.filter(like='ask_size', axis=1).values)).sum(axis=1))) / df['total_size']\n",
    "        df['price_imbalance'] = ((pd.DataFrame(df.filter(like='ask_price', axis=1).values).mul(\n",
    "            pd.DataFrame(df.filter(like='ask_size', axis=1).values)).sum(axis=1) -\n",
    "                                  pd.DataFrame(df.filter(like='bid_price', axis=1).values).mul(\n",
    "                                      pd.DataFrame(df.filter(like='bid_size', axis=1).values)).sum(axis=1))) / df[\n",
    "                                    'total_size']\n",
    "        df['pct_change_micro_price'] = df.micro_price_close.pct_change()\n",
    "        df['simple_mid_price'] = 0.5 * (pd.DataFrame(df.filter(like='ask_price', axis=1)).mean(axis=1) +\n",
    "                                        pd.DataFrame(df.filter(like='bid_price', axis=1)).mean(axis=1))\n",
    "\n",
    "        # TODO: do we want to be using pct_change here or log returns?\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def prep_bars(df, drop_col):\n",
    "        df.columns = [f\"{i[0]}_{j}\" for i, j in zip(df.columns, ['open', 'high', 'low', 'close'] * len(df.columns))]\n",
    "        if drop_col:\n",
    "            df.drop([f\"{drop_col}_{i}\" for i in ['open', 'high', 'low', 'close']], axis=1, inplace=True)\n",
    "        return df\n",
    "\n",
    "    \n",
    "    def drop_lob_columns(df):\n",
    "        #not using this anywhere at the moment\n",
    "        \n",
    "        ask_price_columns_to_be_dropped = list(pd.DataFrame(df.filter(like='ask_price_', axis=1)).columns.values)\n",
    "        ask_size_columns_to_be_dropped = list(pd.DataFrame(df.filter(like='ask_size_', axis=1)).columns.values)\n",
    "        bid_size_columns_to_be_dropped = list(pd.DataFrame(df.filter(like='bid_size_', axis=1)).columns.values)\n",
    "        bid_price_columns_to_be_dropped = list(pd.DataFrame(df.filter(like='bid_price_', axis=1)).columns.values)\n",
    "        time_stamps = ['timestamp', 'timestamp.1', 'exchange_time', 'feed_time']\n",
    "        columns_to_be_dropped = ask_price_columns_to_be_dropped + ask_size_columns_to_be_dropped + \\\n",
    "                                bid_size_columns_to_be_dropped + bid_price_columns_to_be_dropped + \\\n",
    "                                time_stamps\n",
    "        df = df.drop(columns=columns_to_be_dropped)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def get_bars(self, df):\n",
    "        \n",
    "        self._logger.info('Calculating calendar clock')\n",
    "        calendar_bars = df.resample(rule=self._calendar_resample, on='timeStampIdx').agg(['first', 'max', 'min', 'last'])\n",
    "        calendar_bars = self.prep_bars(calendar_bars, 'timeStampIdx')\n",
    "\n",
    "        self._logger.info('Calculating volume clock')\n",
    "        # TODO: Doing this on day-by-day basis. We lose the last turn of the clock each day\n",
    "        df['cum_total_trade_volume'] = df.total_traded_volume.cumsum()\n",
    "        df['volume_bucket'] = np.floor(df.cum_total_trade_volume / self._trade_volume_width)\n",
    "        volume_bars = df.groupby(by='volume_bucket').agg(['first', 'max', 'min', 'last']).iloc[1:-1]\n",
    "        volume_bars = self.prep_bars(volume_bars, 'cum_total_trade_volume')\n",
    "        df.drop(['cum_total_trade_volume', 'volume_bucket'], axis=1, inplace=True)\n",
    "\n",
    "        self._logger.info('Calculating tick clock')\n",
    "        # TODO: ticks here means any change to the order book. Not necessarily a trade. Can add that clock too\n",
    "        df['tick_count'] = range(len(df))\n",
    "        df['tick_bucket'] = np.floor(df.tick_count / self._ticks_width)\n",
    "        tick_bars = df.groupby(by='tick_bucket').agg(['first', 'max', 'min', 'last']).iloc[1:-1]\n",
    "        tick_bars = self.prep_bars(tick_bars, 'tick_count')\n",
    "        df.drop(['tick_count', 'tick_bucket'], axis=1, inplace=True)\n",
    "\n",
    "        self._logger.info('Calculating usd volume clock')\n",
    "        df['cum_usd_volume'] = df.dollar_traded_volume.cumsum()\n",
    "        df['usd_volume_bucket'] = np.floor(df.cum_usd_volume / self._usd_volume_width)\n",
    "        usd_volume_bars = df.groupby(by='usd_volume_bucket').agg(['first', 'max', 'min', 'last']).iloc[1:-1]\n",
    "        usd_volume_bars = self.prep_bars(usd_volume_bars, 'cum_usd_volume')\n",
    "        df.drop(['cum_usd_volume', 'usd_volume_bucket'], axis=1, inplace=True)\n",
    "\n",
    "        self.calendar_bars.append(calendar_bars)\n",
    "        self.volume_bars.append(volume_bars)\n",
    "        self.tick_bars.append(tick_bars)\n",
    "        self.usd_volume_bars.append(usd_volume_bars)\n",
    "\n",
    "        self._bars_dict['calendar_bars'] = self.calendar_bars\n",
    "        self._bars_dict['volume_bars'] = self.volume_bars\n",
    "        self._bars_dict['tick_bars'] = self.tick_bars\n",
    "        self._bars_dict['usd_volume_bars'] = self.usd_volume_bars\n",
    "    \n",
    "    def get_all_dates_bars(self, input_dict):\n",
    "        output_dict = {}\n",
    "        for input_date in self._input_dates:\n",
    "            output_dict[input_date] = self.get_bars(input_dict[input_date])\n",
    "        \n",
    "        return output_dict           \n",
    "                \n",
    "    @staticmethod\n",
    "    def get_concat_data(input_dict):\n",
    "        concat_dict = dict()\n",
    "        for bar in input_dict.keys():\n",
    "            \n",
    "            concat_dict[bar] = pd.concat([input_dict[bar][i] \n",
    "                                                            for i in range(len(input_dict[bar]))], ignore_index=False)\n",
    "        return concat_dict\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892f5836",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def quantile_plot(x, **kwargs):\n",
    "    quantiles , xr = stats.probplot(x, fit= True)\n",
    "    plt.scatter(xr, quantiles, **kwargs)\n",
    "def returns(s):\n",
    "    arr = np.diff(np.log(s))\n",
    "    return (pd.Series(arr, index=s.index[1:]))\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def plot_autocorr(bar_types,bar_returns):\n",
    "    f,axes=plt.subplots(len(bar_types),figsize=(20,14))\n",
    "\n",
    "    for i, (bar, typ) in enumerate(zip(bar_returns, bar_types)):\n",
    "        sm.graphics.tsa.plot_acf(bar, lags=120, ax=axes[i],\n",
    "                                 alpha=0.05, unbiased=True, fft=True,\n",
    "                                 zero=False,\n",
    "                                 title=f'{typ} AutoCorr')\n",
    "        \n",
    "    file_name = 'multiclocks_autocorrel.png'\n",
    "    plt.savefig(os.path.join(figures_destination,file_name))\n",
    "    plt.legend()\n",
    "    plt.tight_layout()   \n",
    "    \n",
    "def plot_hist(bar_types,bar_rets):\n",
    "    \n",
    "    f,axes=plt.subplots(len(bar_types),figsize=(20,12))\n",
    "    for i, (bar, typ) in enumerate(zip(bar_returns, bar_types)):\n",
    "        g = sns.distplot(bar, ax=axes[i], kde=False, label=typ)\n",
    "        g.set(yscale='log')\n",
    "        axes[i].legend()\n",
    "    file_name = 'multiclocks_histogram.png'\n",
    "    plt.savefig(os.path.join(figures_destination,file_name))\n",
    "    plt.legend()\n",
    "    plt.tight_layout()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f39270",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "symbolIdx = 2\n",
    "symbol = sorted(symbols)[symbolIdx]\n",
    "print(symbol)\n",
    "quotesFileCh = os.path.join(dataFolder, quotes[symbolIdx])\n",
    "tradesFileCh = os.path.join(dataFolder, trades[symbolIdx])\n",
    "\n",
    "## get common Dates\n",
    "quotesDates = sorted([f.split(\".csv\")[0] for f in os.listdir(quotesFileCh)])\n",
    "tradesDates = sorted([f.split(\".csv\")[0] for f in os.listdir(tradesFileCh)])\n",
    "intersectionDates = list(set(quotesDates).intersection(tradesDates))\n",
    "quotesDates[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0955d8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trades_cols =['size', 'time', 'type', 'value']\n",
    "\n",
    "calendar_resample = \"300S\"\n",
    "trade_volume_width=100 \n",
    "ticks_width =100\n",
    "usd_volume_width=100\n",
    "datesChoice = quotesDates[0:5]\n",
    "testClass = DataLoader(data_location=dataFolder, \n",
    "           symbol=symbol, \n",
    "           dates= datesChoice,                        \n",
    "           use_columns=trades_cols, \n",
    "           calendar_resample = \"300S\",\n",
    "           trade_volume_width=trade_volume_width, \n",
    "           ticks_width =ticks_width, \n",
    "           usd_volume_width=usd_volume_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27400b2c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hash_of_file =  \"_\".join((str(symbol), \"volume_width\",str(trade_volume_width), \"calendar_resample\", str(calendar_resample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c951b0db",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## load data ##\n",
    "input_dict = testClass.load_and_format_data() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cb7015",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(quotesDates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a96046",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tick_bar_dfs =[]\n",
    "volume_bar_dfs =[]\n",
    "usd_volume_bar_dfs = []\n",
    "calendar_bar_dfs  =[]\n",
    "dates = list(input_dict.keys())\n",
    "for date in dates:\n",
    "    df = testClass.load_and_format_data()[str(date)]\n",
    "    input_dict = testClass.get_bars(df)\n",
    "    tick_bar_df = testClass.get_concat_data(testClass._bars_dict)['tick_bars']\n",
    "    volume_bar_df = testClass.get_concat_data(testClass._bars_dict)['volume_bars']\n",
    "    usd_volume_bar_df = testClass.get_concat_data(testClass._bars_dict)['usd_volume_bars']\n",
    "    calendar_bar_df = testClass.get_concat_data(testClass._bars_dict)['calendar_bars']\n",
    "    tick_bar_dfs.append(tick_bar_df)\n",
    "    volume_bar_dfs.append(volume_bar_df)\n",
    "    usd_volume_bar_dfs.append(usd_volume_bar_df)\n",
    "    calendar_bar_dfs.append(calendar_bar_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9f0d0b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tick_df_index = 1\n",
    "pattern = 'micro'\n",
    "column_names =(tick_bar_dfs[tick_df_index].columns[tick_bar_dfs[tick_df_index].columns.str.contains(pat=str(pattern))].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03ecdcf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tick_bucket_returns =tick_bar_dfs[tick_df_index][column_names].median(axis=1).pct_change().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb8f26c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "consolidated_tick_bar_df = pd.concat(tick_bar_dfs).dropna()\n",
    "consolidated_volume_bar_df = pd.concat(volume_bar_dfs).dropna()\n",
    "consolidated_calendar_bar_df = pd.concat(calendar_bar_dfs).dropna()\n",
    "consolidated_usd_volume_bar_df = pd.concat(usd_volume_bar_dfs).dropna()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5488f93e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "bars = {'Volume bars clock':consolidated_volume_bar_df,\n",
    "        'Tick bars clock': consolidated_tick_bar_df,'USDVolume bars clock':\n",
    "        consolidated_usd_volume_bar_df,\n",
    "        'Chrono clock': consolidated_calendar_bar_df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222ae0a6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "barsKeys = list(bars.keys())\n",
    "barsKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b7274f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_test =bars['Chrono clock'].dropna()\n",
    "\n",
    "df_test_vol = volatilityEstimation(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97af9b4c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = descriptiveStats(df_test).base_descriptive_stats()\n",
    "print(pd.Series(data).to_frame().to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa6d85a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "vr = returns(bars[barsKeys[1]].micro_price_close).replace([np.inf, -np.inf], 0)# volume\n",
    "tr = returns(bars[barsKeys[0]].micro_price_close).replace([np.inf, -np.inf], 0)# tick\n",
    "dr = returns(bars[barsKeys[2]].micro_price_close).dropna().replace([np.inf, -np.inf], 0) # usd volume\n",
    "df_ret = returns(bars[barsKeys[3]].micro_price_close).dropna().replace([np.inf, -np.inf], 0)    # calendar\n",
    "bar_returns = [tr, vr, dr, df_ret]\n",
    "bar_types = ['tick','volume','dollar','calendar']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85e7d54",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Next Few blocks are for plotting. move down for fathon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8730b43",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# def return_scaled(x):\n",
    "#     scaler = StandardScaler()\n",
    "#     x_scaled = scaler.fit_transform(np.array(x).reshape(-1,1))\n",
    "#     return np.asarray(x_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b14340a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(10,7))\n",
    "# sns.kdeplot(tr, \n",
    "#             ax=ax,\n",
    "#             label=\"Tick\", \n",
    "#             bw=0.25,\n",
    "#             color='darkblue')\n",
    "# sns.kdeplot(dr,\n",
    "#             ax=ax,\n",
    "#             label=\"USD Volume\",\n",
    "#             bw=0.55,linewidth=2.25,\n",
    "#             color='blue', linestyle =':')\n",
    "\n",
    "# sns.kdeplot(vr,\n",
    "#             ax=ax,\n",
    "#             label=\"Volume\",\n",
    "#             bw=0.55,linewidth=1.25,\n",
    "#             color='red', linestyle ='--')\n",
    "\n",
    "\n",
    "# plt.xlabel('Returns', fontsize=9)\n",
    "# plt.ylabel('Density', fontsize=9)\n",
    "# plt.xticks(fontsize = 9, rotation = 45)\n",
    "# plt.yticks(fontsize = 9)\n",
    "# plt.title(str(symbol))\n",
    "# file_name = str(hash_of_file) + '_multiclocks_density_plot.png'\n",
    "# plt.savefig(os.path.join(figures_destination,file_name))\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a31b1d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Simple normality tests\n",
    "# from scipy.stats import shapiro, normaltest, jarque_bera\n",
    "# bar_choice = volume_standard.values\n",
    "# stat, p = shapiro(bar_choice)\n",
    "# print('stat = %.3f, p = %.3f\\n ' % (stat, p))\n",
    "# if p > 0.05:\n",
    "#     print('prob gaussian')\n",
    "# else:\n",
    "#     print('non gaussian')\n",
    "\n",
    "# stat_nt, p_nt = normaltest(bar_choice)\n",
    "\n",
    "# print('stat = %.3f, p = %.3f\\n ' % (stat_nt, p_nt))\n",
    "\n",
    "# stat_jb, p_jb = jarque_bera(bar_choice)\n",
    "\n",
    "# print('stat = %.3f, p = %.3f\\n ' % (stat_jb, p_jb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19fd78b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# return_scaled(dr).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d554cb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import scipy\n",
    "# from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "# for barIdx in range(0,3):\n",
    "#     vol_estimation = volatilityEstimation(bars[barsKeys[barIdx]])\n",
    "\n",
    "#     garman_klass_vol_10 = vol_estimation.garmanKlass(trading_periods = 10)\n",
    "#     garman_klass_vol_50 = vol_estimation.garmanKlass(trading_periods = 50)\n",
    "#     garman_klass_vol_100 = vol_estimation.garmanKlass(trading_periods = 100)\n",
    "\n",
    "#     g1 =sns.distplot(garman_klass_vol_10, hist=True, kde=True, \n",
    "#                  bins=int(200/5), color = 'darkblue', \n",
    "#                  hist_kws={'edgecolor':'red'},\n",
    "#                  kde_kws={'linewidth': 2}, \n",
    "#                     label=\"10-trading ticks\")\n",
    "#     g2 =sns.distplot(garman_klass_vol_50, hist=True, kde=True, \n",
    "#                  bins=int(200/5), color = 'green', \n",
    "#                  hist_kws={'edgecolor':'lightblue'},\n",
    "#                  kde_kws={'linewidth': 2},\n",
    "#                     label=\"50-trading ticks\")\n",
    "#     g3 =sns.distplot(garman_klass_vol_100, hist=True, kde=True, \n",
    "#                  bins=int(200/5), color = 'red', \n",
    "#                  hist_kws={'edgecolor':'lightblue'},\n",
    "#                  kde_kws={'linewidth': 2},\n",
    "#                     label=\"100-trading ticks\")\n",
    "#     plt.xlabel('xlabel', fontsize=9)\n",
    "#     plt.ylabel('ylabel', fontsize=9)\n",
    "#     plt.xticks(fontsize = 9, rotation = 45)\n",
    "#     plt.yticks(fontsize = 9)\n",
    "#     plt.gca().xaxis.set_major_formatter(PercentFormatter(10))\n",
    "#     plt.legend(fontsize=7)\n",
    "#     _=plt.xlabel(str(barsKeys[barIdx])+' Garman Klass Volatility') #y label\n",
    "#     file_name = str(symbol)+str(barsKeys[barIdx])+'_GK-Klass Histogram.png'\n",
    "#     plt.savefig(os.path.join(figures_destination,file_name))\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b747a5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print(barsKeys[barIdx])\n",
    "# for barIdx in range(0,3):\n",
    "#     vol_estimation = volatilityEstimation(bars[barsKeys[barIdx]])\n",
    "#     arrival_rates = vol_estimation.arrival_rates()\n",
    "\n",
    "#     g1 =sns.distplot(arrival_rates, hist=True, kde=True, \n",
    "#                  bins=int(200/5), color = 'darkblue', \n",
    "#                  hist_kws={'edgecolor':'red'},\n",
    "#                  kde_kws={'linewidth': 2}, \n",
    "#                     label=\"arrival-rates\")\n",
    "#     # g2 =sns.distplot(garman_klass_vol_50, hist=True, kde=True, \n",
    "#     #              bins=int(200/5), color = 'green', \n",
    "#     #              hist_kws={'edgecolor':'lightblue'},\n",
    "#     #              kde_kws={'linewidth': 2},\n",
    "#     #                 label=\"50-trading ticks\")\n",
    "#     # plt.gca().xaxis.set_major_formatter(PercentFormatter(1))\n",
    "#     plt.xlabel('xlabel', fontsize=9)\n",
    "#     plt.ylabel('ylabel', fontsize=9)\n",
    "#     plt.xticks(fontsize = 9, rotation = 45)\n",
    "#     plt.yticks(fontsize = 9)\n",
    "#     #plt.gca().xaxis.set_major_formatter(PercentFormatter(1))\n",
    "#     plt.legend(fontsize=7)\n",
    "#     _=plt.xlabel('Arrival rates for '+ str(barsKeys[barIdx])) #y label\n",
    "  \n",
    "# #     file_name = testSymbol+'_'+str(hash_of_file) + str(barsKeys[barIdx]) +'arrival-rates.png'\n",
    "# #     plt.savefig(os.path.join(figures_destination,file_name))\n",
    "# #     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4049f90",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print(barsKeys[barIdx])\n",
    "# for barIdx in range(0,3):\n",
    "#     vol_estimation = volatilityEstimation(bars[barsKeys[barIdx]])\n",
    "#     relative_jump_measure_20 = vol_estimation.relative_jump_measure(200)\n",
    "#     print(pd.DataFrame(relative_jump_measure_20).quantile([.1, .2, .75]).style.format(\"{:.2%}\"))\n",
    "\n",
    "# #     g1 =sns.distplot(relative_jump_measure_10, hist=True, kde=True, \n",
    "# #                  bins=int(200/5), color = 'darkblue', \n",
    "# #                  hist_kws={'edgecolor':'red'},\n",
    "# #                  kde_kws={'linewidth': 2}, \n",
    "# #                     label=\"10-trading-ticks\")\n",
    "    \n",
    "#     g2 =sns.distplot(relative_jump_measure_20, hist=True, kde=True, \n",
    "#                  bins=int(200/5), color = 'green', \n",
    "#                  hist_kws={'edgecolor':'lightblue'},\n",
    "#                  kde_kws={'linewidth': 2},\n",
    "#                     label=\"200-trading ticks\")\n",
    "    \n",
    "#     # plt.gca().xaxis.set_major_formatter(PercentFormatter(1))\n",
    "#     plt.legend()\n",
    "#     _=plt.xlabel('Relative jump metric for '+ str(barsKeys[barIdx])) #y label\n",
    "\n",
    "#     file_name = testSymbol+'_'+str(hash_of_file) + str(barsKeys[barIdx]) +'relative_jump_metric.png'\n",
    "    \n",
    "#     plt.savefig(os.path.join(figures_destination,file_name))\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26219f3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "import glob\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "%matplotlib inline\n",
    "import fathon\n",
    "from fathon import fathonUtils as fu\n",
    "from MFDFA import fgn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15018bb8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## pick a data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bd9574",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = vr\n",
    "b_raw = np.asanyarray(data)\n",
    "b = fu.toAggregated(b_raw)\n",
    "pydfb = fathon.DFA(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc09a10",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# params\n",
    "\n",
    "winSizes = fu.linRangeByStep(10, 2000)\n",
    "revSeg = True\n",
    "polOrd = 3\n",
    "qs = np.arange(-3, 4, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df23a3f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "n, F = pydfb.computeFlucVec(winSizes, revSeg=revSeg, polOrd=polOrd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f2c819",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.log(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcb62c3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "H, H_intercept = pydfb.fitFlucVec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f59f246",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.log(n), np.log(F), 'ro')\n",
    "plt.plot(np.log(n), H_intercept+H*np.log(n), '--', label='H = {:.2f}'.format(H))\n",
    "plt.xlabel('ln(n)', fontsize=14)\n",
    "plt.ylabel('ln(F(n))', fontsize=14)\n",
    "plt.title('DFA', fontsize=14)\n",
    "plt.legend(loc=0, fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a9a9f6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "pymfdfb = fathon.MFDFA(data)\n",
    "n, F = pymfdfb.computeFlucVec(winSizes, qs, revSeg=revSeg, polOrd=polOrd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1ca967",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list_H, list_H_intercept = pymfdfb.fitFlucVec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1c191e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.log(n), np.log(F[0, :]), 'ro')\n",
    "plt.plot(np.log(n), list_H_intercept[0]+list_H[0]*np.log(n), '--', label='h_{:.1f} = {:.2f}'.format(qs[0], list_H[0]))\n",
    "half_idx = int(len(qs)/2)\n",
    "plt.plot(np.log(n), np.log(F[half_idx, :]), 'co')\n",
    "plt.plot(np.log(n), list_H_intercept[half_idx]+list_H[half_idx]*np.log(n),\n",
    "         '--', label='h_{:.1f} = {:.2f}'.format(qs[half_idx], list_H[half_idx]))\n",
    "plt.plot(np.log(n), np.log(F[-1, :]), 'yo')\n",
    "plt.plot(np.log(n), list_H_intercept[-1]+list_H[-1]*np.log(n), '--',\n",
    "         label='h_{:.1f} = {:.2f}'.format(qs[-1], list_H[-1]))\n",
    "plt.xlabel('ln(n)', fontsize=14)\n",
    "plt.ylabel('ln(F(n))', fontsize=14)\n",
    "plt.title('MFDFA', fontsize=14)\n",
    "plt.legend(loc=0, fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b41b73",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(qs, list_H, 'ro-')\n",
    "plt.xlabel('q', fontsize=14)\n",
    "plt.ylabel('h(q)', fontsize=14)\n",
    "plt.title('h(q)', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45290d19",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tau = pymfdfb.computeMassExponents()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c55dee",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(qs, tau, 'o--', color='mediumvioletred')\n",
    "plt.xlabel('q', fontsize=14)\n",
    "plt.ylabel('$\\\\tau$(q)', fontsize=14)\n",
    "plt.title('$\\\\tau$(q)', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340d47ba",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "alpha, mfSpect = pymfdfb.computeMultifractalSpectrum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db1d23",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_=plt.plot(alpha, mfSpect, 'bo--')\n",
    "_=plt.xlabel('$\\\\alpha$', fontsize=14)\n",
    "_=plt.ylabel('f($\\\\alpha$)', fontsize=14)\n",
    "_=plt.title('f($\\\\alpha$)', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2d19f9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
