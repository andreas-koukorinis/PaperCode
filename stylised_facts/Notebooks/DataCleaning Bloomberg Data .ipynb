{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from dateutil.parser import parse\n",
    "from pandas.tseries.offsets import BDay\n",
    "import pickle as pkl\n",
    "import fnmatch\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "import glob\n",
    "#Set PANDAS to show all columns in DataFrame\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def listdirs(folder): #return only directories from a master folder\n",
    "    return [d for d in os.listdir(folder) if os.path.isdir(os.path.join(folder, d))]\n",
    "\n",
    "def find_path(basepath, stringname):\n",
    "    for fname in os.listdir(basepath):\n",
    "        path = os.path.join(basepath, fname)\n",
    "        if os.path.isdir(path):\n",
    "            if stringname in fname:\n",
    "                return path\n",
    "def get_ticket(rx_path):\n",
    "    ticker=rx_path.split(\".\")[0].split(\"/\")[-1].split(\"-\")[0]\n",
    "    return ticker\n",
    "\n",
    "def ticker_trades_dir(ticker):\n",
    "    dest=os.path.join(os.getenv('FINANCE_DATA'), \"_\".join((ticker,'trades')))\n",
    "    if not os.path.isdir(dest):\n",
    "        os.makedirs(dest)\n",
    "def quotes_trades_dir(ticker):\n",
    "    dest=os.path.join(os.getenv('FINANCE_DATA'), \"_\".join((ticker,'quotes')))\n",
    "    if not os.path.isdir(dest):\n",
    "        os.makedirs(dest)\n",
    "def agg_on_trd_time(gr):\n",
    "    \"\"\"\n",
    "    Utility func to aggregate trades on timestamp. All trades with equal time stamp\n",
    "    will collapse to one row and the traded price will be the volume weighted traded\n",
    "    price.\n",
    "    \"\"\"\n",
    "    vTrdPrice = np.sum(gr['TradedPrice'] * gr['Volume'])/np.sum(gr['Volume'])\n",
    "    volume = np.sum(gr['Volume'])\n",
    "\n",
    "    return pd.Series({'Volume': volume,\n",
    "                      'TradedPrice': vTrdPrice})\n",
    "\n",
    "def open_pickle_file(path, pickle_file):\n",
    "    file_loc = os.path.join(path, pickle_file)\n",
    "    pickle_to_file = pickle.load(open(file_loc, \"rb\"))\n",
    "    return pickle_to_file\n",
    "def common_member(a, b): \n",
    "      \n",
    "    a_set = set(a) \n",
    "    b_set = set(b) \n",
    "      \n",
    "    # check length  \n",
    "    if len(a_set.intersection(b_set)) > 0: \n",
    "        return(a_set.intersection(b_set))   \n",
    "    else: \n",
    "        return(\"no common elements\") \n",
    "\n",
    "def f(x):\n",
    "     return Series(dict(Number_of_tweets = x['content'].count(), \n",
    "                        Company=x['Company'].min(),\n",
    "                        Description=x['from_user_description'].min(),\n",
    "                        ))\n",
    "    \n",
    "def obv_calc(df):\n",
    "    df['SignedVolume']=df['Volume']*np.sign(df['TradedPrice'].diff()).cumsum()\n",
    "    df['SignedVolume'][:1]=0\n",
    "    df['OBV']=df['SignedVolume'].cumsum()\n",
    "    df =df.drop(columns=['SignedVolume'])\n",
    "    return df\n",
    "def chaikin_mf(df, period=5):\n",
    "    df[\"MF Multiplier\"] = (df['TradedPrice']-(df['TradedPrice'].expanding(period).min() ) \\\n",
    "                           - (df['TradedPrice'].expanding(period).max() - df['TradedPrice']))/(df['TradedPrice'].expanding(period).max() - df['TradedPrice'].expanding(period).min())\n",
    "    df[\"MF Volume\"] = df['MF Multiplier'] * df['Volume'] \n",
    "    df['CMF']= df['MF Volume'].sum()/df[\"Volume\"].rolling(5).sum()\n",
    "    df=df.drop(columns=['MF Multiplier','MF Volume'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "##locations ##\n",
    "#dataOnlyDrive = ('/media/ak/DataOnly')\n",
    "# ''' Exterrnal Files'''\n",
    "extPath = '/media/ak/My Passport/Experiment Data'\n",
    "barketData = '/media/ak/My Passport/Barket Data/'\n",
    "#cleanBloombergDataLocation = os.path.join(dataOnlyDrive, 'CleanBloombergData')\n",
    "destinationFolder = '/media/ak/My Passport/ExperimentData/ProcessedDataForExperiments'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DU1_Comdty',\n",
       " 'DU1_Quotes',\n",
       " 'DU1_Trades',\n",
       " 'FB1_Comdty',\n",
       " 'FB1_Quotes',\n",
       " 'FB1_Trades',\n",
       " 'FV1_Comdty',\n",
       " 'raw bloomberg data',\n",
       " 'TU1_Comdty-20181028',\n",
       " 'TY1_Comdty_quotes',\n",
       " 'TY1_Comdty_trades',\n",
       " 'US1_Comdty_quotes',\n",
       " 'US1_Comdty_trades',\n",
       " 'UST10y',\n",
       " 'UST2y',\n",
       " 'UST5y',\n",
       " 'VIX_Index_quotes',\n",
       " 'VIX_Index_trades',\n",
       " 'VXX_Equity',\n",
       " 'VXX_Equity_quotes',\n",
       " 'VXX_Equity_trades']"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(barketData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetDrive = barketData\n",
    "bmrg_folders=[s for s in os.listdir(targetDrive ) if ('2y') in s]\n",
    "bmrg_trades=sorted([s for s in os.listdir(targetDrive ) if s.endswith('y_trades')])\n",
    "bmrg_quotes=sorted([s for s in os.listdir(targetDrive ) if s.endswith('y_quotes')])\n",
    "bmrg_tickers=sorted([bmrg_trades[idx].split('_t')[0] for idx,_ in enumerate(bmrg_trades)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UST2y'"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bmrg_folders[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "Folder = os.path.join(targetDrive, bmrg_folders[0])\n",
    "# tradesFolder = os.path.join(targetDrive, bmrg_folders[1])\n",
    "\n",
    "list_Files = os.listdir(Folder)\n",
    "# trade breakdown size\ttime\ttype\tvalue\n",
    "#file_to_load = pd.read_csv(os.path.join(symbolFolder, list_Files[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20180413'"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_Files[0].split(\"-\")[-1].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 USGG2YR_Index-20180413.csv\n",
      "1 USGG2YR_Index-20180416.csv\n",
      "2 USGG2YR_Index-20180417.csv\n",
      "3 USGG2YR_Index-20180418.csv\n",
      "4 USGG2YR_Index-20180419.csv\n",
      "5 USGG2YR_Index-20180420.csv\n",
      "6 USGG2YR_Index-20180423.csv\n",
      "7 USGG2YR_Index-20180424.csv\n",
      "8 USGG2YR_Index-20180425.csv\n",
      "9 USGG2YR_Index-20180426.csv\n",
      "10 USGG2YR_Index-20180427.csv\n",
      "11 USGG2YR_Index-20180430.csv\n",
      "12 USGG2YR_Index-20180501.csv\n",
      "13 USGG2YR_Index-20180502.csv\n",
      "14 USGG2YR_Index-20180716.csv\n",
      "15 USGG2YR_Index-20180717.csv\n",
      "16 USGG2YR_Index-20180718.csv\n",
      "17 USGG2YR_Index-20180719.csv\n",
      "18 USGG2YR_Index-20180720.csv\n",
      "19 USGG2YR_Index-20180723.csv\n",
      "20 USGG2YR_Index-20180724.csv\n",
      "21 USGG2YR_Index-20180725.csv\n",
      "22 USGG2YR_Index-20180726.csv\n",
      "23 USGG2YR_Index-20180727.csv\n",
      "24 USGG2YR_Index-20180730.csv\n",
      "25 USGG2YR_Index-20180731.csv\n",
      "26 USGG2YR_Index-20180801.csv\n",
      "27 USGG2YR_Index-20180802.csv\n",
      "28 USGG2YR_Index-20180803.csv\n",
      "29 USGG2YR_Index-20180806.csv\n",
      "30 USGG2YR_Index-20180807.csv\n",
      "31 USGG2YR_Index-20180809.csv\n",
      "32 USGG2YR_Index-20180528.csv\n",
      "33 USGG2YR_Index-20180529.csv\n",
      "34 USGG2YR_Index-20180530.csv\n",
      "35 USGG2YR_Index-20180531.csv\n",
      "36 USGG2YR_Index-20180601.csv\n",
      "37 USGG2YR_Index-20180604.csv\n",
      "38 USGG2YR_Index-20180605.csv\n",
      "39 USGG2YR_Index-20180606.csv\n",
      "40 USGG2YR_Index-20180607.csv\n",
      "41 USGG2YR_Index-20180608.csv\n",
      "42 USGG2YR_Index-20180611.csv\n",
      "43 USGG2YR_Index-20180612.csv\n",
      "44 USGG2YR_Index-20180613.csv\n",
      "45 USGG2YR_Index-20180614.csv\n",
      "46 USGG2YR_Index-20180906.csv\n",
      "47 USGG2YR_Index-20180907.csv\n",
      "48 USGG2YR_Index-20180910.csv\n",
      "49 USGG2YR_Index-20180911.csv\n",
      "50 USGG2YR_Index-20180912.csv\n",
      "51 USGG2YR_Index-20180913.csv\n",
      "52 USGG2YR_Index-20180914.csv\n",
      "53 USGG2YR_Index-20180917.csv\n",
      "54 USGG2YR_Index-20180918.csv\n",
      "55 USGG2YR_Index-20180919.csv\n",
      "56 USGG2YR_Index-20180920.csv\n",
      "57 USGG2YR_Index-20180921.csv\n",
      "58 USGG2YR_Index-20180924.csv\n",
      "59 USGG2YR_Index-20180925.csv\n",
      "60 USGG2YR_Index-20180926.csv\n",
      "61 USGG2YR_Index-20180927.csv\n",
      "62 USGG2YR_Index-20180928.csv\n",
      "63 USGG2YR_Index-20181001.csv\n",
      "64 USGG2YR_Index-20181002.csv\n",
      "65 USGG2YR_Index-20180503.csv\n",
      "66 USGG2YR_Index-20180525.csv\n",
      "67 USGG2YR_Index-20180615.csv\n",
      "68 USGG2YR_Index-20180713.csv\n",
      "69 USGG2YR_Index-20180810.csv\n",
      "70 USGG2YR_Index-20180905.csv\n",
      "71 USGG2YR_Index-20181003.csv\n",
      "72 USGG2YR_Index-20180813.csv\n",
      "73 USGG2YR_Index-20180814.csv\n",
      "74 USGG2YR_Index-20180815.csv\n",
      "75 USGG2YR_Index-20180816.csv\n",
      "76 USGG2YR_Index-20180817.csv\n",
      "77 USGG2YR_Index-20180820.csv\n",
      "78 USGG2YR_Index-20180821.csv\n",
      "79 USGG2YR_Index-20180822.csv\n",
      "80 USGG2YR_Index-20180823.csv\n",
      "81 USGG2YR_Index-20180824.csv\n",
      "82 USGG2YR_Index-20180827.csv\n",
      "83 USGG2YR_Index-20180828.csv\n",
      "84 USGG2YR_Index-20180829.csv\n",
      "85 USGG2YR_Index-20180830.csv\n",
      "86 USGG2YR_Index-20180831.csv\n",
      "87 USGG2YR_Index-20180903.csv\n",
      "88 USGG2YR_Index-20180904.csv\n",
      "89 USGG2YR_Index-20181004.csv\n",
      "90 USGG2YR_Index-20181005.csv\n",
      "91 USGG2YR_Index-20181008.csv\n",
      "92 USGG2YR_Index-20181009.csv\n",
      "93 USGG2YR_Index-20181010.csv\n",
      "94 USGG2YR_Index-20181011.csv\n",
      "95 USGG2YR_Index-20181012.csv\n",
      "96 USGG2YR_Index-20181015.csv\n",
      "97 USGG2YR_Index-20181016.csv\n",
      "98 USGG2YR_Index-20181017.csv\n",
      "99 USGG2YR_Index-20181018.csv\n",
      "100 USGG2YR_Index-20181019.csv\n",
      "101 USGG2YR_Index-20181022.csv\n",
      "102 USGG2YR_Index-20181023.csv\n",
      "103 USGG2YR_Index-20181024.csv\n",
      "104 USGG2YR_Index-20181025.csv\n",
      "105 USGG2YR_Index-20181026.csv\n",
      "106 USGG2YR_Index-20180618.csv\n",
      "107 USGG2YR_Index-20180619.csv\n",
      "108 USGG2YR_Index-20180620.csv\n",
      "109 USGG2YR_Index-20180621.csv\n",
      "110 USGG2YR_Index-20180622.csv\n",
      "111 USGG2YR_Index-20180625.csv\n",
      "112 USGG2YR_Index-20180626.csv\n",
      "113 USGG2YR_Index-20180627.csv\n",
      "114 USGG2YR_Index-20180628.csv\n",
      "115 USGG2YR_Index-20180629.csv\n",
      "116 USGG2YR_Index-20180702.csv\n",
      "117 USGG2YR_Index-20180703.csv\n",
      "118 USGG2YR_Index-20180704.csv\n",
      "119 USGG2YR_Index-20180705.csv\n",
      "120 USGG2YR_Index-20180706.csv\n",
      "121 USGG2YR_Index-20180709.csv\n",
      "122 USGG2YR_Index-20180710.csv\n",
      "123 USGG2YR_Index-20180711.csv\n",
      "124 USGG2YR_Index-20180712.csv\n",
      "125 USGG2YR_Index-20180504.csv\n",
      "126 USGG2YR_Index-20180507.csv\n",
      "127 USGG2YR_Index-20180508.csv\n",
      "128 USGG2YR_Index-20180509.csv\n",
      "129 USGG2YR_Index-20180510.csv\n",
      "130 USGG2YR_Index-20180511.csv\n",
      "131 USGG2YR_Index-20180514.csv\n",
      "132 USGG2YR_Index-20180515.csv\n",
      "133 USGG2YR_Index-20180516.csv\n",
      "134 USGG2YR_Index-20180518.csv\n",
      "135 USGG2YR_Index-20180521.csv\n",
      "136 USGG2YR_Index-20180522.csv\n",
      "137 USGG2YR_Index-20180523.csv\n",
      "138 USGG2YR_Index-20180524.csv\n"
     ]
    }
   ],
   "source": [
    "for idx, fileName in enumerate(list_Files):\n",
    "    print(idx, fileName)\n",
    "    dateName = fileName.split(\"-\")[-1].split(\".\")[0]\n",
    "    file  = pd.read_csv(os.path.join(Folder, fileName))\n",
    "    trades = file[file['type']=='TRADE'].drop(columns=['Unnamed: 0'])\n",
    "    quotes = file[file['type']!='TRADE'].drop(columns=['Unnamed: 0'])\n",
    "    tradesFileName = os.path.join(destinationFolder,'UST2y_Trades',dateName+'.csv')\n",
    "    quotesFileName = os.path.join(destinationFolder,'UST2y_Quotes',dateName+'.csv')\n",
    "    trades.to_csv(tradesFileName, index=False)\n",
    "    quotes.to_csv(quotesFileName, index=False)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/ak/My Passport/ExperimentData/ProcessedDataForExperiments/UST2y_Trades/20180524.csv'"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tradesFileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/ak/My Passport/ExperimentData/ProcessedDataForExperiments/UST10y_Quotes/20180524.csv'"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotesFileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UST2y']\n"
     ]
    }
   ],
   "source": [
    "# symbols = ['FB1', 'TU1', 'TY1', 'FV1']\n",
    "print(bmrg_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UST2y\n",
      "UST2y\n"
     ]
    }
   ],
   "source": [
    "symbolIdx= 0\n",
    "print(bmrg_folders[symbolIdx])\n",
    "print(bmrg_folders[symbolIdx])\n",
    "# get dates and files\n",
    "symbol_quotes = os.path.join(targetDrive,str(bmrg_folders[symbolIdx]))\n",
    "symbol_trades = os.path.join(targetDrive,str(bmrg_folders[symbolIdx]))\n",
    "symbolQuoteDates = [quoteFile.split(\".\")[0] for quoteFile in os.listdir(symbol_quotes)] \n",
    "symbolTradeDates = [tradeFile.split(\".\")[0] for tradeFile in os.listdir(symbol_trades)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createLOB(rawLOBFile):\n",
    "    '''\n",
    "    create a clean LOB based on the Bloomberg Files that I got from Barket.\n",
    "    :param rawLOBFile:\n",
    "    :return: LOB\n",
    "    '''\n",
    "    dfBID = rawLOBFile[rawLOBFile['type'] == 'BID']\n",
    "\n",
    "    dfASK = rawLOBFile[rawLOBFile['type'] == 'ASK']\n",
    "    dfTRADE = rawLOBFile[rawLOBFile['type'] == 'TRADE']\n",
    "\n",
    "    dfBID.loc[:,('TradeTime')] = pd.to_datetime(dfBID.time).values\n",
    "    dfASK.loc[:,('TradeTime')] = pd.to_datetime(dfASK.time).values\n",
    "    dfTRADE.loc[:,('TradeTime')] = pd.to_datetime(dfTRADE.time).values\n",
    "    dfTRADE.loc[:,('TradeId')] = dfTRADE.index.values\n",
    "    dfBID = dfBID.dropna().fillna(\"ffill\").sort_values('TradeTime')\n",
    "    dfASK = dfASK.dropna().fillna(\"ffill\").sort_values('TradeTime')\n",
    "    dfTRADE = dfTRADE.dropna().fillna(\"ffill\").sort_values('TradeTime')\n",
    "\n",
    "    dfLOB = pd.merge_asof(dfBID, dfASK, on='TradeTime', allow_exact_matches=True).sort_values('TradeTime')\n",
    "\n",
    "    LOB = pd.merge_asof(dfLOB, dfTRADE, on='TradeTime', allow_exact_matches=True)\n",
    "\n",
    "    return pd.DataFrame(LOB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tradeTestDate = os.path.join(symbol_trades, quoteTradeDates[0]+'.csv')\n",
    "# tradeQuoteDate = os.path.join(symbol_quotes, quoteTradeDates[0]+'.csv')\n",
    "# trades =pd.read_csv(tradeTestDate,low_memory=False)\n",
    "# quotes = pd.read_csv(tradeQuoteDate, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "quoteDates = [symbolQuoteDates[i].split(\"-\")[-1] for i, _ in enumerate(symbolQuoteDates)]\n",
    "tradeDates = [symbolTradeDates[i].split(\"-\")[-1] for i, _ in enumerate(symbolTradeDates)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionaries that have all the data we may need/want. on for top of the LOB and one for trades\n",
    "quoteTradeDates=[eventDate for eventDate in quoteDates if eventDate in tradeDates]\n",
    "len(quoteTradeDates)\n",
    "dfAllTrades ={}\n",
    "dfAllQuotes ={}\n",
    "for idx,date in enumerate(quoteTradeDates):\n",
    "    tradeTestDate = pd.read_csv(os.path.join(os.path.join(destinationFolder,'UST2y_Quotes'), quoteTradeDates[idx]+'.csv'))\n",
    "    tradeQuoteDate = pd.read_csv(os.path.join(os.path.join(destinationFolder,'UST2y_Trades'), quoteTradeDates[idx]+'.csv'))\n",
    "    dfAllTrades[date] = tradeTestDate\n",
    "    dfAllQuotes[date] = tradeQuoteDate\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>time</th>\n",
       "      <th>type</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 00:00:05</td>\n",
       "      <td>BID</td>\n",
       "      <td>2.5282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 00:00:05</td>\n",
       "      <td>ASK</td>\n",
       "      <td>2.5242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 00:00:07</td>\n",
       "      <td>BID</td>\n",
       "      <td>2.5282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 00:00:07</td>\n",
       "      <td>ASK</td>\n",
       "      <td>2.5242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 00:00:15</td>\n",
       "      <td>BID</td>\n",
       "      <td>2.5282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 00:00:15</td>\n",
       "      <td>ASK</td>\n",
       "      <td>2.5242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 00:00:17</td>\n",
       "      <td>BID</td>\n",
       "      <td>2.5282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 00:00:17</td>\n",
       "      <td>ASK</td>\n",
       "      <td>2.5242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 00:00:29</td>\n",
       "      <td>BID</td>\n",
       "      <td>2.5282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 00:00:29</td>\n",
       "      <td>ASK</td>\n",
       "      <td>2.5242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 00:00:31</td>\n",
       "      <td>BID</td>\n",
       "      <td>2.5282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 00:00:31</td>\n",
       "      <td>ASK</td>\n",
       "      <td>2.5242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 00:00:33</td>\n",
       "      <td>BID</td>\n",
       "      <td>2.5282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 00:00:33</td>\n",
       "      <td>ASK</td>\n",
       "      <td>2.5242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 00:00:35</td>\n",
       "      <td>BID</td>\n",
       "      <td>2.5282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 00:00:35</td>\n",
       "      <td>ASK</td>\n",
       "      <td>2.5242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 00:00:43</td>\n",
       "      <td>BID</td>\n",
       "      <td>2.5282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 00:00:43</td>\n",
       "      <td>ASK</td>\n",
       "      <td>2.5242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 00:00:45</td>\n",
       "      <td>BID</td>\n",
       "      <td>2.5282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 00:00:45</td>\n",
       "      <td>ASK</td>\n",
       "      <td>2.5242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 00:00:51</td>\n",
       "      <td>BID</td>\n",
       "      <td>2.5282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 00:00:51</td>\n",
       "      <td>ASK</td>\n",
       "      <td>2.5242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 00:00:53</td>\n",
       "      <td>BID</td>\n",
       "      <td>2.5282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 00:00:53</td>\n",
       "      <td>ASK</td>\n",
       "      <td>2.5242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 00:01:04</td>\n",
       "      <td>BID</td>\n",
       "      <td>2.5282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 00:01:04</td>\n",
       "      <td>ASK</td>\n",
       "      <td>2.5242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 00:01:05</td>\n",
       "      <td>BID</td>\n",
       "      <td>2.5282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 00:01:05</td>\n",
       "      <td>ASK</td>\n",
       "      <td>2.5242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 00:01:35</td>\n",
       "      <td>BID</td>\n",
       "      <td>2.5282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 00:01:35</td>\n",
       "      <td>ASK</td>\n",
       "      <td>2.5242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30796</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 20:59:19</td>\n",
       "      <td>BID</td>\n",
       "      <td>2.5121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30797</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 20:59:19</td>\n",
       "      <td>ASK</td>\n",
       "      <td>2.5081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30798</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 20:59:28</td>\n",
       "      <td>BID</td>\n",
       "      <td>2.5121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30799</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 20:59:28</td>\n",
       "      <td>ASK</td>\n",
       "      <td>2.5081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30800</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 20:59:29</td>\n",
       "      <td>BID</td>\n",
       "      <td>2.5121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30801</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 20:59:29</td>\n",
       "      <td>ASK</td>\n",
       "      <td>2.5101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30802</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 20:59:30</td>\n",
       "      <td>BID</td>\n",
       "      <td>2.5121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30803</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 20:59:30</td>\n",
       "      <td>ASK</td>\n",
       "      <td>2.5101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30804</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 20:59:32</td>\n",
       "      <td>BID</td>\n",
       "      <td>2.5121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30805</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 20:59:32</td>\n",
       "      <td>ASK</td>\n",
       "      <td>2.5101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30806</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 20:59:35</td>\n",
       "      <td>BID</td>\n",
       "      <td>2.5121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30807</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 20:59:35</td>\n",
       "      <td>ASK</td>\n",
       "      <td>2.5101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30808</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 20:59:36</td>\n",
       "      <td>BID</td>\n",
       "      <td>2.5121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30809</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 20:59:36</td>\n",
       "      <td>ASK</td>\n",
       "      <td>2.5101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30810</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 20:59:42</td>\n",
       "      <td>BID</td>\n",
       "      <td>2.5121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30811</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 20:59:42</td>\n",
       "      <td>ASK</td>\n",
       "      <td>2.5101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30812</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 20:59:42</td>\n",
       "      <td>BID</td>\n",
       "      <td>2.5121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30813</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 20:59:42</td>\n",
       "      <td>ASK</td>\n",
       "      <td>2.5101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30814</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 20:59:44</td>\n",
       "      <td>BID</td>\n",
       "      <td>2.5121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30815</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 20:59:44</td>\n",
       "      <td>ASK</td>\n",
       "      <td>2.5081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30816</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 20:59:50</td>\n",
       "      <td>BID</td>\n",
       "      <td>2.5121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30817</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 20:59:50</td>\n",
       "      <td>ASK</td>\n",
       "      <td>2.5081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30818</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 20:59:51</td>\n",
       "      <td>BID</td>\n",
       "      <td>2.5121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30819</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 20:59:51</td>\n",
       "      <td>ASK</td>\n",
       "      <td>2.5081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30820</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 20:59:58</td>\n",
       "      <td>BID</td>\n",
       "      <td>2.5121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30821</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 20:59:58</td>\n",
       "      <td>ASK</td>\n",
       "      <td>2.5081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30822</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 20:59:59</td>\n",
       "      <td>BID</td>\n",
       "      <td>2.5121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30823</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-24 20:59:59</td>\n",
       "      <td>ASK</td>\n",
       "      <td>2.5081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30824</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-25 00:00:00</td>\n",
       "      <td>BID</td>\n",
       "      <td>2.5081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30825</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-25 00:00:00</td>\n",
       "      <td>ASK</td>\n",
       "      <td>2.5040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30826 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       size                 time type   value\n",
       "0         0  2018-05-24 00:00:05  BID  2.5282\n",
       "1         0  2018-05-24 00:00:05  ASK  2.5242\n",
       "2         0  2018-05-24 00:00:07  BID  2.5282\n",
       "3         0  2018-05-24 00:00:07  ASK  2.5242\n",
       "4         0  2018-05-24 00:00:15  BID  2.5282\n",
       "5         0  2018-05-24 00:00:15  ASK  2.5242\n",
       "6         0  2018-05-24 00:00:17  BID  2.5282\n",
       "7         0  2018-05-24 00:00:17  ASK  2.5242\n",
       "8         0  2018-05-24 00:00:29  BID  2.5282\n",
       "9         0  2018-05-24 00:00:29  ASK  2.5242\n",
       "10        0  2018-05-24 00:00:31  BID  2.5282\n",
       "11        0  2018-05-24 00:00:31  ASK  2.5242\n",
       "12        0  2018-05-24 00:00:33  BID  2.5282\n",
       "13        0  2018-05-24 00:00:33  ASK  2.5242\n",
       "14        0  2018-05-24 00:00:35  BID  2.5282\n",
       "15        0  2018-05-24 00:00:35  ASK  2.5242\n",
       "16        0  2018-05-24 00:00:43  BID  2.5282\n",
       "17        0  2018-05-24 00:00:43  ASK  2.5242\n",
       "18        0  2018-05-24 00:00:45  BID  2.5282\n",
       "19        0  2018-05-24 00:00:45  ASK  2.5242\n",
       "20        0  2018-05-24 00:00:51  BID  2.5282\n",
       "21        0  2018-05-24 00:00:51  ASK  2.5242\n",
       "22        0  2018-05-24 00:00:53  BID  2.5282\n",
       "23        0  2018-05-24 00:00:53  ASK  2.5242\n",
       "24        0  2018-05-24 00:01:04  BID  2.5282\n",
       "25        0  2018-05-24 00:01:04  ASK  2.5242\n",
       "26        0  2018-05-24 00:01:05  BID  2.5282\n",
       "27        0  2018-05-24 00:01:05  ASK  2.5242\n",
       "28        0  2018-05-24 00:01:35  BID  2.5282\n",
       "29        0  2018-05-24 00:01:35  ASK  2.5242\n",
       "...     ...                  ...  ...     ...\n",
       "30796     0  2018-05-24 20:59:19  BID  2.5121\n",
       "30797     0  2018-05-24 20:59:19  ASK  2.5081\n",
       "30798     0  2018-05-24 20:59:28  BID  2.5121\n",
       "30799     0  2018-05-24 20:59:28  ASK  2.5081\n",
       "30800     0  2018-05-24 20:59:29  BID  2.5121\n",
       "30801     0  2018-05-24 20:59:29  ASK  2.5101\n",
       "30802     0  2018-05-24 20:59:30  BID  2.5121\n",
       "30803     0  2018-05-24 20:59:30  ASK  2.5101\n",
       "30804     0  2018-05-24 20:59:32  BID  2.5121\n",
       "30805     0  2018-05-24 20:59:32  ASK  2.5101\n",
       "30806     0  2018-05-24 20:59:35  BID  2.5121\n",
       "30807     0  2018-05-24 20:59:35  ASK  2.5101\n",
       "30808     0  2018-05-24 20:59:36  BID  2.5121\n",
       "30809     0  2018-05-24 20:59:36  ASK  2.5101\n",
       "30810     0  2018-05-24 20:59:42  BID  2.5121\n",
       "30811     0  2018-05-24 20:59:42  ASK  2.5101\n",
       "30812     0  2018-05-24 20:59:42  BID  2.5121\n",
       "30813     0  2018-05-24 20:59:42  ASK  2.5101\n",
       "30814     0  2018-05-24 20:59:44  BID  2.5121\n",
       "30815     0  2018-05-24 20:59:44  ASK  2.5081\n",
       "30816     0  2018-05-24 20:59:50  BID  2.5121\n",
       "30817     0  2018-05-24 20:59:50  ASK  2.5081\n",
       "30818     0  2018-05-24 20:59:51  BID  2.5121\n",
       "30819     0  2018-05-24 20:59:51  ASK  2.5081\n",
       "30820     0  2018-05-24 20:59:58  BID  2.5121\n",
       "30821     0  2018-05-24 20:59:58  ASK  2.5081\n",
       "30822     0  2018-05-24 20:59:59  BID  2.5121\n",
       "30823     0  2018-05-24 20:59:59  ASK  2.5081\n",
       "30824     0  2018-05-25 00:00:00  BID  2.5081\n",
       "30825     0  2018-05-25 00:00:00  ASK  2.5040\n",
       "\n",
       "[30826 rows x 4 columns]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CleanBloombergData\n",
    "tradeTestDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAllQuotesName = \"\".join(('AllQuotes_',bmrg_folders[symbolIdx].split(\"_q\")[0],'.pkl'))\n",
    "dfAllTradesName = \"\".join(('AllTrades_',bmrg_folders[symbolIdx].split(\"_q\")[0],'.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AllQuotes_UST2y.pkl'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfAllQuotesName\n",
    "\n",
    "# cleanBloombergDataLocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotePath = os.path.join(destinationFolder, dfAllQuotesName)\n",
    "file = open(quotePath, 'wb')\n",
    "pkl.dump(dfAllQuotes,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "tradePath = os.path.join(destinationFolder, dfAllTradesName)\n",
    "file = open(tradePath, 'wb')\n",
    "pkl.dump(dfAllTrades,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(dfAllTrades, open(\"/\".join((cleanBloombergDataLocation, dfAllTradesName)), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/ak/My Passport/ExperimentData/ProcessedDataForExperiments'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destinationFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dQuoteKeys = list(dfAllQuotes.keys()) #one common set of keys at the moment\n",
    "dTradeKeys = list(dfAllTrades.keys())\n",
    "commonDates =list(set(dQuoteKeys).intersection(set(dTradeKeys)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfLOBRaw={}\n",
    "# QuoteColumns =['Unnamed: 0','TimeStamp']\n",
    "# TradeColumns= ['Unnamed: 0', 'size', 'time', 'type', 'value']\n",
    "for idx, dateKey in enumerate(commonDates):\n",
    "    #dfAllQuotes[dKeys[idx]].reset_index(level=0, inplace=True)\n",
    "    dfAllQuotes[commonDates[idx]]['TimeStamp']= pd.to_datetime(dfAllQuotes[commonDates[idx]]['time'])\n",
    "    dfAllTrades[commonDates[idx]]['TradeTimeStamp']= pd.to_datetime(dfAllTrades[commonDates[idx]]['time'])\n",
    "    dfAllTrades[commonDates[idx]]['TradedPrice']= dfAllTrades[commonDates[idx]]['value']\n",
    "    dfAllTrades[commonDates[idx]]['TradedSize']= dfAllTrades[commonDates[idx]]['size']\n",
    "    dfAllTrades[commonDates[idx]].rename(columns = {'type':'QuotedSide','value':'bestPrice','size':'QuoteSize','time':'QuoteTimeStamp'}, inplace = True) \n",
    "    dfAllTrades[commonDates[idx]]['Duration']=dfAllTrades[commonDates[idx]].TradeTimeStamp.diff()/np.timedelta64(1, 'ms')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AllQuotes_VXX_Equity.pkl'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfAllQuotesName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(dfAllQuotes, open(\"/\".join((cleanBloombergDataLocation, dfAllQuotesName)), \"wb\"))\n",
    "pkl.dump(dfAllTrades, open(\"/\".join((cleanBloombergDataLocation, dfAllTradesName)), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>QuoteSize</th>\n",
       "      <th>QuoteTimeStamp</th>\n",
       "      <th>QuotedSide</th>\n",
       "      <th>bestPrice</th>\n",
       "      <th>TradeTimeStamp</th>\n",
       "      <th>TradedPrice</th>\n",
       "      <th>TradedSize</th>\n",
       "      <th>Duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>130</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-04-23 00:02:02</td>\n",
       "      <td>TRADE</td>\n",
       "      <td>142.90625</td>\n",
       "      <td>2018-04-23 00:02:02</td>\n",
       "      <td>142.90625</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>239</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-04-23 00:02:25</td>\n",
       "      <td>TRADE</td>\n",
       "      <td>142.87500</td>\n",
       "      <td>2018-04-23 00:02:25</td>\n",
       "      <td>142.87500</td>\n",
       "      <td>1</td>\n",
       "      <td>23000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>354</td>\n",
       "      <td>100</td>\n",
       "      <td>2018-04-23 00:03:01</td>\n",
       "      <td>TRADE</td>\n",
       "      <td>142.87500</td>\n",
       "      <td>2018-04-23 00:03:01</td>\n",
       "      <td>142.87500</td>\n",
       "      <td>100</td>\n",
       "      <td>36000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  QuoteSize       QuoteTimeStamp QuotedSide  bestPrice  \\\n",
       "0         130          1  2018-04-23 00:02:02      TRADE  142.90625   \n",
       "1         239          1  2018-04-23 00:02:25      TRADE  142.87500   \n",
       "2         354        100  2018-04-23 00:03:01      TRADE  142.87500   \n",
       "\n",
       "       TradeTimeStamp  TradedPrice  TradedSize  Duration  \n",
       "0 2018-04-23 00:02:02    142.90625           1       NaN  \n",
       "1 2018-04-23 00:02:25    142.87500           1   23000.0  \n",
       "2 2018-04-23 00:03:01    142.87500         100   36000.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listKeys = list(dfAllTrades.keys())\n",
    "dfAllTrades[listKeys[1]].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=1\n",
    "dfBID =dfAllQuotes[commonDates[idx]][dfAllQuotes[commonDates[idx]]['type']=='BID'].rename(columns={'type':'BidSide','value':'bestBidPrice','size':'bestBidSize','time':'TimeStampS'})#.drop(QuoteColumns, inplace=True, axis=1)\n",
    "dfASK =dfAllQuotes[commonDates[idx]][dfAllQuotes[commonDates[idx]]['type']=='ASK'].rename(columns={'type':'AskSide','value':'bestAskPrice','size':'bestAskSize','time':'TimeStampS'})#.drop(QuoteColumns, inplace=True, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfLOB =dfBID.merge(dfASK,left_on='TimeStampS', right_on='TimeStampS')\n",
    "dfLOB= dfLOB.drop(['Unnamed: 0_x', 'Unnamed: 0_y','AskSide','BidSide'], axis=1)\n",
    "dfAllTrades[commonDates[idx]].TradeTimeStamp.diff().dropna()/np.timedelta64(1, 'ms')# dfLOB =dfBID.merge(dfASK,left_on='TimeStampS', right_on='TimeStampS')\n",
    "# dfLOB= dfLOB.drop(['Unnamed: 0_x', 'Unnamed: 0_y','AskSide','BidSide'], axis=1)\n",
    "dfAllTrades[commonDates[idx]]['Duration']=dfAllTrades[commonDates[idx]].TradeTimeStamp.diff().dropna()/np.timedelta64(1, 'ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAllTrades[commonDates[idx]].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfClean = dfAllTrades[commonDates[idx]]\n",
    "dfClean =dfClean.rename(index=str, columns={\"TradeTimeStamp\":\"TimeStampS\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMergedLOB = dfBID.merge(dfASK,left_on='TimeStampS', right_on='TimeStampS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMergedLOB.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMergedLOB['microPrice']=  (dfMergedLOB.bestAskPrice*dfMergedLOB.bestAskSize + dfMergedLOB.bestBidPrice*dfMergedLOB.bestBidSize)/(dfMergedLOB.bestAskSize+dfMergedLOB.bestBidSize) \n",
    "dfMergedLOB['bookPressure']=  (dfMergedLOB.bestAskPrice*dfMergedLOB.bestAskSize - dfMergedLOB.bestBidPrice*dfMergedLOB.bestBidSize)/(dfMergedLOB.bestAskSize+dfMergedLOB.bestBidSize) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dfLOBRaw\n",
    "del dfAllQuotesName\n",
    "del dfAllQuotes\n",
    "del dfAllTrades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=0\n",
    "dfBID =dfAllQuotes[dKeys[idx]][dfAllQuotes[dKeys[idx]]['type']=='BID'].rename(columns={'type':'BidSide','value':'bestBidPrice','size':'bestBidSize','time':'BidTimeStamp'})#.drop(QuoteColumns, inplace=True, axis=1)\n",
    "dfASK =dfAllQuotes[dKeys[idx]][dfAllQuotes[dKeys[idx]]['type']=='ASK'].rename(columns={'type':'AskSide','value':'bestAskPrice','size':'bestAskSize','time':'AskTimeStamp'})#.drop(QuoteColumns, inplace=True, axis=1)\n",
    "dfAllTrades[dKeys[0]].TradeTimeStamp.diff().dropna()/np.timedelta64(1, 'ms')\n",
    "#dfAllQuotes[dateKeys[idx]]['Time']= dfAllQuotes[dateKeys[idx]]['time'].dt.time\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDuration = dfClean[['TimeStampS','TradedPrice','TradedSize','Duration']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfQuotes =dfMergedLOB[['TimeStampS','microPrice', 'bookPressure']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique(col):\n",
    "    return {x:sum(col == x) for x in set(col)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfQuotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroDurationTrades =dfDuration[dfDuration['Duration']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonZeroDurationTrades =dfDuration[dfDuration['Duration']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroDurationTrades.Returns = zeroDurationTrades.TradedPrice.pct_change()\n",
    "nonZeroDurationTrades.Returns = nonZeroDurationTrades.TradedPrice.pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = np.diff(nonZeroDurationTrades.Returns)\n",
    "print('avg length between known sites', np.mean(diffs))\n",
    "print('sd of number of bp between sites', np.std(diffs))\n",
    "plt.plot(diffs)\n",
    "plt.xlabel('length between known sites')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('histogram of adjacent site information')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=plt.hist(nonZeroDurationTrades.TradedSize)\n",
    "plt.show()\n",
    "_=plt.hist(zeroDurationTrades.TradedSize, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfBID =dfAllQuotes[dKeys[0]][dfAllQuotes[dKeys[0]]['type']=='BID'].rename(columns={'type':'BidSide','value':'bestBidPrice','size':'bestBidSize','TimeStamp':'BidTimeStamp'})\n",
    "dfASK =dfAllQuotes[dKeys[0]][dfAllQuotes[dKeys[0]]['type']=='ASK'].rename(columns={'type':'AskSide','value':'bestAskPrice','size':'bestAskSize','TimeStamp':'AskTimeStamp'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfASK.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [ 'Unnamed: 0','TimeStamp','AskSide']\n",
    "dfASK.drop(columns, inplace=True, axis=1)\n",
    "dfASK.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [ 'Unnamed: 0','TimeStamp','BidSide']\n",
    "dfBID.drop(columns, inplace=True, axis=1)\n",
    "dfBID.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfLOBraw =pd.concat([dfBID, dfASK],axis=1, join='outer').ffill().dropna()\n",
    "dfLOBraw.tail(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_rolling=test_file['TradedPrice'].rolling(5).mean()\n",
    "v= test_file['Volume'].rolling(5).mean()\n",
    "df=test_file\n",
    "# df.groupby(df.index).rolling(7).apply(lambda x: np.average(x.TradedPrice, weights=x.Volume))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Groupby column with rolling mean.\n",
    "# df_grouped_rolling = df.groupby(df.index)[['TradedPrice', 'Volume']].rolling(window=3, min_periods=2).min()\n",
    "# df_grouped_rolling.groupby(df_grouped_rolling.index).rolling(7).apply(lambda x: np.average(x.TradedPrice, weights=x.Volume))\n",
    "# # # df_grouped_rolling.head(10)\n",
    "# df_grouped_rolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx=0\n",
    "# trades_files_loc = os.path.join(data_dir,bmrg_trades[idx])\n",
    "# trades_files= os.listdir(os.path.join(data_dir,bmrg_trades[idx]))\n",
    "# # file_idx=1\n",
    "# for file_idx,_ in enumerate(trades_files):\n",
    "#     symbol='G_1_Comdty'\n",
    "#     print('working on symbol:', symbol)\n",
    "#     trades_df = pd.read_csv(os.path.join(trades_files_loc, trades_files[file_idx]), index_col=0)\n",
    "#     print('reading this:', trades_files[file_idx])\n",
    "#     trades_df=trades_df.rename(index=str, columns={\"size\": \"Volume\",\"value\":\"TradedPrice\"}).drop(columns=['type'])\n",
    "#     trades_df['TradedTime'] = pd.to_datetime(trades_df['time'])\n",
    "#     trades_df= trades_df.drop(columns=['time'])\n",
    "#     res = trades_df.groupby('TradedTime').apply(agg_on_trd_time)\n",
    "#     res.reset_index(inplace=True)\n",
    "#     res.loc[:, 'Duration'] = res['TradedTime']- res['TradedTime'].shift(1)\n",
    "#     res['Duration'].fillna(value=0, inplace=True)\n",
    "#     res.loc[:, 'Duration'] = res['Duration'].apply(lambda tt: tt.total_seconds())\n",
    "#     res.loc[1:, 'ReturnTradedPrice'] = \\\n",
    "#         res['TradedPrice'].rolling(window=2).apply(lambda xx: np.log(xx[-1]/xx[0]))\n",
    "#     res.loc[0, 'ReturnTradedPrice'] = 0.\n",
    "#     target_file_name\n",
    "#     target_file_name = os.path.join(os.path.join(data_dir,symbol),trades_files[file_idx])\n",
    "#     print('saving here:', target_file_name)karima\n",
    "#     res[['TradedTime', 'TradedPrice', 'ReturnTradedPrice', 'Volume', \\\n",
    "#          'Duration']].to_csv(target_file_name, index=False)\n",
    "# print(res.columns.values)                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, symbol_trades in enumerate(bmrg_trades):\n",
    "#     trades_files_loc = os.path.join(data_dir,bmrg_trades[idx])\n",
    "#     trades_files= os.listdir(os.path.join(data_dir,bmrg_trades[idx]))\n",
    "#     symbol=\"_\".join((bmrg_trades[idx].split('_')[0],\"Comdty\"))\n",
    "#     print('working on symbol:', symbol)\n",
    "#     for file_idx,_ in enumerate(trades_files):\n",
    "#         print('reading this:', trades_files[file_idx])\n",
    "#         trades_df = pd.read_csv(os.path.join(trades_files_loc, trades_files[file_idx]), index_col=0)\n",
    "#         trades_df=trades_df.rename(index=str, columns={\"size\": \"Volume\",\"value\":\"TradedPrice\"}).drop(columns=['type'])\n",
    "#         trades_df['TradedTime'] = pd.to_datetime(trades_df['time'])\n",
    "#         trades_df= trades_df.drop(columns=['time'])\n",
    "#         res = trades_df.groupby('TradedTime').apply(agg_on_trd_time)\n",
    "#         res.reset_index(inplace=True)\n",
    "#         res.loc[:, 'Duration'] = res['TradedTime']- res['TradedTime'].shift(1)\n",
    "#         res['Duration'].fillna(value=0, inplace=True)\n",
    "#         res.loc[:, 'Duration'] = res['Duration'].apply(lambda tt: tt.total_seconds())\n",
    "#         res.loc[1:, 'ReturnTradedPrice'] = \\\n",
    "#             res['TradedPrice'].rolling(window=2).apply(lambda xx: np.log(xx[-1]/xx[0]))\n",
    "#         res.loc[0, 'ReturnTradedPrice'] = 0\n",
    "#         target_file_name = os.path.join(os.path.join(data_dir,symbol),trades_files[file_idx])\n",
    "#         print('saving here:', target_file_name)\n",
    "#         res[['TradedTime', 'TradedPrice', 'ReturnTradedPrice', 'Volume', \\\n",
    "#              'Duration']].to_csv(target_file_name, index=False)\n",
    "\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'XM1_Comdty'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbol_path = find_path(data_dir, str(bmrg_symbols[0]))\n",
    "# symbol_files=os.listdir(symbol_path)\n",
    "\n",
    "# symbol=symbol_files[1].split('-')[0]\n",
    "# date=symbol_files[1].split('-')[1].split('.')[0]\n",
    "idx=3\n",
    "trades_loc= os.path.join(data_dir,bmrg_trades[1])\n",
    "list_files =os.listdir(os.path.join(data_dir,bmrg_trades[1]))\n",
    "trades_file = os.path.join(trades_loc, list_files[idx])\n",
    "df= pd.read_csv(trades_file,index_col=0 )\n",
    "df.drop(columns=['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "import datetime as dt\n",
    "# sample = parse(df['time'].iloc[:,1])\n",
    "# print(sample)\n",
    "sample=parse(df['time'].iloc[12])\n",
    "# # datetime.datetime(2010, 2, 15, 0, 0)\n",
    "# print(dt.strftime('%d/%m/%Y'))\n",
    "# # 15/02/2010\n",
    "sample_time=sample.time()\n",
    "sample_time.strftime('%H:%M:%S.%f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crypto Data#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "crypto=os.path.join(data_only_drive,'crypto/BTCUSD.PERP.BMEX')\n",
    "os.listdir(data_only_drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_path=os.path.join(data_only_drive,'crypto')\n",
    "trades_crypto_path = os.path.join(crypto_path,'trades')\n",
    "perp_trades_loc= os.path.join(trades_crypto_path,'BTCUSD.PERP.BMEX')\n",
    "perp_trades_list=os.listdir(perp_trades_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOB = os.path.join(crypto_path,'LOB')\n",
    "lob_files= os.listdir(os.path.join(LOB,os.listdir(LOB)[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file = pd.read_csv(os.path.join(perp_trades_loc, perp_trades_list[0]), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(perp_trades_loc, perp_trades_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file=sample_file.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_df= pd.to_datetime(sample_file['received_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_df['TradedPrice']= sample_file['price']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file =sample_file.rename(index=str, columns={\"received_at\":\"TradedTime\",\"side\":\"Side\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file['TradedTime']=pd.to_datetime(sample_file['TradedTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file =sample_file.rename(index=str, columns={\"price\":\"TradedPrice\",\"size\":\"Volume\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time(date_time):\n",
    "    timestamp= date_time.strftime('%H:%M:%S.%f')\n",
    "    return timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file['TradedTime']=sample_file['TradedTime'].apply(get_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "res = sample_file.groupby('TradedTime').apply(agg_on_trd_time)\n",
    "res.reset_index(inplace=True)\n",
    "res.loc[:, 'Duration'] = res['TradedTime']- res['TradedTime'].shift(1)\n",
    "res['Duration'].fillna(value=0, inplace=True)\n",
    "res.loc[:, 'Duration'] = res['Duration'].apply(lambda tt: tt.total_seconds())\n",
    "res.loc[1:, 'ReturnTradedPrice'] = \\\n",
    "res['TradedPrice'].rolling(window=2).apply(lambda xx: np.log(xx[-1]/xx[0]))\n",
    "res.loc[0, 'ReturnTradedPrice'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getenv('FINANCE_DATA') #main directory\n",
    "data_only_drive= '/mnt/usb-Seagate_Expansion_Desk_NA8XEHR6-0:0-part2'\n",
    "bmrg_symbols=[s for s in os.listdir(data_dir) if s.endswith('20181028') or s.endswith('20181027')]\n",
    "\n",
    "\n",
    "bmrg_symbols_destinations=[s for s in os.listdir(data_dir) if s.endswith('trades') or s.endswith('quotes')]\n",
    "bmrg_tickers=[bmrg_symbols_destinations[idx].split('_t')[0] or \\\n",
    "              bmrg_symbols_destinations[idx].split('_q')[0] for idx,_ in enumerate(bmrg_symbols_destinations)]\n",
    "ftse_symbols= [s for s in os.listdir(data_dir) if s.endswith('.L')]\n",
    "features_models_dd= os.path.join(data_dir, 'features_models')\n",
    "labels = os.path.join(features_models_dd, 'labels')\n",
    "model_features= os.path.join(features_models_dd, 'features')\n",
    "features_models_dOd= os.path.join(data_only_drive, 'features_models')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#     symbol_features_path = os.path.join(model_features, symbol,'MODEL_BASED') # set model-based features path for the symbol\n",
    "#     symbol_labels_list= os.listdir(symbol_labels_path) # make a list of all the labels\n",
    "#     market_features_path = os.path.join(data_only_drive,'Data','features_models',symbol,'MARKET_BASED')\n",
    "#     features_dates_dir= os.listdir(symbol_features_path)\n",
    "#     market_features_dates= [os.listdir(market_features_path)[idx].split(\".\")[0] for idx, _ in enumerate(market_features_path)]\n",
    "#     labels_dates= [symbol_labels_list[idx].split(\".\")[0] for idx, _ in enumerate(symbol_labels_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir\n",
    "ftse_symbols= [s for s in os.listdir(data_dir) if s.endswith('.L')]\n",
    "test=os.listdir(os.path.join(data_dir, ftse_symbols[1]))\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for idx,_ in enumerate(ftse_symbols):\n",
    "    symbol=ftse_symbols[idx] # set symbol\n",
    "    features_models_path = '/media/ak/WorkDrive/Data/features_models/models/'\n",
    "    features_only_path = '/media/ak/WorkDrive/Data/features_models/features/'\n",
    "    symbol_raw_data_path = os.listdir(os.path.join(data_dir, symbol))\n",
    "    symbol_hmm_model_path = os.listdir(os.path.join(features_models_path, symbol,'HMM'))\n",
    "    symbol_labels_path= os.listdir(os.path.join(labels, symbol,'NON_DIRECTIONAL')) # set labels path for the symbol\n",
    "    market_features_path = os.listdir(os.path.join(data_only_drive,'Data','features_models',symbol,'MARKET_BASED'))\n",
    "    symbol_model_features = os.listdir(os.path.join(features_only_path, symbol, 'MODEL_BASED'))\n",
    "#     print('symbol: ', symbol)\n",
    "# #     print('number of days: ', len(symbol_raw_data_path))\n",
    "# #     print('number of hmm models: ', len(symbol_hmm_model_path))\n",
    "# #     print(\"number of date files in labels:\",len(symbol_labels_path))\n",
    "#     print(\"mearket features:\", len(market_features_path))\n",
    "#     print(\"common elements between market features and labels:\",len(list(common_member(market_features_path, \\\n",
    "#                                                                                        symbol_labels_path))))\n",
    "# #     if len(symbol_raw_data_path)!= len(symbol_labels_path):\n",
    "# #         print('check symbol:',symbol)\n",
    "# #         problem_list.append(symbol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##test case##\n",
    "symbol='AAL.L' # set symbol\n",
    "features_models_path = '/media/ak/WorkDrive/Data/features_models/models/'\n",
    "symbol_raw_data_path = os.path.join(data_dir, symbol)\n",
    "symbol_hmm_model_path = os.path.join(features_models_path, symbol,'HMM')\n",
    "symbol_labels_path= os.path.join(labels, symbol,'NON_DIRECTIONAL') # set labels path for the symbol\n",
    "market_features_path = os.path.join(data_only_drive,'Data','features_models',symbol,'MARKET_BASED')\n",
    "print('symbol: ', symbol)\n",
    "print(symbol_raw_data_path)\n",
    "print(symbol_hmm_model_path)\n",
    "print(symbol_labels_path)\n",
    "print(market_features_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_features = os.listdir(os.path.join(features_only_path, symbol, 'MODEL_BASED')) #this is a list of folders, indexed by date\n",
    "labels_dates= [symbol_labels_path[idx].split(\".\")[0] for idx, _ in enumerate(symbol_labels_path)]\n",
    "date=labels_dates[1]\n",
    "# for _, date in enumerate(labels_dates):\n",
    "date_symbol_features = os.path.join(features_only_path, symbol, 'MODEL_BASED', date)\n",
    "date_feature_list= os.listdir(date_symbol_features)\n",
    "for idx, _ in enumerate(date_feature_list):\n",
    "    open_pickle_file(path=date_symbol_features, pickle_file=date_feature_list[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date\n",
    "date_symbol_features = os.path.join(features_only_path, symbol, 'MODEL_BASED', date)\n",
    "date_symbol_features\n",
    "date_feature_list= os.listdir(date_symbol_features)\n",
    "print date_feature_list\n",
    "for idx, _ in enumerate(date_feature_list):\n",
    "     open_pickle_file(path=date_symbol_features, pickle_file=date_feature_list[idx])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, date  in enumerate(symbol_model_features):\n",
    "#     print os.listdir(os.path.join(features_only_path, symbol, 'MODEL_BASED', date))\n",
    "date_feature_list= os.listdir(date_symbol_features)\n",
    "for idx, _ in enumerate(date_feature_list):\n",
    "    print date_feature_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[symbol_labels_list[idx].split(\".\")[0] for idx, _ in enumerate(symbol_labels_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration=pd.read_csv(file_location,index_col=0)['Duration']\n",
    "non_directional_labels = pd.read_csv(file_location,index_col=0)['label_PrMov__window_25__thres_arbitrary__10.0'] #this is a problem\n",
    "df = pd.read_csv(file_location,index_col=0)\n",
    "features_dates_dir= os.listdir(symbol_features_path)#\n",
    "# path for a specific hmm model date --- out of sample pickle files\n",
    "pickle_features_path= os.path.join(symbol_features_path, features_dates_dir[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(pickle_features_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'III.L_3_states_features_date:_20171024_now:_20181226_\n",
    "compute_date= '20181226'\n",
    "date= features_dates_dir[5]\n",
    "features_pickle_file = \"_\".join((symbol,'3_states_features_date:',features_dates_dir[5],'now:',compute_date,'.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features =open_pickle_file(pickle_features_path,features_pickle_file) #tuple for all the HMM- model features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_features_path = os.path.join(data_only_drive,'Data','features_models',symbol,'MARKET_BASED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_features= os.path.join(market_features_path, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(market_features):\n",
    "    pd.read_csv(market_features)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_features_dates= [os.listdir(market_features_path)[idx].split(\".\")[0] for idx, _ in enumerate(market_features_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_member(a, b): \n",
    "      \n",
    "    a_set = set(a) \n",
    "    b_set = set(b) \n",
    "      \n",
    "    # check length  \n",
    "    if len(a_set.intersection(b_set)) > 0: \n",
    "        return(a_set.intersection(b_set))   \n",
    "    else: \n",
    "        return(\"no common elements\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_elements=list(common_member(features_dates_dir, market_features_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(common_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features_dates_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(market_features_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
