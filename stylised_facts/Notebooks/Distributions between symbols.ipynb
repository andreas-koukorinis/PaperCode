{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import gamma\n",
    "from scipy import signal\n",
    "import time\n",
    "import itertools\n",
    "from itertools import combinations\n",
    "try:\n",
    "    from tqdm import tqdm_notebooks as tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x: x\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "###\n",
    "from collections import defaultdict\n",
    "\n",
    "from scipy.stats import pearsonr, zscore\n",
    "from MFDFA import fgn\n",
    "# Plot settings\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "sns.set()\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/directory/tothe/handshakefile/')\n",
    "\n",
    "from shogun import *\n",
    "from scipy.stats import norm, laplace\n",
    "import random\n",
    "sys.path.append('/home/ak/Research/PaperCode/stylised_facts/hypothesisTesting')\n",
    "sys.path.append('/home/ak/Research/PaperCode/stylised_facts/stylised_facts_data_utilities')\n",
    "import shogunMMDutils as mmdutils\n",
    "\n",
    "# import stylised_facts_data_utilities.createLOB as createLOB\n",
    "# import gpyNARX as gpyNARX\n",
    "#import stylised_facts_data_utilities.longtaildistr as longtail\n",
    "%matplotlib inline\n",
    "\n",
    "# plt.style.use(os.path.join(mpl.get_configdir(),'latexstyle.mplstyle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphicsLocation = '/home/ak/Documents/Research/Papers/StylisedFactsPaper/figures'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shogun_features(x, y):\n",
    "    # create shogun features\n",
    "    return [RealFeatures(x.reshape(1, len(x))), RealFeatures(y.reshape(1, len(y)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes extremities based on two inputs\n",
    "def series_between_percentiles(series, upperValue,lowerValue):\n",
    "    # Find out percentiles and get values in between\n",
    "    lThres = np.percentile(series , lowerValue) # lower percentile\n",
    "    uThres = np.percentile(series , upperValue) # higher percentile\n",
    "    return series[(series > lThres) & (series <uThres)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tableone\n",
      "  Downloading tableone-0.7.12-py3-none-any.whl (32 kB)\n",
      "  Downloading tableone-0.7.11-py3-none-any.whl (32 kB)\n",
      "  Downloading tableone-0.7.10-py2.py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: statsmodels>=0.8.0 in /home/ak/anaconda3/envs/shogun/lib/python3.6/site-packages (from tableone) (0.11.1)\n",
      "Collecting tabulate>=0.8.2\n",
      "  Downloading tabulate-0.8.10-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /home/ak/anaconda3/envs/shogun/lib/python3.6/site-packages (from tableone) (1.0.3)\n",
      "Requirement already satisfied: numpy>=1.12.1 in /home/ak/anaconda3/envs/shogun/lib/python3.6/site-packages (from tableone) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/ak/anaconda3/envs/shogun/lib/python3.6/site-packages (from tableone) (1.1.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ak/anaconda3/envs/shogun/lib/python3.6/site-packages (from pandas>=0.22.0->tableone) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/ak/anaconda3/envs/shogun/lib/python3.6/site-packages (from pandas>=0.22.0->tableone) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ak/anaconda3/envs/shogun/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas>=0.22.0->tableone) (1.16.0)\n",
      "Requirement already satisfied: patsy>=0.5 in /home/ak/anaconda3/envs/shogun/lib/python3.6/site-packages (from statsmodels>=0.8.0->tableone) (0.5.2)\n",
      "Installing collected packages: tabulate, tableone\n",
      "Successfully installed tableone-0.7.10 tabulate-0.8.10\n"
     ]
    }
   ],
   "source": [
    "# import numerical libraries\n",
    "\n",
    "from scipy import stats\n",
    "# import tableone\n",
    "try:\n",
    "    from tableone import TableOne\n",
    "except (ModuleNotFoundError, ImportError):\n",
    "    # install on Colab\n",
    "    !pip install tableone\n",
    "    from tableone import TableOne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions and Investigations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1/ By moving to the tick domain, the need to force each trade into a time slot is removed as one does\n",
    "not need to force the trades into predetermined sampling points as in calendar time. Additionally, when using calendar time sampling\n",
    "\n",
    "2/ can we recover normality in each of the assets and periods?\n",
    "\n",
    "3/duration between trades is\n",
    "also added to the subordination framework to account for\n",
    "the speed with which market participants act in physical\n",
    "time.\n",
    "\n",
    "4/we bring qualitative empirical evidence that the impact of a single\n",
    "trade depends on the intertrade time lags. We find that when the trading rate be- comes faster, the return variance per trade strongly increases and that this behavior persists at coarser time scales. \n",
    "\n",
    "5/So we answer the following question: is the realized variance created by 10 trades arriving over 10 seconds similar to the realized variance created by those very same trades had they arrived during 10 minutes? Any model that uses a transaction time clock implies that the two situations are similar. Our empirical findings show that they are not, and that trades arriving in a shorter duration have higher variance, thus showing the importance of the physical inter-trade time duration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ActivityCLockData= '/media/ak/My Passport/Experiment Data/ActivityClockData/'\n",
    "OHLCData= '/media/ak/My Passport/Experiment Data/OHLCData/'\n",
    "folderList = os.listdir(ActivityCLockData)\n",
    "OHLCDataList = os.listdir(OHLCData)\n",
    "ActivityClockDataList = os.listdir(ActivityCLockData)\n",
    "\n",
    "symbols =['FB1','JB1','FV1','G_1','DU1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JB1ClocksData = np.sort([s for s in os.listdir(ActivityCLockData) if 'Clocks' in s and ('JB1') in s])\n",
    "FV1ClocksData = np.sort([s for s in os.listdir(ActivityCLockData) if 'Clocks' in s and ('FV1') in s])\n",
    "G1ClocksData = np.sort([s for s in os.listdir(ActivityCLockData) if 'Clocks' in s and ('G_1') in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DU1ClocksData = np.sort([s for s in os.listdir(ActivityCLockData) if 'Clocks' in s and ('DU1') in s])\n",
    "FB1ClocksData = np.sort([s for s in os.listdir(ActivityCLockData) if 'Clocks' in s and ('FB1') in s])\n",
    "AllClocksData = [DU1ClocksData, FB1ClocksData, JB1ClocksData, FV1ClocksData, G1ClocksData  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pklidx = 0\n",
    "pklClockFile = \"\".join((ActivityCLockData,DU1ClocksData[pklidx]))\n",
    "pickleBarDF = pickle.load(open(pklClockFile , \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "types_of_bars = ['OriginalDF', 'TickBarDf', 'VolumeBarDf', 'DollarVolumeBarDf']\n",
    "range_list = [x for x in range(0,10)]#list(range(0, 25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pklClockFile = [\"\".join((ActivityCLockData,clocksDataList[pklIdx])) for clocksDataList in AllClocksData for pklIdx in range(0,25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x =clocksDataList[2].split(\"_\")[4]\n",
    "y = clocksDataList[2].split(\"_\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clock_dict = defaultdict(dict)\n",
    "for clocksDataList in AllClocksData :\n",
    "    for pklIdx in range(0,25): \n",
    "        clock_dict[clocksDataList[pklIdx].split(\"_\")[4]][clocksDataList[pklIdx].split(\"_\")[1]]  = \"\".join((ActivityCLockData,clocksDataList[pklIdx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clocksDataList[0].split(\"_\")[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now lets get all the dataframes out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickBar_dfs = [pickle.load(open(pklClockFile , \"rb\"))['TickBarDf'] for pklClockFile in pklClockFile_locations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrames = [pickle.load(open(pklClockFile , \"rb\"))[frame] for pklClockFile in pklClockFile_locations for frame in types_of_bars ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - read from AllClocksData\n",
    "## 2 - read from pklidx\n",
    "## 3 - get pklClockFile\n",
    "## 4 - load picklefile, and extract the dataframe\n",
    "## 5  - take the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pick DU1 First ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "microPriceChanges = [dataFrames[i].MicroPrice.pct_change().dropna() for i in range(0, len(dataFrames))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= microPriceChanges[0].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(series_between_percentiles(test, 0.9, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### do a dict comprehension to create a dictionary with all the date you want\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpc_dict = {i:series_between_percentiles(microPriceChanges[i].dropna(), 0.9, 0.1) for i in range(0,100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_p = signal.resample(mpc_dict[0], 1000)\n",
    "sample_q = signal.resample(mpc_dict[2], 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_p, feat_q = shogun_features(sample_p,sample_q)\n",
    "mmdInstance = mmdutils.SignificanceResultsMMD(sample_p,sample_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_p_2, feat_q_2 = mmdInstance.shogun_features(sample_p,sample_q)\n",
    "# this function needs re-writing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unit-testing my functionality next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmd = QuadraticTimeMMD(feat_p_2, feat_q_2)\n",
    "\n",
    "mmd_2 = mmdInstance.mmd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmd.set_statistic_type(ST_BIASED_FULL)\n",
    "kernel = GaussianKernel(10, 1)\n",
    "mmd.set_kernel(kernel)\n",
    "mmd.compute_statistic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = GaussianKernel(10, 1)\n",
    "mmd.set_kernel(kernel)\n",
    "kernel1 = GaussianKernel(10, 0.1)\n",
    "kernel2 = GaussianKernel(10, 1)\n",
    "kernel3 = GaussianKernel(10, 10)\n",
    "kernel4 = GaussianKernel(5, 0.15)\n",
    "mmd.add_kernel(kernel1)\n",
    "mmd.add_kernel(kernel2)\n",
    "mmd.add_kernel(kernel3)\n",
    "mmd.add_kernel(kernel4)\n",
    "\n",
    "mmd.set_train_test_mode(True)\n",
    "mmd.set_train_test_ratio(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_2 = GaussianKernel(20, 1)\n",
    "mmd_2.set_kernel(kernel)\n",
    "kernel1 = GaussianKernel(10, 0.1)\n",
    "kernel2 = GaussianKernel(10, 1)\n",
    "kernel3 = GaussianKernel(10, 10)\n",
    "kernel4 = GaussianKernel(5, 0.15)\n",
    "mmd_2.add_kernel(kernel1)\n",
    "mmd_2.add_kernel(kernel2)\n",
    "mmd_2.add_kernel(kernel3)\n",
    "mmd_2.add_kernel(kernel4)\n",
    "\n",
    "mmd_2.set_train_test_mode(True)\n",
    "mmd_2.set_train_test_ratio(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1\n",
    "num_folds = 3\n",
    "alpha = 0.05\n",
    "mmd_2.set_kernel_selection_strategy(KSM_CROSS_VALIDATION, num_runs, num_folds, alpha)\n",
    "mmd_2.select_kernel()\n",
    "learnt_kernel_single = GaussianKernel.obtain_from_generic(mmd_2.get_kernel())\n",
    "width = learnt_kernel_single.get_width()\n",
    "print('Print best kernel width is:', width)\n",
    "\n",
    "\n",
    "_, p_value_biased = mmdInstance.compute_the_p_value_biased(mmd=mmd_2, kernel=learnt_kernel_single)\n",
    "_, p_value_unbiased = mmdInstance.compute_the_p_value_unbiased(mmd=mmd_2, kernel=learnt_kernel_single)\n",
    "\n",
    "\n",
    "# mmdutils.SignificanceResultsMMD.test_by_hand(mmd=mmd_2, p_value=p_value_unbiased, alpha=alpha)\n",
    "# mmdutils.SignificanceResultsMMD.full_two_sample(mmd=mmd_2, alpha=alpha)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 200\n",
    "p_values = np.zeros(num_runs)\n",
    "\n",
    "last = time.time()\n",
    "for i in range(num_runs):\n",
    "    feats_p, feats_q = shogun_features(fb1sample,jb1sample)\n",
    "    width=1\n",
    "    k = GaussianKernel(10, width)\n",
    "\n",
    "    mmd = QuadraticTimeMMD()\n",
    "    mmd.set_p(feats_p)\n",
    "    mmd.set_q(feats_q)\n",
    "    mmd.set_kernel(k)\n",
    "\n",
    "    mmd.set_num_null_samples(50)\n",
    "    stat = mmd.compute_statistic() # would be good if compute_p_value() with no arguments computed the statistic itself\n",
    "    p_values[i] = mmd.compute_p_value(stat)\n",
    "\n",
    "# does this look more or less uniform (it has to be)?\n",
    "plt.hist(p_values);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "sns.distplot(p_values, bins =50,color = \"blue\")\n",
    "plt.axvline(x=np.percentile(p_values, 50), c='red',linestyle = 'dotted')\n",
    "legend = [\"median $p-$ value\"]\n",
    "ax.legend(loc='best', frameon=False)\n",
    "_=plt.title(\"Distribution of $p$-values\")\n",
    "_=plt.xlabel(\"$x$\")\n",
    "title= 'DistributionPvaluesJB1DU1.png'\n",
    "plt.savefig(os.path.join(graphicsLocation,title))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import squareform, pdist, cdist\n",
    "\n",
    "def sq_distances(X,Y=None):\n",
    "    \"\"\"\n",
    "    If Y=None, then this computes the distance between X and itself\n",
    "    \"\"\"\n",
    "    assert(X.ndim==2)\n",
    "\n",
    "    # IMPLEMENT: compute pairwise distance matrix. Don't use explicit loops, but the above scipy functions\n",
    "    # if X=Y, use more efficient pdist call which exploits symmetry\n",
    "    if Y is None:\n",
    "        sq_dists = squareform(pdist(X, 'sqeuclidean'))\n",
    "    else:\n",
    "        assert(Y.ndim==2)\n",
    "        assert(X.shape[1]==Y.shape[1])\n",
    "        sq_dists = cdist(X, Y, 'sqeuclidean')\n",
    "\n",
    "    return sq_dists\n",
    "def gauss_kernel(X, Y=None, sigma=1.0):\n",
    "    \"\"\"\n",
    "    Computes the standard Gaussian kernel k(x,y)=exp(- ||x-y||**2 / (2 * sigma**2))\n",
    "\n",
    "    X - 2d array, samples on left hand side\n",
    "    Y - 2d array, samples on right hand side, can be None in which case they are replaced by X\n",
    "    \n",
    "    returns: kernel matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # IMPLEMENT: compute squared distances and kernel matrix\n",
    "    sq_dists = sq_distances(X,Y)\n",
    "    K = np.exp(-sq_dists / (2 * sigma**2))\n",
    "    return K\n",
    "\n",
    "# IMPLEMENT\n",
    "def linear_kernel(X, Y):\n",
    "    return np.dot(X, Y.T)\n",
    "def quadratic_time_mmd(X,Y,kernel):\n",
    "    assert X.ndim == Y.ndim == 2\n",
    "    K_XX = kernel(X,X)\n",
    "    K_XY = kernel(X,Y)\n",
    "    K_YY = kernel(Y,Y)\n",
    "       \n",
    "    n = len(K_XX)\n",
    "    m = len(K_YY)\n",
    "    \n",
    "    # IMPLEMENT: unbiased MMD statistic (could also use biased, doesn't matter if we use permutation tests)\n",
    "    np.fill_diagonal(K_XX, 0)\n",
    "    np.fill_diagonal(K_YY, 0)\n",
    "    mmd = np.sum(K_XX) / (n*(n-1))  + np.sum(K_YY) / (m*(m-1))  - 2*np.sum(K_XY)/(n*m)\n",
    "    return mmd\n",
    "def two_sample_permutation_test(test_statistic, X, Y, num_permutations, prog_bar=True):\n",
    "    assert X.ndim == Y.ndim\n",
    "    \n",
    "    statistics = np.zeros(num_permutations)\n",
    "    \n",
    "    range_ = range(num_permutations)\n",
    "    for i in range_:\n",
    "        # concatenate samples\n",
    "        if X.ndim == 1:\n",
    "            Z = np.hstack((X,Y))\n",
    "        elif X.ndim == 2:\n",
    "            Z = np.vstack((X,Y))\n",
    "            \n",
    "        # IMPLEMENT: permute samples and compute test statistic\n",
    "        perm_inds = np.random.permutation(len(Z))\n",
    "        Z = Z[perm_inds]\n",
    "        X_ = Z[:len(X)]\n",
    "        Y_ = Z[len(X):]\n",
    "        my_test_statistic = test_statistic(X_, Y_)\n",
    "        statistics[i] = my_test_statistic\n",
    "    return statistics\n",
    "def plot_permutation_samples(null_samples, statistic=None, title=None):\n",
    "    #plt.hist(\n",
    "    sns.distplot(null_samples, bins=50, color='grey')\n",
    "    plt.axvline(x=np.percentile(null_samples, 2.5), c='b')\n",
    "    legend = [\"95% quantiles\"]\n",
    "    if statistic is not None:\n",
    "        plt.axvline(x=statistic, c='r', linewidth=3, linestyle='--')\n",
    "        legend += [\"Actual test statistic\"]\n",
    "    plt.legend(legend, loc='best')\n",
    "    plt.axvline(x=np.percentile(null_samples, 97.5), c='b')\n",
    "    plt.xlabel(\"Test statistic value\")\n",
    "    plt.ylabel(\"Counts\")\n",
    "    if title is not None:\n",
    "        plt.savefig(os.path.join(graphicsLocation,title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = (fb1sample,jb1sample)\n",
    "my_kernel = lambda X,Y : gauss_kernel(X,Y,sigma=0.2)\n",
    "my_mmd = lambda X,Y : quadratic_time_mmd(X[:,np.newaxis],Y[:,np.newaxis], my_kernel)\n",
    "num_permutations = 200\n",
    "statistics = two_sample_permutation_test(my_mmd, X, Y, num_permutations)\n",
    "my_statistic = my_mmd(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.linspace(np.min(X), np.max(X))\n",
    "bins=150\n",
    "my_kernel = lambda X,Y : gauss_kernel(X,Y, sigma=0.5)\n",
    "#sns.distplot(X, bins =bins, color = \"skyblue\", label =\"FB1\")\n",
    "sns.distplot(Y, bins =bins,color = \"red\", label =\"JB1\")\n",
    "# plt.title(\"Witness function\")\n",
    "# # IMPLEMENT: evaluate MMD witness function on grid\n",
    "# phi_X = np.mean(my_kernel(X[:,np.newaxis], grid[:,np.newaxis]), axis=0)\n",
    "# phi_Y = np.mean(my_kernel(Y[:,np.newaxis], grid[:,np.newaxis]), axis=0)\n",
    "# witness = phi_X-phi_Y\n",
    "# plt.xlabel(\"$x$\")\n",
    "# plt.ylabel(\"$p(x)$\")\n",
    "# _=plt.legend([ 'FB1','JB1'])\n",
    "# plt.plot(grid, witness, linestyle ='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
