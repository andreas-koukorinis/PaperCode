{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "753aff55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import freqopttest.util as util\n",
    "import freqopttest.data as data\n",
    "import freqopttest.kernel as kernel\n",
    "import freqopttest.tst as tst\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "import freqopttest.glo as glo\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7c735bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ak/T71/August11th2022Experiments/experimentOne/FB1 True\n"
     ]
    }
   ],
   "source": [
    "file = pd.read_pickle('/media/ak/T7/August11th2022Experiments/LinearMMDInputFiles/G_1_calendar_mfdfa_dicts_all_dates.pkl')\n",
    "variables = ['n_F', 'list_H', 'list_H_intercept', 'tau', 'alpha', 'mfSpect']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88cc0592",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/ak/T71/August11th2022Experiments/mfdfaDataFrames/JB1_calendar_alpha.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-70431baa4033>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmfdfaDataFrames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'JB1_calendar_alpha.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/newTimeSeries/lib/python3.6/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"infer\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# 1) try standard library Pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/newTimeSeries/lib/python3.6/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/ak/T71/August11th2022Experiments/mfdfaDataFrames/JB1_calendar_alpha.pkl'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d47c822",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len = unpickled_Df.shape[1]\n",
    "print(len)\n",
    "test_results_one_dict = defaultdict(dict)\n",
    "test_results_two_dict = defaultdict(dict)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if __name__ == '__main__':\n",
    "        shift = 1\n",
    "        window = 5  # slide\n",
    "        for start_point in range(0, (len - window - shift)):\n",
    "            end_point = start_point + shift\n",
    "\n",
    "            X = np.array(unpickled_Df.iloc[:, start_point:end_point])\n",
    "            Y = np.array(unpickled_Df.iloc[:, end_point + shift:end_point + 2 * shift])\n",
    "            Z = np.array(unpickled_Df.iloc[:, start_point + window:end_point + window])\n",
    "\n",
    "            data_sample = data.TSTData(X, Y)  # data to train the model\n",
    "            test_data_one = data_sample = data.TSTData(X, Z)\n",
    "            test_data_two = data_sample = data.TSTData(Y, Z)\n",
    "\n",
    "            tr, te = data_sample.split_tr_te(tr_proportion=0.9, seed=100)\n",
    "\n",
    "            # choose the best kernel that maximizes the test power\n",
    "            med = util.meddistance(tr.stack_xy())\n",
    "\n",
    "            widths = [(med * f) for f in 2.0 ** np.linspace(-1, 4, 25)]\n",
    "            list_kernels = [kernel.KGauss(w ** 2) for w in widths]\n",
    "\n",
    "            besti, powers = tst.LinearMMDTest.grid_search_kernel(tr, list_kernels, alpha=0.01)\n",
    "            # print(list_kernels)\n",
    "            plt.plot(widths, powers, 'o-')\n",
    "            plt.xlabel('Gaussian width')\n",
    "            plt.ylabel('test power')\n",
    "            plt.title('median distance = %.3g. Best width: %.3g' % (med, widths[besti]))\n",
    "            plt.show()\n",
    "            test_data_one = data_sample = data.TSTData(X, Z)\n",
    "            test_data_two = data_sample = data.TSTData(Y, Z)\n",
    "\n",
    "            # The actual test\n",
    "            best_ker = list_kernels[besti]\n",
    "            lin_mmd_test = tst.LinearMMDTest(best_ker, alpha=0.01)\n",
    "            # test_result_one = lin_mmd_test.perform_test(test_data_one)\n",
    "            # test_result_two = lin_mmd_test.perform_test(test_data_two)\n",
    "\n",
    "            # Test 1 Results Dict\n",
    "            test_results_one_dict[start_point]['widths'] = widths\n",
    "            test_results_one_dict[start_point]['med'] = med\n",
    "            test_results_one_dict[start_point]['widths'] = widths\n",
    "            test_results_one_dict[start_point]['besti'] = besti\n",
    "            test_results_one_dict[start_point]['powers'] = powers\n",
    "\n",
    "            test_results_one_dict[start_point]['med_on_test_data'] = util.meddistance(test_data_one.stack_xy())\n",
    "            test_results_one_dict[start_point]['test_result'] = lin_mmd_test.perform_test(test_data_one)\n",
    "            test_results_one_dict[start_point]['test_variance'] = lin_mmd_test.variance(X, Z, best_ker)  # test variance\n",
    "            test_results_one_dict[start_point]['two_moments'] = lin_mmd_test.two_moments(X, Z,\n",
    "                                                                                         best_ker)  # test variance\n",
    "            test_results_one_dict[start_point]['compute_unbiased_linear_estimator'] = lin_mmd_test.compute_stat(\n",
    "                test_data_one)\n",
    "\n",
    "            # Test 2 Results Dict\n",
    "            test_results_two_dict[start_point]['test_result'] = lin_mmd_test.perform_test(test_data_two)\n",
    "            test_results_two_dict[start_point]['test_variance'] = lin_mmd_test.variance(Y, Z, best_ker)\n",
    "            test_results_two_dict[start_point]['med_on_test_data'] = util.meddistance(\n",
    "                test_data_two.stack_xy())  # test variance\n",
    "            test_results_two_dict[start_point]['two_moments'] = lin_mmd_test.two_moments(Y, Z,\n",
    "                                                                                         best_ker)  # test variance\n",
    "            test_results_two_dict[start_point]['compute_unbiased_linear_estimator'] = lin_mmd_test.compute_stat(\n",
    "                test_data_two)\n",
    "\n",
    "            # need a better hash file below\n",
    "\n",
    "        pickle_out_dict_one = os.path.join(LinearMMDOutputFiles, \"\".join(\n",
    "            (str(symbol) + \"_\" + str(variable) + \"_shift_\" + str(shift) + \"_wind_\" + str(window) + \"_\" + str(\n",
    "                'linear_test') + \"_ONE.pkl\")))\n",
    "        pickle.dump(test_results_one_dict, open(pickle_out_dict_one, 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        pickle_out_dict_two = os.path.join(LinearMMDOutputFiles, \"\".join(\n",
    "            (str(symbol) + \"_\" + str(variable) + \"_shift_\" + str(shift) + \"_wind_\" + str(window) + \"_\" + str(\n",
    "                'linear_test') + \"_TWO.pkl\")))\n",
    "        pickle.dump(test_results_two_dict, open(pickle_out_dict_two, 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        print(test_results_one_dict)\n",
    "\n",
    "    # print(test_results_one_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc73b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "def analyze_column(start_point, end_point, shift, window):\n",
    "    mmd_tester = MMDTester()\n",
    "    try:\n",
    "        test_results_one, test_results_two = mmd_tester.analyze(start_point, end_point, shift, window)\n",
    "        col1, col2 = unpickled_df.columns[start_point], unpickled_df.columns[end_point + shift]\n",
    "        result_key = f\"{col1} vs {col2}\"\n",
    "        if result_key not in results_dict:\n",
    "            results_dict[result_key] = {}\n",
    "        if window not in results_dict[result_key]:\n",
    "            results_dict[result_key][window] = {}\n",
    "        results_dict[result_key][window][shift] = (test_results_one, test_results_two)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "# Generate all possible combinations of column pairs, window sizes, and shifts\n",
    "column_pairs = list(combinations(unpickled_dataframe.columns, 2))\n",
    "windows = range(5, 201, 5)\n",
    "shifts = range(1, 11)\n",
    "\n",
    "# Create a list of arguments for the analyze_column function\n",
    "args_list = []\n",
    "for start_point, end_point in combinations(range(len(unpickled_dataframe.columns)), 2):\n",
    "    for window in windows:\n",
    "        for shift in shifts:\n",
    "            args_list.append((start_point, end_point, shift, window))\n",
    "\n",
    "# Use multiprocessing to analyze the data in parallel\n",
    "with Pool(processes=cpu_count()) as pool:\n",
    "    pool.starmap(analyze_column, args_list)\n",
    "\n",
    "# Convert the results dictionary to a DataFrame\n",
    "result_dict = {}\n",
    "for col_pair, window_dict in results_dict.items():\n",
    "    for window, shift_dict in window_dict.items():\n",
    "        for shift, test_results in shift_dict.items():\n",
    "            result_key = f\"{col_pair}, window={window}, shift={shift}\"\n",
    "            result_dict[result_key] = test_results\n",
    "result_df = pd.DataFrame.from_dict(result_dict, orient='index', columns=['Test Results 1', 'Test Results 2'])\n",
    "\n",
    "# # Save the results to a CSV file\n",
    "# result_df.to_csv('test_results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2417685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f91ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:newTimeSeries] *",
   "language": "python",
   "name": "conda-env-newTimeSeries-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
