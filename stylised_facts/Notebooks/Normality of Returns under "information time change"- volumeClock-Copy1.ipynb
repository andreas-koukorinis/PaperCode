{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import gamma\n",
    "from scipy import signal\n",
    "from scipy import stats\n",
    "import time\n",
    "try:\n",
    "    from tqdm import tqdm_notebooks as tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x: x\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "###\n",
    "\n",
    "from scipy.stats import pearsonr, zscore\n",
    "from MFDFA import fgn\n",
    "# Plot settings\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "sns.set()\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/directory/tothe/handshakefile/')\n",
    "\n",
    "from shogun import *\n",
    "from scipy.stats import norm, laplace\n",
    "import random\n",
    "import hypothesisTesting.shogunMMDutils as mmdutils\n",
    "import hypothesisTesting.kernel2sampletest as k2st\n",
    "\n",
    "# import stylised_facts_data_utilities.createLOB as createLOB\n",
    "import stylised_facts_data_utilities.gpyNARX as gpyNARX\n",
    "import stylised_facts_data_utilities.longtaildistr as longtail\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use(os.path.join(mpl.get_configdir(),'latexstyle.mplstyle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shogun_features(x, y):\n",
    "    # create shogun features\n",
    "    return [RealFeatures(x.reshape(1, len(x))), RealFeatures(y.reshape(1, len(y)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numerical libraries\n",
    "\n",
    "from scipy import stats\n",
    "# import tableone\n",
    "try:\n",
    "    from tableone import TableOne\n",
    "except (ModuleNotFoundError, ImportError):\n",
    "    # install on Colab\n",
    "    !pip install tableone\n",
    "    from tableone import TableOne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions and Investigations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1/ By moving to the tick domain, the need to force each trade into a time slot is removed as one does\n",
    "not need to force the trades into predetermined sampling points as in calendar time. Additionally, when using calendar time sampling\n",
    "\n",
    "2/ can we recover normality in each of the assets and periods?\n",
    "\n",
    "3/duration between trades is\n",
    "also added to the subordination framework to account for\n",
    "the speed with which market participants act in physical\n",
    "time.\n",
    "\n",
    "4/we bring qualitative empirical evidence that the impact of a single\n",
    "trade depends on the intertrade time lags. We find that when the trading rate be- comes faster, the return variance per trade strongly increases and that this behavior persists at coarser time scales. \n",
    "\n",
    "5/So we answer the following question: is the realized variance created by 10 trades arriving over 10 seconds similar to the realized variance created by those very same trades had they arrived during 10 minutes? Any model that uses a transaction time clock implies that the two situations are similar. Our empirical findings show that they are not, and that trades arriving in a shorter duration have higher variance, thus showing the importance of the physical inter-trade time duration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ActivityCLockData= '/media/ak/My Passport/Experiment Data/ActivityClockData/'\n",
    "OHLCData= '/media/ak/My Passport/Experiment Data/OHLCData/'\n",
    "folderList = os.listdir(ActivityCLockData)\n",
    "OHLCDataList = os.listdir(OHLCData)\n",
    "ActivityClockDataList = os.listdir(ActivityCLockData)\n",
    "\n",
    "symbols =['FB1','JB1','FV1','G_1','DU1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol ='JB1'\n",
    "ClocksData = np.sort([s for s in os.listdir(ActivityCLockData) if 'Clocks' in s and (str('JB1')) in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DU1ClocksData = np.sort([s for s in os.listdir(ActivityCLockData) if 'Clocks' in s and ('DU1') in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pklidx =1\n",
    "pklClockFile = \"\".join((ActivityCLockData,ClocksData[pklidx]))\n",
    "allDfs = pickle.load(open(pklClockFile , \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['OriginalDF', 'TickBarDf', 'VolumeBarDf', 'DollarVolumeBarDf'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allDfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/ak/anaconda3/envs/ds3_kernels/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning:\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "df =pd.DataFrame(allDfs[list(allDfs.keys())[1]]) #TickBar\n",
    "dfLOB =df[['BidSize', 'QuoteTime', 'BestBid',\n",
    "       'TradeTime', 'AskSize', \n",
    "       'BestAsk', 'TradeVolume', 'TradedTime', 'type',\n",
    "       'TradePrice', 'TimeStamp', 'milliSeconds',\n",
    "       'DollarVolume', 'MicroPrice', 'TradeSize', 'DollarVolumeTraded']] #Pick Only the Frames you want\n",
    "dfLOB['MicroPriceChange'] =  dfLOB.MicroPrice.pct_change()\n",
    "date = ClocksData[pklidx].split('_')[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(allDfs.keys())[1]\n",
    "from collections import defaultdict\n",
    "d = defaultdict(dict)\n",
    "d[str(symbol)][date] = dfLOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickBarDict = defaultdict(dict)\n",
    "volumeBarDict = defaultdict(dict)\n",
    "dollarVolumeBarDict = defaultdict(dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol ='JB1'\n",
    "ClocksData = np.sort([s for s in os.listdir(ActivityCLockData) if 'Clocks' in s and (str('JB1')) in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FB1\n",
      "40\n",
      "JB1\n",
      "40\n",
      "FV1\n",
      "40\n",
      "G_1\n",
      "40\n",
      "DU1\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "tickBarDict = defaultdict(dict)\n",
    "volumeBarDict = defaultdict(dict)\n",
    "dollarVolumeBarDict = defaultdict(dict)\n",
    "for symbol in symbols:\n",
    "    print(symbol)\n",
    "    ClocksData = np.sort([s for s in os.listdir(ActivityCLockData) if 'Clocks' in s and (str('JB1')) in s])\n",
    "    print(len(ClocksData))\n",
    "    for pklidx in range(0, len(ClocksData)-1):\n",
    "        pklClockFile = \"\".join((ActivityCLockData,ClocksData[pklidx]))        \n",
    "        date = ClocksData[pklidx].split('_')[3]\n",
    "        tickBarDict[symbol][date] = pickle.load(open(pklClockFile , \"rb\"))['TickBarDf']\n",
    "        tickBarDict[symbol][date]['MicroPriceChange'] =  tickBarDict[symbol][date].MicroPrice.pct_change()\n",
    "        volumeBarDict[symbol][date] = pickle.load(open(pklClockFile , \"rb\"))['VolumeBarDf']\n",
    "        volumeBarDict[symbol][date]['MicroPriceChange'] =  volumeBarDict[symbol][date].MicroPrice.pct_change()\n",
    "        dollarVolumeBarDict[symbol][date] = pickle.load(open(pklClockFile , \"rb\"))['DollarVolumeBarDf']\n",
    "        dollarVolumeBarDict[symbol][date]['MicroPriceChange'] =  dollarVolumeBarDict[symbol][date].MicroPrice.pct_change()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDfs =defaultdict(dict)\n",
    "for key in list(dollarVolumeBarDict.keys()):\n",
    "    allDfs['dollarVolume'][key] = pd.concat(list(dollarVolumeBarDict[key].values() ), axis =0, keys = list(dollarVolumeBarDict[key].keys()))\n",
    "    allDfs['volume'][key] = pd.concat(list(volumeBarDict[key].values() ), axis =0, keys = list(volumeBarDict[key].keys()))\n",
    "    allDfs['tick'][key] = pd.concat(list(tickBarDict[key].values() ), axis =0, keys = list(tickBarDict[key].keys()))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micropr_calculation(df):\n",
    "    df['MicroPrice'] = df.MicroPrice.pct_change()\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphicsLocation = '/home/ak/Documents/Research/Papers/StylisedFactsPaper/figures'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## steps ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tickBarDictCondensedDU1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f33586ad5f9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# keep track of the keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mkeysDU1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtickBarDictCondensedDU1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# concatenate all the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mallDU1\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtickBarDictCondensedDU1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtickBarDictCondensedDU1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tickBarDictCondensedDU1' is not defined"
     ]
    }
   ],
   "source": [
    "# keep track of the keys\n",
    "keysDU1 = list(tickBarDictCondensedDU1.keys()) \n",
    "# concatenate all the data\n",
    "allDU1 =pd.concat(list(tickBarDictCondensedDU1.values() ), axis =0, keys = list(tickBarDictCondensedDU1.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medianMicroPrice_DU1 = allDU1.median(level=1).MicroPriceChange.dropna()\n",
    "medianMicroPrice_DU1_acrossDays = allDU1.median(level=0).MicroPrice.dropna()\n",
    "# produce medians across time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kick out the tails\n",
    "DU1medianMPChange =k2st.SeriesBetweenPercentiles(series= medianMicroPrice_DU1 , upperValue= 95, lowerValue=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take out one sample\n",
    "du1sample = signal.resample(DU1medianMPChange, 5000)\n",
    "plt.plot(du1sample)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.average(DU1medianMPChange)\n",
    "sigma2 = np.var(DU1medianMPChange)\n",
    "print(mu, sigma2)\n",
    "normalDU1=norm.rvs(size=n, loc=mu, scale=sigma2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a normal distribution using the same \n",
    "fig, ax = plt.subplots(1, 1)\n",
    "n= 5000\n",
    "# calculate pdf over a range of values\n",
    "xx = np.arange(np.min(DU1medianMPChange), np.max(DU1medianMPChange), 0.001)\n",
    "yy = stats.norm.pdf(xx)\n",
    "plt.hist(DU1medianMPChange, density=True, histtype='stepfilled', alpha=0.2, bins=50)\n",
    "\n",
    "plt.plot(xx, yy, 'r',lw=2)\n",
    "_=plt.legend([ 'median '])\n",
    "plt.axvline(x=np.percentile(du1sample, 50), c='blue',linestyle = 'dotted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(du1sample, kde=False, fit=stats.norm, bins=100)\n",
    "xx = np.arange(np.min(DU1medianMPChange), np.max(DU1medianMPChange), 0.001)\n",
    "yy = stats.norm.pdf(xx)\n",
    "plt.title(\"Density of DU1 data\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$p(x)$\")\n",
    "_=plt.legend([ 'Normal'])\n",
    "plt.axvline(x=np.percentile(du1sample, 50), c='blue',linestyle = 'dotted')\n",
    "title= 'VolumeClockNormalFit'+'DU1'+'.png'\n",
    "plt.savefig(os.path.join(graphicsLocation,title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.gofplots import qqplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqplot(DU1medianMPChange, line='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pick FB1 second ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbol_choice_2='FB1'\n",
    "# FB1ClocksData = np.sort([s for s in os.listdir(ActivityCLockData) if 'Clocks' in s and (symbol_choice_2) in s])\n",
    "# pklidx=1\n",
    "# tickBarDictCondensedFB1 = dict()\n",
    "# for pklidx in range(0,25):\n",
    "#     pklClockFile = \"\".join((ActivityCLockData,FB1ClocksData[pklidx]))\n",
    "#     tickBarDF =pickle.load(open(pklClockFile , \"rb\"))['TickBarDf'] #tickbar\n",
    "#     tickBarDF['MicroPriceChange'] = tickBarDF.MicroPrice.pct_change()\n",
    "#     tickBarDictCondensedFB1[FB1ClocksData[pklidx].split('_')[3]]=tickBarDF [['BidSize', 'QuoteTime','BestBid',\n",
    "#        'TradeTime', 'AskSize','BestAsk', 'TradeVolume', 'TradedTime', 'type',\n",
    "#        'TradePrice', 'TimeStamp', 'milliSeconds','DollarVolume', 'MicroPrice', 'MicroPriceChange','TradeSize', 'DollarVolumeTraded']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                           \n",
    "keysFB1 = list(tickBarDictCondensedFB1.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concatenate all the dictionaries into one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "allFB1 =pd.concat(list(tickBarDictCondensedFB1.values() ), axis =0, keys = list(tickBarDictCondensedFB1.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write a piece of code that essentially looks at returns across all the clocks and then assesses normality of returns. do we recover normality using different clocks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute median prices properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "medianMicroPrice_FB1 = allFB1.median(level=1).MicroPriceChange.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "medianMicroPrice_FB1_acrossDays = allFB1.median(level=0).MicroPrice.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(medianMicroPrice_FB1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove extremities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SeriesBetweenPercentiles(series, upperValue,lowerValue):\n",
    "    # Find out percentiles and get values in between\n",
    "    lThres = np.percentile(series , lowerValue) # lower percentile\n",
    "    uThres = np.percentile(series , upperValue) # higher percentile\n",
    "    return series[(series > lThres) & (series <uThres)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FB1medianMPChange =SeriesBetweenPercentiles(series= medianMicroPrice_FB1 , upperValue= 95, lowerValue=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "symbol_choice= 'DU1'\n",
    "plt.hist(DU1medianMPChange, color='grey', density=True, histtype='stepfilled', alpha=0.3, bins=30)\n",
    "plt.axvline(x=np.percentile(DU1medianMPChange, 50), c='red',linestyle = 'dotted')\n",
    "legend = [symbol_choice+\" 50% quantile\"]\n",
    "ax.legend(loc='best', frameon=False)\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "plt.title(\"Densities of microprice returns\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$p(x)$\")\n",
    "plt.legend(legend, loc='best')\n",
    "title= 'DensityMicroPrice'+str(symbol_choice)+'.png'\n",
    "plt.savefig(os.path.join(graphicsLocation,title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du1sample = signal.resample(DU1medianMPChange, 1000)\n",
    "fb1sample = signal.resample(FB1medianMPChange, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_p, feat_q = shogun_features(fb1sample,du1sample)\n",
    "mmdInstance = mmdutils.SignificanceResultsMMD(fb1sample,du1sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_p_2, feat_q_2 = mmdInstance.shogun_features(fb1sample,du1sample)\n",
    "# this function needs re-writing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unit-testing my functionality next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmd = QuadraticTimeMMD(feat_p_2, feat_q_2)\n",
    "\n",
    "mmd_2 = mmdInstance.mmd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmd.set_statistic_type(ST_BIASED_FULL)\n",
    "kernel = GaussianKernel(10, 1)\n",
    "mmd.set_kernel(kernel)\n",
    "mmd.compute_statistic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = GaussianKernel(10, 1)\n",
    "mmd.set_kernel(kernel)\n",
    "kernel1 = GaussianKernel(10, 0.1)\n",
    "kernel2 = GaussianKernel(10, 1)\n",
    "kernel3 = GaussianKernel(10, 10)\n",
    "kernel4 = GaussianKernel(5, 0.15)\n",
    "mmd.add_kernel(kernel1)\n",
    "mmd.add_kernel(kernel2)\n",
    "mmd.add_kernel(kernel3)\n",
    "mmd.add_kernel(kernel4)\n",
    "\n",
    "mmd.set_train_test_mode(True)\n",
    "mmd.set_train_test_ratio(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_2 = GaussianKernel(20, 1)\n",
    "mmd_2.set_kernel(kernel)\n",
    "kernel1 = GaussianKernel(10, 0.1)\n",
    "kernel2 = GaussianKernel(10, 1)\n",
    "kernel3 = GaussianKernel(10, 10)\n",
    "kernel4 = GaussianKernel(5, 0.15)\n",
    "mmd_2.add_kernel(kernel1)\n",
    "mmd_2.add_kernel(kernel2)\n",
    "mmd_2.add_kernel(kernel3)\n",
    "mmd_2.add_kernel(kernel4)\n",
    "\n",
    "mmd_2.set_train_test_mode(True)\n",
    "mmd_2.set_train_test_ratio(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1\n",
    "num_folds = 3\n",
    "alpha = 0.05\n",
    "mmd_2.set_kernel_selection_strategy(KSM_CROSS_VALIDATION, num_runs, num_folds, alpha)\n",
    "mmd_2.select_kernel()\n",
    "learnt_kernel_single = GaussianKernel.obtain_from_generic(mmd_2.get_kernel())\n",
    "width = learnt_kernel_single.get_width()\n",
    "print('Print best kernel width is:', width)\n",
    "\n",
    "\n",
    "_, p_value_biased = mmdInstance.compute_the_p_value_biased(mmd=mmd_2, kernel=learnt_kernel_single)\n",
    "_, p_value_unbiased = mmdInstance.compute_the_p_value_unbiased(mmd=mmd_2, kernel=learnt_kernel_single)\n",
    "\n",
    "\n",
    "# mmdutils.SignificanceResultsMMD.test_by_hand(mmd=mmd_2, p_value=p_value_unbiased, alpha=alpha)\n",
    "# mmdutils.SignificanceResultsMMD.full_two_sample(mmd=mmd_2, alpha=alpha)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 200\n",
    "p_values = np.zeros(num_runs)\n",
    "\n",
    "last = time.time()\n",
    "for i in range(num_runs):\n",
    "    feats_p, feats_q = shogun_features(fb1sample,jb1sample)\n",
    "    width=1\n",
    "    k = GaussianKernel(10, width)\n",
    "\n",
    "    mmd = QuadraticTimeMMD()\n",
    "    mmd.set_p(feats_p)\n",
    "    mmd.set_q(feats_q)\n",
    "    mmd.set_kernel(k)\n",
    "\n",
    "    mmd.set_num_null_samples(50)\n",
    "    stat = mmd.compute_statistic() # would be good if compute_p_value() with no arguments computed the statistic itself\n",
    "    p_values[i] = mmd.compute_p_value(stat)\n",
    "\n",
    "# does this look more or less uniform (it has to be)?\n",
    "plt.hist(p_values);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "sns.distplot(p_values, bins =50,color = \"blue\")\n",
    "plt.axvline(x=np.percentile(p_values, 50), c='red',linestyle = 'dotted')\n",
    "legend = [\"median $p-$ value\"]\n",
    "ax.legend(loc='best', frameon=False)\n",
    "_=plt.title(\"Distribution of $p$-values\")\n",
    "_=plt.xlabel(\"$x$\")\n",
    "title= 'DistributionPvaluesJB1DU1.png'\n",
    "plt.savefig(os.path.join(graphicsLocation,title))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import squareform, pdist, cdist\n",
    "\n",
    "def sq_distances(X,Y=None):\n",
    "    \"\"\"\n",
    "    If Y=None, then this computes the distance between X and itself\n",
    "    \"\"\"\n",
    "    assert(X.ndim==2)\n",
    "\n",
    "    # IMPLEMENT: compute pairwise distance matrix. Don't use explicit loops, but the above scipy functions\n",
    "    # if X=Y, use more efficient pdist call which exploits symmetry\n",
    "    if Y is None:\n",
    "        sq_dists = squareform(pdist(X, 'sqeuclidean'))\n",
    "    else:\n",
    "        assert(Y.ndim==2)\n",
    "        assert(X.shape[1]==Y.shape[1])\n",
    "        sq_dists = cdist(X, Y, 'sqeuclidean')\n",
    "\n",
    "    return sq_dists\n",
    "def gauss_kernel(X, Y=None, sigma=1.0):\n",
    "    \"\"\"\n",
    "    Computes the standard Gaussian kernel k(x,y)=exp(- ||x-y||**2 / (2 * sigma**2))\n",
    "\n",
    "    X - 2d array, samples on left hand side\n",
    "    Y - 2d array, samples on right hand side, can be None in which case they are replaced by X\n",
    "    \n",
    "    returns: kernel matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # IMPLEMENT: compute squared distances and kernel matrix\n",
    "    sq_dists = sq_distances(X,Y)\n",
    "    K = np.exp(-sq_dists / (2 * sigma**2))\n",
    "    return K\n",
    "\n",
    "# IMPLEMENT\n",
    "def linear_kernel(X, Y):\n",
    "    return np.dot(X, Y.T)\n",
    "def quadratic_time_mmd(X,Y,kernel):\n",
    "    assert X.ndim == Y.ndim == 2\n",
    "    K_XX = kernel(X,X)\n",
    "    K_XY = kernel(X,Y)\n",
    "    K_YY = kernel(Y,Y)\n",
    "       \n",
    "    n = len(K_XX)\n",
    "    m = len(K_YY)\n",
    "    \n",
    "    # IMPLEMENT: unbiased MMD statistic (could also use biased, doesn't matter if we use permutation tests)\n",
    "    np.fill_diagonal(K_XX, 0)\n",
    "    np.fill_diagonal(K_YY, 0)\n",
    "    mmd = np.sum(K_XX) / (n*(n-1))  + np.sum(K_YY) / (m*(m-1))  - 2*np.sum(K_XY)/(n*m)\n",
    "    return mmd\n",
    "def two_sample_permutation_test(test_statistic, X, Y, num_permutations, prog_bar=True):\n",
    "    assert X.ndim == Y.ndim\n",
    "    \n",
    "    statistics = np.zeros(num_permutations)\n",
    "    \n",
    "    range_ = range(num_permutations)\n",
    "    for i in range_:\n",
    "        # concatenate samples\n",
    "        if X.ndim == 1:\n",
    "            Z = np.hstack((X,Y))\n",
    "        elif X.ndim == 2:\n",
    "            Z = np.vstack((X,Y))\n",
    "            \n",
    "        # IMPLEMENT: permute samples and compute test statistic\n",
    "        perm_inds = np.random.permutation(len(Z))\n",
    "        Z = Z[perm_inds]\n",
    "        X_ = Z[:len(X)]\n",
    "        Y_ = Z[len(X):]\n",
    "        my_test_statistic = test_statistic(X_, Y_)\n",
    "        statistics[i] = my_test_statistic\n",
    "    return statistics\n",
    "def plot_permutation_samples(null_samples, statistic=None, title=None):\n",
    "    #plt.hist(\n",
    "    sns.distplot(null_samples, bins=50, color='grey')\n",
    "    plt.axvline(x=np.percentile(null_samples, 2.5), c='b')\n",
    "    legend = [\"95% quantiles\"]\n",
    "    if statistic is not None:\n",
    "        plt.axvline(x=statistic, c='r', linewidth=3, linestyle='--')\n",
    "        legend += [\"Actual test statistic\"]\n",
    "    plt.legend(legend, loc='best')\n",
    "    plt.axvline(x=np.percentile(null_samples, 97.5), c='b')\n",
    "    plt.xlabel(\"Test statistic value\")\n",
    "    plt.ylabel(\"Counts\")\n",
    "    if title is not None:\n",
    "        plt.savefig(os.path.join(graphicsLocation,title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = (fb1sample,jb1sample)\n",
    "my_kernel = lambda X,Y : gauss_kernel(X,Y,sigma=0.2)\n",
    "my_mmd = lambda X,Y : quadratic_time_mmd(X[:,np.newaxis],Y[:,np.newaxis], my_kernel)\n",
    "num_permutations = 200\n",
    "statistics = two_sample_permutation_test(my_mmd, X, Y, num_permutations)\n",
    "my_statistic = my_mmd(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.linspace(np.min(X), np.max(X))\n",
    "bins=150\n",
    "my_kernel = lambda X,Y : gauss_kernel(X,Y, sigma=0.5)\n",
    "#sns.distplot(X, bins =bins, color = \"skyblue\", label =\"FB1\")\n",
    "sns.distplot(Y, bins =bins,color = \"red\", label =\"JB1\")\n",
    "# plt.title(\"Witness function\")\n",
    "# # IMPLEMENT: evaluate MMD witness function on grid\n",
    "# phi_X = np.mean(my_kernel(X[:,np.newaxis], grid[:,np.newaxis]), axis=0)\n",
    "# phi_Y = np.mean(my_kernel(Y[:,np.newaxis], grid[:,np.newaxis]), axis=0)\n",
    "# witness = phi_X-phi_Y\n",
    "# plt.xlabel(\"$x$\")\n",
    "# plt.ylabel(\"$p(x)$\")\n",
    "# _=plt.legend([ 'FB1','JB1'])\n",
    "# plt.plot(grid, witness, linestyle ='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
