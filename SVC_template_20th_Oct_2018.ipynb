{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime\n",
    "import sys\n",
    "import scipy.stats as stats\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn import linear_model, decomposition\n",
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.svm import SVR\n",
    "# from sklearn.metrics import precision_recall_curve\n",
    "# from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "# from sklearn.metrics import classification_report\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.multiclass import OneVsRestClassifier\n",
    "# from sklearn import decomposition\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn import svm\n",
    "# from sklearn.grid_search import GridSearchCV\n",
    "# from sklearn.externals import joblib\n",
    "# Added version check for recent scikit-learn 0.18 checksok ca\n",
    "from distutils.version import LooseVersion as Version\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "from sklearn.svm import SVC # \"Support Vector Classifier\"\n",
    "# from sklearn.decomposition import FastICA\n",
    "# from sklearn.feature_selection import SelectKBest\n",
    "# from sklearn.feature_selection import f_regression\n",
    "# from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "# from sklearn.cross_validation import KFold\n",
    "# from sklearn.cross_validation import cross_val_score\n",
    "# from sklearn import preprocessing\n",
    "\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "from math import sqrt\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#### ticker ####\n",
    "ticker = 'test_SYNT_2states' #testing a new synthetic ticker\n",
    "# paths\n",
    "data_dir = os.getenv('FINANCE_DATA') #main directory\n",
    "features_path = '/home/ak/Data/features_models/features/' #where features are saved\n",
    "labels_path = '/home/ak/Data/features_models/labels' #where labels are saved\n",
    "ticker_labels_path = os.path.join(labels_path, ticker + '/NON_DIRECTIONAL')\n",
    "main_path = '/home/ak/Data/features_models/'\n",
    "models_path = os.path.join(main_path, 'models')\n",
    "ticker_models_path = os.path.join(models_path, ticker)\n",
    "\n",
    "#### predictions path ####\n",
    "###functions##'\n",
    "\n",
    "def fwd_dates(_dates_list, _key_date):\n",
    "    # returns a list of dates that are forward from the key_date\n",
    "    fwd_dates_list = [i for i in _dates_list if i > _key_date]\n",
    "    return fwd_dates_list\n",
    "\n",
    "list_models =os.listdir(ticker_models_path)\n",
    "data_dates= [os.path.splitext(item)[0] for item in os.listdir(os.path.join(data_dir, ticker))]\n",
    "model_dates=[list_models[item].split('_')[2] for item, _ in enumerate(list_models)]\n",
    "##\n",
    "class DataLoader(object):\n",
    "    def __init__(self, path_, ticker):\n",
    "        self.main_path = path_\n",
    "        self.ticker = ticker\n",
    "        self.labels_path = os.path.join(self.main_path, 'labels')\n",
    "        self.features_path = os.path.join(self.main_path, 'features')\n",
    "        self.ticker_labels_path = os.path.join(self.labels_path, self.ticker)\n",
    "        self.ticker_features_path = os.path.join(self.features_path, self.ticker)\n",
    "\n",
    "    def ticker_features(self, date):\n",
    "        file_loc = os.path.join(self.ticker_features_path, str(date) + '.pickle')\n",
    "        with open(file_loc, 'rb') as handle:\n",
    "            ticker_features = pickle.load(handle)\n",
    "        return ticker_features\n",
    "\n",
    "    def ticker_labels_pickle(self, date):\n",
    "        file_loc = os.path.join(self.ticker_labels_path, str(date) + '.pickle')\n",
    "        with open(file_loc, 'rb') as handle:\n",
    "            ticker_labels = pickle.load(handle)\n",
    "        return ticker_labels\n",
    "\n",
    "    def ticker_labels_csv(self, date):\n",
    "        file_loc = os.path.join(self.ticker_labels_path, str(date) + '.csv')\n",
    "        ticker_labels = pd.read_csv(file_loc)\n",
    "        return ticker_labels\n",
    "\n",
    "    @staticmethod\n",
    "    def open_pickle_file(path, pickle_file):\n",
    "        file_loc = os.path.join(path, pickle_file)\n",
    "        pickle_to_file = pickle.load(open(file_loc, \"rb\"))\n",
    "        return pickle_to_file\n",
    "\n",
    "    @staticmethod\n",
    "    def get_date_from_file(file_, numb_):\n",
    "        return os.path.splitext(file_[numb_])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsmm_core.hmm import hmm_engine\n",
    "from hsmm_core.observation_models import ExpIndMixDiracGauss\n",
    "from hsmm_core.feature_spaces import hmm_features\n",
    "from hsmm_core.hmm import hmm_calibration\n",
    "from hsmm_core.data_utils import load_data, TradingHours\n",
    "from hsmm_core.labelling import DataLabellingSimple\n",
    "from hsmm_core.consts import ThresholdMethod, LabellingChoice\n",
    "import pickle\n",
    "from hsmm_core.consts import InitialisationMethod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_models[1]\n",
    "filename = os.path.join(ticker_models_path, list_models[1])\n",
    "loaded_model = pickle.load(open(filename, 'rb'))['SVC'].best_estimator_\n",
    "#loaded_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_cls = DataLoader(path_=main_path, ticker=ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Parameters -setting up the HMM etc\n",
    "no_states = 2\n",
    "sigmas = [0.05, 0.002]  # fast and slow\n",
    "# Duration is measured in seconds for now (to be revised). lambda units are seconds^{-1}\n",
    "# so here we consider\n",
    "\n",
    "lambdas = [1. / 35., 1. / 10.]\n",
    "weights = [0.1, 0.6]\n",
    "\n",
    "obs_model = ExpIndMixDiracGauss(no_states)\n",
    "obs_model.set_up_initials(priors={'sigmas': sigmas, 'lambdas': lambdas, 'weights': weights})\n",
    "\n",
    "hmm_ = hmm_engine(obs_model, no_states)\n",
    "\n",
    "# set up some priors\n",
    "tpm = np.array([[0.4, 0.6], [0.7, 0.3]])\n",
    "pi = np.array([0.4, 0.6])\n",
    "hmm_.set_up_initials(priors={'tpm': tpm, 'pi': pi})\n",
    "rng = np.random.RandomState(1234)\n",
    "trd_hours_filter = TradingHours.all_trading_day\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "#get fwd dates\n",
    "#get the feature and label data\n",
    "#do prediction?\n",
    "#produce output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating hmm on date 20170630\n",
      "Number of points in data set is 5000, number of points with large price change 3315\n",
      "Calibrating hmm on date 20170710\n",
      "Number of points in data set is 5000, number of points with large price change 3404\n",
      "Calibrating hmm on date 20170613\n",
      "Number of points in data set is 5000, number of points with large price change 3345\n",
      "Calibrating hmm on date 20170612\n",
      "Number of points in data set is 5000, number of points with large price change 3355\n",
      "Calibrating hmm on date 20170712\n",
      "Number of points in data set is 5000, number of points with large price change 3384\n",
      "Calibrating hmm on date 20170616\n",
      "Number of points in data set is 5000, number of points with large price change 3375\n",
      "Calibrating hmm on date 20170615\n",
      "Number of points in data set is 5000, number of points with large price change 3332\n",
      "Calibrating hmm on date 20170614\n",
      "Number of points in data set is 5000, number of points with large price change 3359\n",
      "Calibrating hmm on date 20170619\n",
      "Number of points in data set is 5000, number of points with large price change 3270\n",
      "Calibrating hmm on date 20170704\n",
      "Number of points in data set is 5000, number of points with large price change 3345\n",
      "Calibrating hmm on date 20170706\n",
      "Number of points in data set is 5000, number of points with large price change 3331\n",
      "Calibrating hmm on date 20170711\n",
      "Number of points in data set is 5000, number of points with large price change 3347\n",
      "Calibrating hmm on date 20170627\n",
      "Number of points in data set is 5000, number of points with large price change 3348\n",
      "Calibrating hmm on date 20170703\n",
      "Number of points in data set is 5000, number of points with large price change 3333\n",
      "Calibrating hmm on date 20170705\n",
      "Number of points in data set is 5000, number of points with large price change 3390\n",
      "Calibrating hmm on date 20170620\n",
      "Number of points in data set is 5000, number of points with large price change 3354\n",
      "Calibrating hmm on date 20170707\n",
      "Number of points in data set is 5000, number of points with large price change 3294\n",
      "Calibrating hmm on date 20170623\n",
      "Number of points in data set is 5000, number of points with large price change 3367\n",
      "Calibrating hmm on date 20170626\n",
      "Number of points in data set is 5000, number of points with large price change 3367\n",
      "Calibrating hmm on date 20170601\n",
      "Number of points in data set is 5000, number of points with large price change 3296\n",
      "Calibrating hmm on date 20170602\n",
      "Number of points in data set is 5000, number of points with large price change 3301\n",
      "Calibrating hmm on date 20170622\n",
      "Number of points in data set is 5000, number of points with large price change 3312\n",
      "Calibrating hmm on date 20170605\n",
      "Number of points in data set is 5000, number of points with large price change 3377\n",
      "Calibrating hmm on date 20170606\n",
      "Number of points in data set is 5000, number of points with large price change 3360\n",
      "Calibrating hmm on date 20170607\n",
      "Number of points in data set is 5000, number of points with large price change 3350\n",
      "Calibrating hmm on date 20170608\n",
      "Number of points in data set is 5000, number of points with large price change 3315\n",
      "Calibrating hmm on date 20170609\n",
      "Number of points in data set is 5000, number of points with large price change 3352\n",
      "Calibrating hmm on date 20170621\n",
      "Number of points in data set is 5000, number of points with large price change 3392\n",
      "Calibrating hmm on date 20170628\n",
      "Number of points in data set is 5000, number of points with large price change 3349\n",
      "Calibrating hmm on date 20170629\n",
      "Number of points in data set is 5000, number of points with large price change 3375\n"
     ]
    }
   ],
   "source": [
    "init_params = {\n",
    "    \"obs_model_params\": {\n",
    "        'obs_model_name': 'ExpIndMixDiracGauss',\n",
    "        'em_init_method': InitialisationMethod.cluster\n",
    "\n",
    "    },\n",
    "    \"hidden_model_params\": {\n",
    "        'no_hidden_states': no_states,\n",
    "        'pi': pi,\n",
    "        'tpm': tpm,\n",
    "        'em_init_method': InitialisationMethod.uniform\n",
    "    },\n",
    "    \"update_tag\": 'tpsml'\n",
    "}\n",
    "\n",
    "data = load_data(ticker, which_trading_hours=TradingHours.all_trading_day) #prob dont need this\n",
    "\n",
    "hmm_calibration_engine = hmm_calibration(no_parallel_procs=None,\n",
    "                                             init_params=init_params)\n",
    "\n",
    "trained_hmms = hmm_calibration_engine.hmm_fit_func(ticker, data, trd_hours_filter,\n",
    "                                                       force_recalc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dates[1]\n",
    "hmm_fwd_dates = fwd_dates(_dates_list=data_dates, _key_date=model_dates[1])  # create fwd out of sample dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#         features_load = feature_engine.generate_features(data[date])\n",
    "#         labels_load = pd.read_csv(os.path.join(ticker_labels_path,str(date)+'.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, date_hmm in trained_hmms.iteritems():\n",
    "    feature_engine = hmm_features(date_hmm) #date_hmm is every fitted_model_date\n",
    "    fwd_features = feature_engine.generate_features(data_dic[fwd_date])\n",
    "    fwd_labels = data_cls.ticker_labels_csv(date=fwd_date)\n",
    "    features_fwd, labels_fwd = remove_nans(fwd_features, fwd_labels)\n",
    "    features_load = feature_engine.generate_features(data[date])\n",
    "    labels_load = pd.read_csv(os.path.join(ticker_labels_path,str(date)+'.csv'))\n",
    "    features, labels_clean = remove_nans(features_load, labels_load)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20170621\n",
      "20170630\n",
      "20170613\n",
      "20170612\n",
      "20170712\n",
      "20170616\n",
      "20170710\n",
      "20170614\n",
      "20170619\n",
      "20170606\n",
      "20170627\n",
      "20170615\n",
      "20170703\n",
      "20170705\n",
      "20170704\n",
      "20170707\n",
      "20170706\n",
      "20170626\n",
      "20170601\n",
      "20170602\n",
      "20170622\n",
      "20170623\n",
      "20170620\n",
      "20170607\n",
      "20170608\n",
      "20170609\n",
      "20170711\n",
      "20170628\n",
      "20170629\n",
      "20170605\n"
     ]
    }
   ],
   "source": [
    "for date, _ in trained_hmms.iteritems():\n",
    "    feature_engine = hmm_features(date_hmm) #model engine for feature generation\n",
    "    hmm_fwd_dates = fwd_dates(_dates_list=data_dates, _key_date=date)  # create fwd out of sample dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
