{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MKLpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.svm as svm\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_recall_fscore_support\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.multiclass import OneVsRestClassifier  # support from multiclass\n",
    "import time\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_pickle_file(path, pickle_file):\n",
    "    file_loc = os.path.join(path, pickle_file)\n",
    "    pickle_to_file = pickle.load(open(file_loc, \"rb\"))\n",
    "    return pickle_to_file\n",
    "def checkDir(dirLoc):\n",
    "    print\n",
    "    if os.path.exists(dirLoc):#Checks if the dir exists\n",
    "        print(\"The directory exists\")\n",
    "    else:\n",
    "        print (\"No directory found for \"+dirLoc) #Output if no directory\n",
    "#         print\n",
    "#         os.makedirs(CheckDir)#Creates a new dir for the given name\n",
    "#         print \"Directory created for \"+CheckDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FinDataReal', 'FinData', 'Data']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folderList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CNA.L', 'PRU.L', 'Labels', 'REL.L', 'BARC.L']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataOnlyDrive = '/media/ak/DataOnly'  # external date only drive\n",
    "#ext_drive_loc = ('/media/ak/My Passport/Experiment Data/')\n",
    "data_dir = os.getenv('FINANCE_DATA') #internal folder with finance data \n",
    "folderList =  [s for s in os.listdir(dataOnlyDrive) if s.startswith('Dat') or s.startswith('Fin')]\n",
    "# inputDrive = data_only_drive\n",
    "# features = os.path.join(inputDrive,'features')\n",
    "# labels = os.path.join(inputDrive, 'labels')\n",
    "\n",
    "# symbols = [s for s in os.listdir(features) if s.endswith('.L')]\n",
    "folderIdx = 0\n",
    "finalLocation= \"/\".join((dataOnlyDrive,folderList[folderIdx]))\n",
    "symbols = [s for s in os.listdir(finalLocation) if s.endswith('.L')] #keep a list of the symbols\n",
    "os.listdir(finalLocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pick a symbol ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ak/DataOnly/FinDataReal/PRU.L/MODEL_BASED\n"
     ]
    }
   ],
   "source": [
    "##picking up a specific feature\n",
    "symbolIdx = 1 #pick one of the symbols\n",
    "#symbols[symbolIdx] -->output :PRU.L\n",
    "\n",
    "# do a join to get the location\n",
    "symbolLocation = \"/\".join((finalLocation,symbols[symbolIdx])) \n",
    "\n",
    "# get he features now\n",
    "symbolFeaturesLocation = \"/\".join((symbolLocation,'MODEL_BASED')) # where all the HMM output is\n",
    "\n",
    "print(symbolFeaturesLocation) # <-- all the HMM model output is here, for each model there is a Date Folder and then OOS files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pick Features for a specific Dates ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "symbolFeaturesDatesList = os.listdir(symbolFeaturesLocation) #list of all the MODEL dates we have generated features files for. each #\n",
    "# each of these dates in symbolFeaturesDates corresponds to a list of dates\n",
    "## (symbolFeaturesDatesList = '20170829', '20170710', '20170801', ... ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now lets move to labels ...pick the location of the labels and make a list of all the available labels ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ak/DataOnly/FinDataReal/Labels/PRU.L/NON_DIRECTIONAL\n"
     ]
    }
   ],
   "source": [
    "#location of labels : /media/ak/DataOnly/FinDataReal/Labels/[Symbol :PRU.L]/NON_DIRECTIONAL\n",
    "\n",
    "symbolLabelsLocation = \"/\".join((finalLocation, 'Labels',symbols[symbolIdx],'NON_DIRECTIONAL')) \n",
    "\n",
    "#list of all the label dates\n",
    "\n",
    "symbolLabelsDates =[dateFile.split(\".\")[0] for dateFile in os.listdir(symbolLabelsLocation)]\n",
    "\n",
    "# Output symbolLabelsDates --> ['20170704', '20180226', '20180208',...] all we are doing is going for this \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are picking this HMM model date: 20170120\n"
     ]
    }
   ],
   "source": [
    "##now lets go down into each HMM-model date, and pick all the forward futures (out of sample)\n",
    "featrsIdx = 3 #symbol-hmm-model-date index\n",
    "\n",
    "#this symbolFeaturesDatesList[featrsIdx] will give you a date: 20170710 =which contains all the HMM induced featureb\n",
    "\n",
    "symbolOneFeaturesDate = \"/\".join((symbolFeaturesLocation, symbolFeaturesDatesList[featrsIdx]))\n",
    "# output looks like this: /media/ak/DataOnly/FinDataReal/PRU.L/MODEL_BASED/20170710\n",
    "\n",
    "symbolEachModelFeaturesDates=[file.split(\"_\")[5] for file in os.listdir(symbolOneFeaturesDate )] \n",
    "\n",
    "print(\"you are picking this HMM model date:\",symbolFeaturesDatesList[featrsIdx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### method to find the list of dates (intersection) that we have both HMM-model-features and labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = symbolEachModelFeaturesDates # set of OutOfSample Dates produced by each HMM Model\n",
    "b = symbolLabelsDates # set of all the Labels \n",
    "\n",
    "# we want to match OOS Dates with Labels \n",
    "commonDates =list(set(a) & set(b))\n",
    "\n",
    "# for these dates we have features and we have labels- so we can fit and predict! this produces a list of [date1, date 2, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRU.L_3_states_features_date:_20170728_now:_20181229_.pickle\n",
      "20181229\n"
     ]
    }
   ],
   "source": [
    "#Now we need to reconstruct the dates for the fitting\n",
    "commonIdx =0 #common dates index\n",
    "commonDates[commonIdx] # common features/labels- so pick a day with idx\n",
    "\n",
    "print(os.listdir(symbolOneFeaturesDate)[2]) #we picked one file\n",
    "# symbolFeaturesDatesList[featrsIdx] \n",
    "# the cache of the file includes the date it was saved, the first date is the Common Date and the next date is the \"creation date\"\n",
    "createDate = os.listdir(symbolOneFeaturesDate)[2].split(\"_\")[7] #stupid hack\n",
    "print(createDate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now we reconstruct the file but with arbitrary new date ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRU.L_3_states_features_date:_20170726_now:_20181229_.pickle\n"
     ]
    }
   ],
   "source": [
    "commonDatesFeatureDateFile = \\\n",
    "\"\".join((symbols[1],\"_3_states_features_date:_\",commonDates[commonIdx], \\\n",
    "         \"_now:_\",createDate,\"_.pickle\"))\n",
    "print(commonDatesFeatureDateFile) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now lets load up the pickle and the labels ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "testIdx= 3\n",
    "featuresIdxDirFileLoc = \"/\".join((symbolFeaturesLocation, symbolFeaturesDatesList[testIdx] \\\n",
    "                               ,  commonDatesFeatureDateFile))\n",
    "featuresTupleFile = pickle.load(open(featuresIdxDirFileLoc,\"rb\"), encoding='iso-8859-1')\n",
    "#encoding='iso-8859-1' or encoding ='latin1' or 'bytes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pick up the features ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfFeatures = pd.concat([featuresTupleFile[0], featuresTupleFile[1],\\\n",
    "                                                 featuresTupleFile[2], featuresTupleFile[3]], axis=1, sort=False).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3599, 21)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfFeatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsCommonDateFile = \".\".join((commonDates[commonIdx],'csv'))\n",
    "\n",
    "labelsCommonFileLoc = \"/\".join((symbolLabelsLocation, labelsCommonDateFile))\n",
    "labelsDf=pd.read_csv(labelsCommonFileLoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take the labels out\n",
    "labels =labelsDf['label_PrMov__window_5__thres_arbitrary__0.1'] \n",
    "\n",
    "labelsShift = labels.isna().sum() # going to use this for \"allignment of features and labels\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### put labels and features together before you dropnans ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfXY = pd.concat([dfFeatures, labels], axis=1, sort='False').dropna()\n",
    "labelName = str(dfXY.columns[dfXY.columns.str.contains(pat='label')].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fischer_score_dlambda</th>\n",
       "      <th>fischer_score_dsigma</th>\n",
       "      <th>fischer_score_dweight</th>\n",
       "      <th>lambda_lambda</th>\n",
       "      <th>lambda_sigma</th>\n",
       "      <th>lambda_weight</th>\n",
       "      <th>sigma_sigma</th>\n",
       "      <th>sigma_weight</th>\n",
       "      <th>weight_weight</th>\n",
       "      <th>gamma_0</th>\n",
       "      <th>...</th>\n",
       "      <th>ksi_0_to_0</th>\n",
       "      <th>ksi_0_to_1</th>\n",
       "      <th>ksi_0_to_2</th>\n",
       "      <th>ksi_1_to_0</th>\n",
       "      <th>ksi_1_to_1</th>\n",
       "      <th>ksi_1_to_2</th>\n",
       "      <th>ksi_2_to_0</th>\n",
       "      <th>ksi_2_to_1</th>\n",
       "      <th>ksi_2_to_2</th>\n",
       "      <th>label_PrMov__window_5__thres_arbitrary__0.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.515550</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.296892</td>\n",
       "      <td>9.999996e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.616586</td>\n",
       "      <td>-934.088591</td>\n",
       "      <td>-1.132378</td>\n",
       "      <td>-36.553836</td>\n",
       "      <td>14.286524</td>\n",
       "      <td>-0.607832</td>\n",
       "      <td>-9.769922e+06</td>\n",
       "      <td>-8.119140</td>\n",
       "      <td>-9.308414</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>9.978199e-01</td>\n",
       "      <td>2.179722e-03</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>3.813301e-07</td>\n",
       "      <td>7.338355e-10</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.000702e-08</td>\n",
       "      <td>1.230780e-10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.370264</td>\n",
       "      <td>-934.287746</td>\n",
       "      <td>0.471146</td>\n",
       "      <td>-82.878651</td>\n",
       "      <td>11.451600</td>\n",
       "      <td>-0.919716</td>\n",
       "      <td>-9.778087e+06</td>\n",
       "      <td>-6.024955</td>\n",
       "      <td>-11.805625</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>9.658294e-01</td>\n",
       "      <td>3.199084e-02</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>1.971026e-03</td>\n",
       "      <td>2.086978e-04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>14.480640</td>\n",
       "      <td>-934.300571</td>\n",
       "      <td>2.073181</td>\n",
       "      <td>-124.683190</td>\n",
       "      <td>11.042120</td>\n",
       "      <td>-1.272411</td>\n",
       "      <td>-9.778613e+06</td>\n",
       "      <td>-5.883499</td>\n",
       "      <td>-14.366444</td>\n",
       "      <td>5.533573e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>5.091251e-07</td>\n",
       "      <td>9.402840e-01</td>\n",
       "      <td>2.751592e-02</td>\n",
       "      <td>4.423217e-08</td>\n",
       "      <td>2.944503e-02</td>\n",
       "      <td>2.754465e-03</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   fischer_score_dlambda  fischer_score_dsigma  fischer_score_dweight  \\\n",
       "0               0.000003              0.000000               1.515550   \n",
       "1               2.616586           -934.088591              -1.132378   \n",
       "2               8.370264           -934.287746               0.471146   \n",
       "3              14.480640           -934.300571               2.073181   \n",
       "\n",
       "   lambda_lambda  lambda_sigma  lambda_weight   sigma_sigma  sigma_weight  \\\n",
       "0      -0.000031      0.000000       0.000004  0.000000e+00      0.000000   \n",
       "1     -36.553836     14.286524      -0.607832 -9.769922e+06     -8.119140   \n",
       "2     -82.878651     11.451600      -0.919716 -9.778087e+06     -6.024955   \n",
       "3    -124.683190     11.042120      -1.272411 -9.778613e+06     -5.883499   \n",
       "\n",
       "   weight_weight       gamma_0  ...    ksi_0_to_0    ksi_0_to_1    ksi_0_to_2  \\\n",
       "0       2.296892  9.999996e-01  ...  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "1      -9.308414  2.220446e-16  ...  2.220446e-16  9.978199e-01  2.179722e-03   \n",
       "2     -11.805625  2.220446e-16  ...  2.220446e-16  2.220446e-16  2.220446e-16   \n",
       "3     -14.366444  5.533573e-07  ...  2.220446e-16  2.220446e-16  2.220446e-16   \n",
       "\n",
       "     ksi_1_to_0    ksi_1_to_1    ksi_1_to_2    ksi_2_to_0    ksi_2_to_1  \\\n",
       "0  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "1  2.220446e-16  3.813301e-07  7.338355e-10  2.220446e-16  2.000702e-08   \n",
       "2  2.220446e-16  9.658294e-01  3.199084e-02  2.220446e-16  1.971026e-03   \n",
       "3  5.091251e-07  9.402840e-01  2.751592e-02  4.423217e-08  2.944503e-02   \n",
       "\n",
       "     ksi_2_to_2  label_PrMov__window_5__thres_arbitrary__0.1  \n",
       "0  0.000000e+00                                          0.0  \n",
       "1  1.230780e-10                                          0.0  \n",
       "2  2.086978e-04                                          0.0  \n",
       "3  2.754465e-03                                          0.0  \n",
       "\n",
       "[4 rows x 22 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfXY.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get working now on features and labels for MKL ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dfX.. 3595\n"
     ]
    }
   ],
   "source": [
    "dfX = dfXY.drop(columns=[ label_name])\n",
    "print(\"Shape of dfX..\",dfX.shape[0])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split the data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =dfX\n",
    "y = dfXY[dfXY.columns[dfXY.columns.str.contains(pat='label')]].iloc[:, 0]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Classifiers for Comparison ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf.fit(X_train, y_train)\n",
    "classifier = SVC(kernel='linear', C=0.01)\n",
    "fitd_clf=classifier.fit(X_train, y_train)\n",
    "# Create a SVC classifier using an RBF kernel\n",
    "rbf_classifier= SVC(kernel='rbf', random_state=0, gamma=1000, C=1000)\n",
    "rbf_clf=rbf_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
