{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsmm_core.data_utils import DataLoader, TradingHours\n",
    "from hsmm_core.feature_spaces import hmm_features\n",
    "from hsmm_core.hsmm_runner import HmmCalibration\n",
    "import time\n",
    "from hsmm_core.consts import InitialisationMethod\n",
    "from hsmm_core.data_utils import TradingHours, DataLoader\n",
    "from hsmm_core.labelling import DataLabellingSimple\n",
    "from hsmm_core.consts import ThresholdMethod, LabellingChoice\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier,  GradientBoostingClassifier\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "sc = StandardScaler()\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##useful functions\n",
    "def fwd_dates(_dates_list, _key_date):\n",
    "    # returns a list of dates that are forward from the key_date\n",
    "    fwd_dates_list = [i for i in _dates_list if i > _key_date]\n",
    "    return fwd_dates_list\n",
    "def remove_nans(features_tuple, labels, idx=1):\n",
    "    # not the cleanest but useful\n",
    "    # function to clean up nans as I seem to use it a lot, so better to have one function\n",
    "    # combines the features and labels and removes rows with nans across so we dont lose the ordering\n",
    "    # returns features and labels\n",
    "    features_df = pd.concat([features_tuple[0], features_tuple[1], features_tuple[2], \\\n",
    "                             features_tuple[3]], axis=1, sort=False)\n",
    "    labels_only = labels.drop(columns=['ReturnTradedPrice', 'Duration', 'states', 'TradedTime',\n",
    "                                       'TradedPrice'], axis=1)\n",
    "    df_concat = pd.concat([features_df, labels_only.iloc[:, 0:idx]], axis=1, sort='False')\n",
    "    # only using 1st set of labels- but we can re-write this a bit\n",
    "    df_x_nan = df_concat.dropna()  # dropping all nans\n",
    "    label_column_loc_ = df_x_nan.shape[1] - 1  # location of labels column in the clean df\n",
    "    labels_ = df_x_nan.iloc[:, label_column_loc_:label_column_loc_ + 1]  # keep pure labels\n",
    "    features_ = df_x_nan.drop(df_x_nan.columns[label_column_loc_], axis=1)  # keeping the features only\n",
    "    return features_, labels_ \n",
    "\n",
    "def prec_recall_report(y_true, y_predict):\n",
    "    # function to ge the sci-kit learn classification metrics into a pretty DF for csv!\n",
    "    report = pd.DataFrame(list(precision_recall_fscore_support(y_true, y_predict)),\n",
    "                          index=['Precision', 'Recall', 'F1-score', 'Support']).T\n",
    "    # Now add the 'Avg/Total' row\n",
    "    report.loc['Avg/Total', :] = precision_recall_fscore_support(y_true, y_predict, average='weighted')\n",
    "    report.loc['Avg/Total', 'Support'] = report['Support'].sum()\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finance_data=os.getenv('FINANCE_DATA')\n",
    "# os.listdir(finance_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ticker= 'os'\n",
    "###sort out the data###\n",
    "data_symbols=os.listdir(os.getenv('FINANCE_DATA'))\n",
    "synt_data_files=os.listdir(os.path.join(os.getenv('FINANCE_DATA'), ticker)) #unlabelled data\n",
    "ticker_dir = os.path.join(os.getenv('FINANCE_DATA'),ticker)\n",
    "# test_file_name =os.path.join(ticker_dir,synt_data[0])\n",
    "synt_data_dates=[os.path.splitext(file_name)[0] for file_name in synt_data_files]\n",
    "synt_data_dates.sort()#sort the dates. no special reason makes it cleaner to play with the indexation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_data_dir = os.path.join(os.path.expanduser(\"~\"), 'FinData')\n",
    "total_path = os.path.join(fin_data_dir, ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ak/FinData/SYNT_2states'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsmm_core.data_utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20160601',\n",
       " '20160602',\n",
       " '20160603',\n",
       " '20160606',\n",
       " '20160607',\n",
       " '20160608',\n",
       " '20160609',\n",
       " '20160610',\n",
       " '20160613',\n",
       " '20160614',\n",
       " '20160615',\n",
       " '20160616',\n",
       " '20160617',\n",
       " '20160620',\n",
       " '20160621',\n",
       " '20160622',\n",
       " '20160623',\n",
       " '20160624',\n",
       " '20160627',\n",
       " '20160628',\n",
       " '20160629',\n",
       " '20160630',\n",
       " '20160701',\n",
       " '20160704',\n",
       " '20160705',\n",
       " '20160706',\n",
       " '20160707',\n",
       " '20160708',\n",
       " '20160711',\n",
       " '20160712',\n",
       " '20160713',\n",
       " '20160714',\n",
       " '20160715',\n",
       " '20160718',\n",
       " '20160719',\n",
       " '20170601',\n",
       " '20170602',\n",
       " '20170605']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_files_for_ticker(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_dir = os.getenv('FINANCE_DATA')\n",
    "\n",
    "labels_path = os.path.join(data_dir, 'features_models/labels')\n",
    "\n",
    "ticker_labels_path = os.path.join(labels_path, ticker + '/NON_DIRECTIONAL')\n",
    "\n",
    "if not os.path.exists(os.path.join(data_dir, ticker)):\n",
    "    os.makedirs(os.path.join(data_dir, ticker))\n",
    "\n",
    "if not os.path.exists(ticker_labels_path):\n",
    "    os.makedirs(ticker_labels_path)\n",
    "\n",
    "# example=pd.read_csv(test_file_name, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ak/FinData/features_models/labels/SYNT_2states/NON_DIRECTIONAL'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_labels_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_states = 3\n",
    "\n",
    "data_loader_init = {\n",
    "    'trading_hours_filter': TradingHours.only_mkt_hours,\n",
    "}\n",
    "\n",
    "\n",
    "init_params = {\n",
    "    'obs_model_name': 'ExpIndMixDiracGauss',\n",
    "    'em_obs_init_method': InitialisationMethod.cluster,\n",
    "    'em_hidden_init_method': InitialisationMethod.uniform,\n",
    "    'no_hidden_states': n_hidden_states,\n",
    "    'update_tag': 'tpsml'\n",
    "}\n",
    "sd = synt_data_dates[0]\n",
    "ed = synt_data_dates[-1]\n",
    "\n",
    "data_loader = DataLoader(**data_loader_init)\n",
    "\n",
    "data_loader_hash = data_loader.data_loader_hash()\n",
    "\n",
    "data = data_loader.load_trades_data(ticker, start_date=sd, end_date=ed)\n",
    "\n",
    "hmm_calibration_engine = HmmCalibration(init_params=init_params)\n",
    "# hmm_calibration_engine.run_calibration_all_data(ticker, data,  data_loader_hash,\n",
    "#                                                 force_recalc=True, use_multiprocessing=False,\n",
    "#                                                 n_processes=2)\n",
    "# features_engine = hmm_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ak/FinData/features_models/labels/SYNT_2states/NON_DIRECTIONAL'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_labels_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create labels###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rolling_window': 25, 'labelling_method': 'PrMov', 'updown_threshold': 0.1, 'threshold_method': 'arbitrary'}\n"
     ]
    }
   ],
   "source": [
    "trading_hours_filter = TradingHours.only_mkt_hours\n",
    "\n",
    "\n",
    "window = 25\n",
    "threshold = 0.1\n",
    "\n",
    "labelling_method_params = [{\n",
    "\n",
    "    'labelling_method': LabellingChoice.price_move_in_window,\n",
    "    'rolling_window': window,\n",
    "    # Uncomment below if you want to check a price move only above a certain level\n",
    "    'updown_threshold': threshold,  # this is multiplied by 100\n",
    "    'threshold_method': ThresholdMethod.arbitrary,\n",
    "}]\n",
    "\n",
    "data_loader = DataLoader(trading_hours_filter)\n",
    "\n",
    "data = data_loader.load_trades_data(ticker, start_date=sd, end_date=ed)\n",
    "\n",
    "for label_init in labelling_method_params:\n",
    "    print label_init\n",
    "    labeller = DataLabellingSimple(label_init)\n",
    "    labeller.label_training_data(data)\n",
    "\n",
    "# SAVE THE DATA WITH LABELS \n",
    "for date, date_data in data.iteritems():\n",
    "    date_data.to_csv(os.path.join(ticker_labels_path, str(date)+'.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_clf = \"_\".join((\"synt_model\",  str(date), labels_clean.columns.values[0], \"clfs\", \".pickle\"))\n",
    "test_date=data.keys()[2]\n",
    "stored_hmm, _ = hmm_calibration_engine.get_calibrated_hmm(ticker, test_date, data_loader_hash)\n",
    "features_engine.hmm = stored_hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4834, 15), (4834, 1))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oos_dates= fwd_dates(_dates_list=data.keys(), _key_date=data.keys()[2])\n",
    "oos_date=oos_dates[5]\n",
    "features_load = features_engine.generate_features(data[oos_date])\n",
    "labels_load = pd.read_csv(os.path.join(ticker_label_data, str(oos_date)+'.csv'), index_col=0)\n",
    "test_x, test_y =remove_nans(features_load, labels_load)\n",
    "test_x.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.path.join(models_path, ticker))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_engine = hmm_features()\n",
    "\n",
    "for date, date_data in data.iteritems():\n",
    "    stored_hmm, _ = hmm_calibration_engine.get_calibrated_hmm(ticker, date, data_loader_hash)\n",
    "    #load classifiers\n",
    "    features_engine.hmm = stored_hmm\n",
    "#     oos_dates= fwd_dates(_dates_list=data.keys(), _key_date=date)\n",
    "#     for oos_date in oos_dates:\n",
    "#         file_name = \"_\".join(('synt_model',str(oos_date)))\n",
    "#         models_list=os.listdir(ticker_models_path)\n",
    "#         clf_model=[s for s in models_list if file_name in s][0]\n",
    "      \n",
    "#      #         features_load = features_engine.generat(e_features(data[oos_date])\n",
    "#         labels_load = pd.read_csv(os.path.join(ticker_label_data, str(oos_date)+'.csv'), index_col=0)\n",
    "#         features, labels_clean = remove_nans(features_load, labels_load)\n",
    "#         x_std = sc.fit_transform(features.values.astype(np.float))  # fit & transform the features\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(\n",
    "#             x_std, labels_clean, test_size=0.01, random_state=1, stratify=labels_clean)  # probably can get rid of this\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsmm_core.hmm.hmm_impl object at 0x7fc3381a5090>\n"
     ]
    }
   ],
   "source": [
    "print stored_hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date= data.keys()[-1]\n",
    "oos_dates= fwd_dates(_dates_list=data.keys(), _key_date=date)\n",
    "for oos_date in oos_dates:\n",
    "    file_name = \"_\".join(('synt_model',str(oos_date)))\n",
    "    models_list=os.listdir(ticker_models_path)\n",
    "    clf_model=[s for s in models_list if file_name in s][0]\n",
    "    best_clf = pickle.load(open(os.path.join(ticker_models_path,clf_model),\"rb\"))\n",
    "    svc=best_clf['SVC']\n",
    "    features_load = features_engine.generate_features(data[oos_date])\n",
    "    labels_load = pd.read_csv(os.path.join(ticker_label_data, str(oos_date)+'.csv'), index_col=0)\n",
    "    features, labels_clean = remove_nans(features_load, labels_load)\n",
    "    x_std = sc.fit_transform(features.values.astype(np.float))  # fit & transform the features\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        x_std, labels_clean, test_size=0.01, random_state=1, stratify=labels_clean)  \n",
    "    y_true = labels_clean\n",
    "    y_predict_clf= svc.predict(X_train)\n",
    "    classif_rate = np.mean(y_predict_clf.ravel()== y_true.values.ravel())*100\n",
    "    print (\"Classification Rate for %s: %f\" % (clf_model,classif_rate))\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data_for_features = dates_for_features[i],\n",
    "for index, date in enumerate(synt_data_dates):\n",
    "    seq = (synt_data_dates[index], \"csv\"); # This is sequence of strings.\n",
    "#     print str(\".\".join( seq ))\n",
    "    input_date_data = pd.read_csv(os.path.join(ticker_dir, str(\".\".join(seq))))\n",
    "    features_oos=features_engine.generate_features(input_date_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
