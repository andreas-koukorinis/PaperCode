{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsmm_core.prediction_engines import *\n",
    "from hsmm_core.data_utils import load_data, TradingHours\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from hsmm_core.consts import ThresholdMethod, LabellingChoice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier,  GradientBoostingClassifier\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.multiclass import OneVsRestClassifier #support from multiclass\n",
    "#####metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "import sklearn.linear_model as lm\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_dates(_dates_list, _key_date):\n",
    "    #returns a list of dates that are forward from the key_date\n",
    "    fwd_dates_list  = [i for i in _dates_list if i > _key_date]\n",
    "    return fwd_dates_list\n",
    "\n",
    "def remove_nans_labels(features_tuple, labels): \n",
    "    #function to clean up nans as I seem to use it a lot, so better to have one function\n",
    "    #combines the features and labels and removes rows with nans across so we dont lost the ordering\n",
    "    #returns features and labels\n",
    "    features_df = pd.concat([features_tuple[0], features_tuple[1], features_tuple[2], \\\n",
    "                             features_tuple[3]], axis=1, sort=False)\n",
    "    labels_only = labels.drop(columns=['ReturnTradedPrice','Duration', 'states', 'TradedTime',\n",
    "                                         'TradedPrice', 'ticker'], axis=1)\n",
    "    df_concat = pd.concat([features_df, labels_only.iloc[:, 0:1]], axis=1, sort='False')#only using 1st set of label\n",
    "    df_x_nan = df_concat.dropna() #dropping all nans\n",
    "    label_column_loc_ = df_x_nan.shape[1] - 1 #location of labels column in the clean df\n",
    "    labels_ = df_x_nan.iloc[:, label_column_loc_:label_column_loc_ + 1]#keep pure labels\n",
    "    features_ = df_x_nan.drop(df_x_nan.columns[label_column_loc_], axis=1) #keeping the features only\n",
    "    \n",
    "    return features_, labels_\n",
    "\n",
    "def remove_nans_duration(features_tuple, labels): \n",
    "    #function to clean up nans as I seem to use it a lot, so better to have one function\n",
    "    #combines the features and labels and removes rows with nans across so we dont lost the ordering\n",
    "    #returns features and labels\n",
    "    features_df = pd.concat([features_tuple[0], features_tuple[1], features_tuple[2], \\\n",
    "                             features_tuple[3]], axis=1, sort=False)\n",
    "    labels_only = labels.drop(columns=['ReturnTradedPrice', 'states', 'TradedTime',\n",
    "                                         'TradedPrice', 'ticker'], axis=1)\n",
    "    df_concat = pd.concat([features_df, labels_only['Duration']], axis=1, sort='False')#only using 1st set of label\n",
    "    df_x_nan = df_concat.dropna() #dropping all nans\n",
    "    label_column_loc_ = df_x_nan.shape[1] - 1 #location of labels column in the clean df\n",
    "    labels_ = df_x_nan.iloc[:, label_column_loc_:label_column_loc_ + 1]#keep pure labels\n",
    "    features_ = df_x_nan.drop(df_x_nan.columns[label_column_loc_], axis=1) #keeping the features only\n",
    "    \n",
    "    return features_, labels_\n",
    "\n",
    "def prec_recall_report(y_true_, y_predict_):\n",
    "    #function to ge the sci-kit learn classification metrics into a pretty DF for csv!\n",
    "    report = pd.DataFrame(list(precision_recall_fscore_support(y_true_, y_predict_)),\n",
    "                          index=['Precision', 'Recall', 'F1-score', 'Support']).T\n",
    "    # Now add the 'Avg/Total' row\n",
    "    report.loc['Avg/Total', :] = precision_recall_fscore_support(y_true_, y_predict_, average='weighted')\n",
    "    report.loc['Avg/Total', 'Support'] = report['Support'].sum()\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#####OOP#####\n",
    "class DataLoader(object):\n",
    "    def __init__(self, path_, ticker):\n",
    "        self.main_path = path_\n",
    "        self.ticker = ticker\n",
    "        self.labels_path = os.path.join(self.main_path, 'labels')\n",
    "        self.features_path = os.path.join(self.main_path, 'features')\n",
    "        self.ticker_labels_path = os.path.join(self.labels_path, self.ticker)\n",
    "        self.ticker_features_path = os.path.join(self.features_path, self.ticker)\n",
    "\n",
    "    def ticker_features(self, date):\n",
    "        file_loc = os.path.join(self.ticker_features_path, str(date) + '.pickle')\n",
    "        with open(file_loc, 'rb') as handle:\n",
    "            ticker_features = pickle.load(handle)\n",
    "        return ticker_features\n",
    "\n",
    "    def ticker_labels_pickle(self, date):\n",
    "        file_loc = os.path.join(self.ticker_labels_path, str(date) + '.pickle')\n",
    "        with open(file_loc, 'rb') as handle:\n",
    "            ticker_labels = pickle.load(handle)\n",
    "        return ticker_labels\n",
    "\n",
    "    def ticker_labels_csv(self, date):\n",
    "        file_loc = os.path.join(self.ticker_labels_path, str(date) + '.csv')\n",
    "        ticker_labels = pd.read_csv(file_loc)\n",
    "        return ticker_labels\n",
    "\n",
    "    @staticmethod\n",
    "    def open_pickle_file(path, pickle_file):\n",
    "        file_loc = os.path.join(path, pickle_file)\n",
    "        pickle_to_file = pickle.load(open(file_loc, \"rb\"))\n",
    "        return pickle_to_file\n",
    "\n",
    "    @staticmethod\n",
    "    def get_date_from_file(file_, numb_):\n",
    "        return os.path.splitext(file_[numb_])[0]\n",
    "\n",
    "\n",
    "class PriceIndicators(object):\n",
    "    # a class to be expanded that uses features for base case -price only-indicators\n",
    "    \"\"\"\"Requires:\n",
    "    symbol - A stock symbol on which to form a strategy on.\n",
    "    short_window - Lookback period for short moving average.\n",
    "    long_window - Lookback period for long moving average.\"\"\"\n",
    "\n",
    "    def __init__(self, symbol, labels_df):\n",
    "        self.symbol = symbol\n",
    "        self.labels = labels_df\n",
    "\n",
    "    def MACD(self, short_window=5, long_window=20):\n",
    "        short_rolling_px = self.labels['TradedPrice'].rolling(window=short_window).mean()\n",
    "        long_rolling_px = self.labels['TradedPrice'].rolling(window=long_window).mean()\n",
    "        px_indx = long_rolling_px - short_rolling_px\n",
    "        return px_indx\n",
    "\n",
    "\n",
    "class FitModels(object):\n",
    "\n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "    # # Train a SVM classification model\n",
    "\n",
    "    def ridge_clf(self, cv_folds=5):\n",
    "        \n",
    "        model_ridge_clf= RidgeClassifierCV(alphas = np.arange(0.1, 1000,0.1),\\\n",
    "                                           cv=KFold(cv_folds),normalize= True).fit(self.X_train,self.y_train.values.ravel())\n",
    "        #check if class_weight should be used as 'balanced'\n",
    "\n",
    "        return model_ridge_clf\n",
    "\n",
    "    def svm_clf(self, kernel_choice):\n",
    "\n",
    "        param_grid = dict(kernel=[str(kernel_choice)],\n",
    "                          C=[1, 5, 10, 25, 50, 100],\n",
    "                          gamma=[0.0001, 0.001, 0.01, 0.02, 0.05, 0.01])\n",
    "        svc = svm.SVC(class_weight='balanced')\n",
    "        clf = GridSearchCV(svc,param_grid)\n",
    "        clf.fit(self.X_train, np.asanyarray(self.y_train).reshape(self.y_train.shape[0]))\n",
    "\n",
    "        return clf\n",
    "\n",
    "    def gradient_boost_clf(self, learning_rate=0.25):\n",
    "        #this needs to be written properly- but this is somewhat optimised#\n",
    "        GBR = GradientBoostingClassifier(n_estimators=3000, learning_rate=learning_rate,\n",
    "                                           max_depth=4, max_features='sqrt',\n",
    "                                           min_samples_leaf=15, min_samples_split=10)\n",
    "\n",
    "        gb_boost_clf = GBR.fit(self.X_train, self.y_train)\n",
    "\n",
    "        return gb_boost_clf\n",
    "    \n",
    "    def gp_clf(self):\n",
    "        #The length parameter l controls the smoothness of the function and Ïƒf the vertical variation. \n",
    "        #For simplicity, we use the same length parameter l for all input dimensions (isotropic kernel)\n",
    "        \n",
    "        kernel = 1.0 * RBF([1.0]) #isotropic\n",
    "        gpc_rbf_isotropic = GaussianProcessClassifier(kernel=kernel).fit(self.X_train, self.y_train)\n",
    "        #hyperparameters are optimised by default\n",
    "        return gpc_rbf_isotropic\n",
    "\n",
    "    def random_forest_clf(self, no_est=100):\n",
    "        rfc=RandomForestClassifier(n_estimators=no_est, max_depth=4,n_jobs=-1, warm_start=True)\n",
    "        rfc.fit(X_train, y_train)\n",
    "        \n",
    "        return rfc\n",
    "\n",
    "\n",
    "    def run_cv(self, clf_class, **kwargs):\n",
    "        # Construct a kfolds object\n",
    "        kf = KFold(len(self.y_train), n_folds=10, shuffle=True)\n",
    "        y_pred = self.y_train.copy()\n",
    "\n",
    "        # Iterate through folds\n",
    "        for train_index, test_index in kf:\n",
    "            X_train_local, X_test_local = self.X_train[train_index], self.X_train[test_index]\n",
    "            y_train_local = self.y_train[train_index]\n",
    "            # Initialize a classifier with key word arguments\n",
    "            clf = clf_class(**kwargs)\n",
    "            clf.fit(self.X_train, self.y_train)\n",
    "            y_pred[test_index] = clf.predict(X_test_local)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "class PredictModels(FitModels):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "def no_nans(label):\n",
    "    return np.sum(np.isnan(label))\n",
    "\n",
    "\n",
    "def remove_last_element(arr):\n",
    "    return arr[np.arange(arr.size - 1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'SYNT_2states'\n",
    "features_path='/home/ak/Documents/Data/features_models/features/'\n",
    "\n",
    "####paths####\n",
    "labels_path = '/home/ak/Documents/Data/features_models/labels'\n",
    "main_path = '/home/ak/Documents/Data/features_models/'\n",
    "\n",
    "models_path=os.path.join(main_path,'models')\n",
    "hmm_models_path = os.path.join(models_path,'hmm_models')\n",
    "features_ticker_path = os.path.join(features_path, ticker)\n",
    "predictions_path = os.path.join(main_path, 'predictions')\n",
    "# ticker = 'SYNT_4states'\n",
    "\n",
    "features_path = os.path.join(main_path, 'features')\n",
    "ticker_labels_path = os.path.join(labels_path, ticker)\n",
    "ticker_models_path = os.path.join(models_path, ticker)\n",
    "ticker_predictions_path = os.path.join(predictions_path, ticker)\n",
    "\n",
    "ticker_features_path = os.path.join(features_path, ticker)\n",
    "\n",
    "###\n",
    "\n",
    "# list of files    \n",
    "labels_list = os.listdir(ticker_labels_path)\n",
    "\n",
    "features_list = os.listdir(ticker_features_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2_state_trained_hmm_params_.pickle',\n",
       " '2_state_trained_hmm_models_.pickle',\n",
       " 'SYNT_2states',\n",
       " 'SYNT_3states',\n",
       " 'hmm_models',\n",
       " 'SYNT_4states']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "data_cls = DataLoader(path_=main_path, ticker=ticker)\n",
    "idx = 1  # take first label-index for the data-frame of labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating hmm on date 20171221\n",
      "Number of points in data set is 4852, number of points with large price change 3301\n",
      "('saving the model params:', '2_state_trained_hmm_params_.pickle')\n",
      "('saving the model params:', '2_state_trained_hmm_params_.pickle')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "n_hidden_states = 2\n",
    "\n",
    "startprob = np.array([0.6, 0.4])\n",
    "\n",
    "transmat = np.array([[0.2, 0.8], [0.6, 0.4]])\n",
    "\n",
    "\n",
    "init_params = {\n",
    "    \"obs_model_params\": {\n",
    "                                'obs_model_name': 'ExpIndMixDiracGauss',\n",
    "                                'em_init_method': InitialisationMethod.cluster\n",
    "\n",
    "    },\n",
    "    \"hidden_model_params\": {\n",
    "                                'no_hidden_states': n_hidden_states,\n",
    "                                'pi':startprob,\n",
    "                                'tpm': transmat,\n",
    "                                'em_init_method': InitialisationMethod.uniform\n",
    "    },\n",
    "    \"update_tag\": 'tpsml'\n",
    "}\n",
    "\n",
    "\n",
    "# start_dt = '20171002'\n",
    "\n",
    "# end_dt = '20171003'\n",
    "\n",
    "trading_hours_filter = TradingHours.only_mkt_hours\n",
    "\n",
    "data_dic = load_data(ticker, no_of_days=2) #, start_date=start_dt, end_date=end_dt)\n",
    "hmm_calibration_engine = hmm_calibration(no_parallel_procs=None,\n",
    "                                         init_params=init_params)\n",
    "\n",
    "\n",
    "trained_hmms = hmm_calibration_engine.hmm_fit_func(ticker, data_dic, trading_hours_filter,\n",
    "                                                   force_recalc=False)\n",
    "\n",
    "###saving hmm model params###\n",
    "seq_params = \"_\".join((str(n_hidden_states),'state',\"trained\",\"hmm\",\"params\", \".pickle\"))\n",
    "print(\"saving the model params:\",seq_params)\n",
    "pickle.dump(init_params, open(os.path.join(models_path,seq_params), 'wb'))\n",
    "###saving trained model hmms###\n",
    "seq_model = \"_\".join((str(n_hidden_states),'state',\"trained\",\"hmm\",\"models\", \".pickle\"))\n",
    "print(\"saving the model params:\",seq_params)\n",
    "pickle.dump(init_params, open(os.path.join(models_path,seq_model), 'wb'))\n",
    "\n",
    "models_dates=trained_hmms.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for date, date_hmm in trained_hmms.iteritems():\n",
    "#     feature_engine = hmm_features(date_hmm)\n",
    "#     features = feature_engine.generate_features(data_dic[date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20171221', '20171017']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.85042381287\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for date, date_hmm in trained_hmms.iteritems():\n",
    "    newpath=os.path.join(features_ticker_path, str(date))\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "\n",
    "    feature_engine = hmm_features(date_hmm)\n",
    "    features_load = feature_engine.generate_features(data_dic[date])\n",
    "    labels_load = data_cls.ticker_labels_csv(date=date)\n",
    "    features, duration = remove_nans_duration(features_load, labels_load)\n",
    "#     x_std = sc.fit_transform(features.values.astype(np.float)) #fit & transform the features\n",
    "#     X_train, X_test, y_train, y_test = train_test_split( \\\n",
    "#         x_std, labels_clean, test_size=0.05, random_state=1, stratify=labels_clean) #probably can get rid of this\n",
    "# #     models_cls = FitModels(X_train, y_train)\n",
    "# #     best_clfs = {'SVC': models_cls.svm_clf(kernel_choice=\"rbf\"), \n",
    "# #                  'Ridge_clf': models_cls.ridge_clf(), \n",
    "#                  'GBOOST': models_cls.gradient_boost_clf(),\n",
    "#                  'GP_clf': models_cls.gp_clf(),\n",
    "#                  'RF_clf': models_cls.random_forest_clf(),\n",
    "#                 }\n",
    "            \n",
    "#     # This is sequence for the name of the best classifiers.\n",
    "#     seq_clf = \"_\".join((str(date),labels_clean.columns.values[0],\"clfs\", \".pickle\"))\n",
    "#     print(\"saving the classifiers:\",seq_clf)\n",
    "#     pickle.dump(best_clfs, open(os.path.join(ticker_models_path,seq_clf), 'wb'))\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(features.shape[0]==duration.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4846, 15)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[:-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('R2 score:', -13.941455382705358)\n",
      "('Explained_Variance Score:', -12.579151849446776)\n",
      "('R2 score:', -53.21043008911015)\n",
      "('Explained_Variance Score:', -47.786034019888774)\n",
      "('R2 score:', -38.95998696599079)\n",
      "('Explained_Variance Score:', -34.8434531899501)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt\n",
    "y=duration[1:].values\n",
    "X=sc.fit_transform(features[:-1].values.astype(np.float))\n",
    "clf = svm.SVR(C=1.0, epsilon=0.2)\n",
    "# Fit regression model\n",
    "svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1, epsilon=0.25)\n",
    "svr_lin = SVR(kernel='linear', C=1e3,epsilon=0.25)\n",
    "svr_poly = SVR(kernel='poly', C=1e3, degree=2, epsilon=0.25)\n",
    "y_rbf = svr_rbf.fit(X, y).predict(X)\n",
    "y_lin = svr_lin.fit(X, y).predict(X)\n",
    "y_poly = svr_poly.fit(X, y).predict(X)\n",
    "y_predict_vector=[y_rbf, y_lin, y_poly]\n",
    "# #############################################################################\n",
    "# Look at the results\n",
    "# lw = 2\n",
    "# # plt.scatter(y, y, color='darkorange', label='data')\n",
    "# plt.scatter(y, y_rbf, color='navy', lw=lw, label='RBF model')\n",
    "# # plt.plot(y, y_lin, color='c', lw=lw, label='Linear model')\n",
    "# # plt.plot(y, y_poly, color='cornflowerblue', lw=lw, label='Polynomial model')\n",
    "# plt.xlabel('data')\n",
    "# plt.ylabel('target')\n",
    "\n",
    "from sklearn.metrics import explained_variance_score, mean_squared_error, r2_score\n",
    "def regression_metrics(y_predict, y_true):\n",
    "    print ('R2 score:',r2_score(y_true, y_predict))\n",
    "    print ('Explained_Variance Score:',explained_variance_score(y_true, y_predict))\n",
    "          \n",
    "for output in y_predict_vector:\n",
    "    regression_metrics(y, output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This SVR instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-f7c42568290e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                    scoring=\"neg_mean_squared_error\", cv=10)\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0my_svr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_plot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0msvr_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m print(\"SVR prediction for %d inputs in %.3f s\"\n",
      "\u001b[0;32m/home/ak/Envs/DataAnalysis/local/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \"\"\"\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ak/Envs/DataAnalysis/local/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'support_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ak/Envs/DataAnalysis/local/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This SVR instance is not fitted yet. Call 'fit' with appropriate arguments before using this method."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "X_plot = np.linspace(0, 5, 100000)[:, None]\n",
    "train_sizes, train_scores_svr, test_scores_svr = \\\n",
    "    learning_curve(clf, X[:300], y[:300], train_sizes=np.linspace(0.1, 1, 10),\n",
    "                   scoring=\"neg_mean_squared_error\", cv=10)\n",
    "    \n",
    "y_svr = clf.predict(X_plot)\n",
    "svr_predict = time.time() - t0\n",
    "print(\"SVR prediction for %d inputs in %.3f s\"\n",
    "      % (X_plot.shape[0], svr_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "learning_rates=[0.05, 0.1, 0.25, 0.75,1]\n",
    "X_train_sub, X_validation_sub, y_train_sub, y_validation_sub = train_test_split(X_, y_, random_state=0)\n",
    "for learning_rate in learning_rates:\n",
    "    GBR = GradientBoostingClassifier(n_estimators=3000, learning_rate=learning_rate,\n",
    "                                           max_depth=4, max_features='sqrt',\n",
    "                                           min_samples_leaf=15, min_samples_split=10)\n",
    "    GBR.fit(X_train_sub,y_train_sub)\n",
    "    print (\"accuracy_score(validation):{0:3f}\".format(GBR.score(X_validation_sub, y_validation_sub)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification rate for SVC 29.898403\n",
      "classification rate for GP_clf 47.978437\n",
      "classification rate for GBOOST 43.520630\n",
      "classification rate for SVC 46.079440\n",
      "classification rate for GP_clf 37.415106\n",
      "classification rate for GBOOST 43.136448\n",
      "classification rate for SVC 29.347602\n",
      "classification rate for GP_clf 49.598683\n",
      "classification rate for GBOOST 42.539617\n",
      "classification rate for SVC 39.575608\n",
      "classification rate for GP_clf 42.315616\n",
      "classification rate for GBOOST 44.705398\n",
      "classification rate for SVC 48.651986\n",
      "classification rate for GP_clf 37.579749\n",
      "classification rate for GBOOST 41.181313\n",
      "classification rate for SVC 31.067961\n",
      "classification rate for GP_clf 46.684569\n",
      "classification rate for GBOOST 42.016112\n",
      "classification rate for SVC 34.337598\n",
      "classification rate for GP_clf 46.657037\n",
      "classification rate for GBOOST 47.111019\n",
      "classification rate for SVC 28.280124\n",
      "classification rate for GP_clf 48.671473\n",
      "classification rate for GBOOST 43.501545\n"
     ]
    }
   ],
   "source": [
    "oos_data_dic =load_data(ticker, no_of_days=20)\n",
    "hmm_fwd_dates= fwd_dates(_dates_list=oos_data_dic.keys(),_key_date=date) #create fwd out of sample dates\n",
    "\n",
    "for fwd_date in hmm_fwd_dates: #OOS testing\n",
    "    fwd_features = feature_engine.generate_features(oos_data_dic[fwd_date])\n",
    "    fwd_labels = data_cls.ticker_labels_csv(date=fwd_date)\n",
    "    features_fwd, labels_fwd= remove_nans(fwd_features, fwd_labels)    \n",
    "    x_std_fwd = sc.fit_transform(features_fwd.values.astype(np.float)) #fit & transform the features\n",
    "    y_true = labels_fwd\n",
    "    CLFs=['SVC', 'GP_clf', 'GBOOST']\n",
    "    for clf in CLFs:\n",
    "        y_predict_clf = best_clfs[clf].predict(x_std_fwd)\n",
    "        classif_rate= np.mean(y_predict_clf.ravel() == np.asanyarray(y_true).ravel())*100\n",
    "        print(\"classification rate for %s %f\"%(clf, classif_rate))\n",
    "#         clf_report= prec_recall_report(y_true, y_predict_clf)\n",
    "#         report_name = \"_\".join(( 'performance','report','ticker',str(fwd_date),'.csv'))\n",
    "#         report_loc = os.path.join(ticker_predictions_path, report_name)\n",
    "#         clf_report.to_csv(report_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = feature_engine.generate_features(data_dic[date])\n",
    "labels = data_cls.ticker_labels_csv(date=date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test =remove_nans(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_data_dic =load_data(ticker, no_of_days=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "model=best_clfs['KR']\n",
    "model.probability = True\n",
    "\n",
    "\n",
    "\n",
    "# fpr, tpr, _ = roc_curve(y_test, y_predict_probabilities)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(fpr, tpr, color='darkorange',\n",
    "#          lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC Curve')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=best_clfs['SVC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_keys=trained_hmms.keys()\n",
    "trained_hmms[hmm_keys[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.datasets import load_svmlight_file, dump_svmlight_file\n",
    "from MKLpy.regularization import normalization,rescale_01\n",
    "from MKLpy.metrics.pairwise import HPK_kernel\n",
    "from MKLpy.regularization import kernel_centering, kernel_normalization, tracenorm\n",
    "from MKLpy.algorithms import EasyMKL,RMGD,RMKL,AverageMKL\n",
    "import os\n",
    "from sklearn.metrics.pairwise import rbf_kernel as RBF \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/ak/Documents/Research/temp/mkl_example.dmp'\n",
    "\n",
    "# os.listdir(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_mkl=y_train.values.reshape(y_train.shape[0])\n",
    "x_train_mkl=X_train\n",
    "dump_svmlight_file(X_train, y_train_mkl, path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = load_svmlight_file(path)\n",
    "X = X.toarray()\t#Important! MKLpy require dense matrices!\n",
    "X = rescale_01(X)\n",
    "X = normalization(X) \n",
    "\n",
    "KL = [HPK_kernel(X,degree=d) for d in range(1,11)]\n",
    "KL2 = [RBF(X, gamma=gamma) for gamma in [1., 10, 100.]] \n",
    "\n",
    "EasyMKL().fit(KL2,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
