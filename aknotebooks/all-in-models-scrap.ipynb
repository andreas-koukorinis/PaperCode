{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.tseries.offsets import BDay\n",
    "from hsmm_core.hmm import hmm_engine\n",
    "from hsmm_core.observation_models import ExpIndMixDiracGauss\n",
    "from hsmm_core.data_utils import load_data, TradingHours\n",
    "from hsmm_core.data_utils import load_data, TradingHours\n",
    "from hsmm_core.feature_spaces import hmm_features\n",
    "from hsmm_core.hmm import hmm_calibration\n",
    "from hsmm_core.data_utils import load_data, TradingHours\n",
    "from hsmm_core.labelling import DataLabellingSimple\n",
    "from hsmm_core.consts import ThresholdMethod, LabellingChoice\n",
    "import pickle\n",
    "from hsmm_core.consts import InitialisationMethod\n",
    "import datetime as dt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier,  GradientBoostingClassifier\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nans(features_tuple, labels, idx=1):\n",
    "    # not the cleanest but useful\n",
    "    # function to clean up nans as I seem to use it a lot, so better to have one function\n",
    "    # combines the features and labels and removes rows with nans across so we dont lose the ordering\n",
    "    # returns features and labels\n",
    "    features_df = pd.concat([features_tuple[0], features_tuple[1], features_tuple[2], \\\n",
    "                             features_tuple[3]], axis=1, sort=False)\n",
    "    labels_only = labels.drop(columns=['ReturnTradedPrice', 'Duration', 'states', 'TradedTime',\n",
    "                                       'TradedPrice', 'ticker'], axis=1)\n",
    "    df_concat = pd.concat([features_df, labels_only.iloc[:, 0:idx]], axis=1, sort='False')\n",
    "    # only using 1st set of labels- but we can re-write this a bit\n",
    "    df_x_nan = df_concat.dropna()  # dropping all nans\n",
    "    label_column_loc_ = df_x_nan.shape[1] - 1  # location of labels column in the clean df\n",
    "    labels_ = df_x_nan.iloc[:, label_column_loc_:label_column_loc_ + 1]  # keep pure labels\n",
    "    features_ = df_x_nan.drop(df_x_nan.columns[label_column_loc_], axis=1)  # keeping the features only\n",
    "\n",
    "    return features_, labels_ #return features and labels in the X,y order that scikit takes the input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FitModels(object):\n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    # # Train a SVM classification model\n",
    "\n",
    "    def ridge_clf(self, cv_folds=5):\n",
    "        model_ridge_clf = RidgeClassifierCV(alphas=np.arange(0.1, 1000, 0.1), \\\n",
    "                                            cv=KFold(cv_folds), normalize=True).fit(self.X_train,\n",
    "                                                                                    self.y_train.values.ravel())\n",
    "        # check if class_weight should be used as 'balanced'\n",
    "\n",
    "        return model_ridge_clf\n",
    "\n",
    "    def svm_clf(self, kernel_choice):\n",
    "        param_grid = dict(kernel=[str(kernel_choice)],\n",
    "                          C=[1, 5, 10, 25, 50, 100],\n",
    "                          gamma=[0.0001, 0.001, 0.01, 0.02, 0.05, 0.01])\n",
    "        svc = SVC(class_weight='balanced')\n",
    "        clf = GridSearchCV(svc, param_grid)\n",
    "        clf.fit(self.X_train, np.asanyarray(self.y_train).reshape(self.y_train.shape[0]))\n",
    "\n",
    "        return clf\n",
    "\n",
    "    def gradient_boost_clf(self, learning_rate=0.25):\n",
    "        # this needs to be written properly- but this is somewhat optimised#\n",
    "        GBR = GradientBoostingClassifier(n_estimators=3000, learning_rate=learning_rate,\n",
    "                                         max_depth=4, max_features='sqrt',\n",
    "                                         min_samples_leaf=15, min_samples_split=10)\n",
    "\n",
    "        gb_boost_clf = GBR.fit(self.X_train, self.y_train)\n",
    "\n",
    "        return gb_boost_clf\n",
    "\n",
    "    def gp_clf(self):\n",
    "        kernel = 1.0 * RBF([1.0])  # isotropic\n",
    "        gpc_rbf_isotropic = GaussianProcessClassifier(kernel=kernel).fit(self.X_train, self.y_train)\n",
    "        # hyperparameters are optimised by default\n",
    "        return gpc_rbf_isotropic\n",
    "\n",
    "    def random_forest_clf(self, no_est=100):\n",
    "        rfc = RandomForestClassifier(n_estimators=no_est, max_depth=4, n_jobs=-1, warm_start=True)\n",
    "        rfc.fit(X_train, y_train)\n",
    "\n",
    "        return rfc\n",
    "\n",
    "    def run_cv(self, clf_class, **kwargs):\n",
    "        # Construct a kfolds object\n",
    "        kf = KFold(len(self.y_train), n_folds=10, shuffle=True)\n",
    "        y_pred = self.y_train.copy()\n",
    "\n",
    "        # Iterate through folds\n",
    "        for train_index, test_index in kf:\n",
    "            X_train_local, X_test_local = self.X_train[train_index], self.X_train[test_index]\n",
    "            y_train_local = self.y_train[train_index]\n",
    "            # Initialize a classifier with key word arguments\n",
    "            clf = clf_class(**kwargs)\n",
    "            clf.fit(self.X_train, self.y_train)\n",
    "            y_pred[test_index] = clf.predict(X_test_local)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ticker = 'test_SYNT_2states'\n",
    "\n",
    "\n",
    "data_dir = os.getenv('FINANCE_DATA')\n",
    "features_path='/home/ak/Data/features_models/features/'\n",
    "labels_path= '/home/ak/Data/features_models/labels'\n",
    "\n",
    "ticker_labels_path = os.path.join(labels_path,ticker+'/NON_DIRECTIONAL')\n",
    "\n",
    "if not os.path.exists(os.path.join(data_dir, ticker)):\n",
    "    os.makedirs(os.path.join(data_dir, ticker))\n",
    "    \n",
    "if not os.path.exists(ticker_labels_path):\n",
    "    os.makedirs(ticker_labels_path)\n",
    "\n",
    "    ####paths####\n",
    "main_path = '/home/ak/Data/features_models/'\n",
    "\n",
    "models_path=os.path.join(main_path,'models')\n",
    "ticker_models_path = os.path.join(models_path, ticker)\n",
    "# hmm_models_path = os.path.join(models_path,'hmm_models')\n",
    "# features_ticker_path = os.path.join(features_path, ticker)\n",
    "# predictions_path = os.path.join(main_path, 'predictions')\n",
    "if not os.path.exists(ticker_models_path):\n",
    "    os.makedirs(ticker_models_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ak/Data/features_models/labels/test_SYNT_2states/NON_DIRECTIONAL'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_labels_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "no_states = 2\n",
    "sigmas = [0.05, 0.002] # fast and slow\n",
    "# Duration is measured in seconds for now (to be revised). lambda units are seconds^{-1}\n",
    "# so here we consider\n",
    "\n",
    "lambdas = [1./35., 1./10.]\n",
    "weights = [0.1, 0.6]\n",
    "\n",
    "obs_model = ExpIndMixDiracGauss(no_states)\n",
    "obs_model.set_up_initials(priors={'sigmas': sigmas, 'lambdas': lambdas, 'weights': weights})\n",
    "\n",
    "hmm_ = hmm_engine(obs_model, no_states)\n",
    "\n",
    "# set up some priors\n",
    "tpm = np.array([[0.4, 0.6], [0.7, 0.3]])\n",
    "pi = np.array([0.4, 0.6])\n",
    "hmm_.set_up_initials(priors={'tpm': tpm, 'pi': pi})\n",
    "\n",
    "no_dates = 3\n",
    "start_date = pd.datetime(2017, 6, 1)\n",
    "dummy_dates = [start_date + BDay(i) for i in range(no_dates)]\n",
    "\n",
    "no_points = 5000\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# silly hack, add 1 millisecond so that the initial timestamp is printed with milliseconds and does not\n",
    "# break the parsing of Timestamps when loading\n",
    "\n",
    "morning_start = dt.time(8, 0, 0, 1)\n",
    "\n",
    "initial_price = 100\n",
    "\n",
    "for dd in dummy_dates:\n",
    "    random_states = hmm_.sample_states(rng=rng, length=no_points)\n",
    "    observation_points = obs_model.sample_data(no_points, rng=rng, state=random_states)\n",
    "    # The first duration is always zero\n",
    "    observation_points[0, 0] = 0.\n",
    "\n",
    "    file_path = os.path.join(data_dir, ticker)\n",
    "    file_name = '.'.join([dd.strftime('%Y%m%d'), 'csv'])\n",
    "\n",
    "    data_to_save = pd.DataFrame({'states': random_states,\n",
    "                                 'Duration': observation_points[:, 0],\n",
    "                                 'ReturnTradedPrice': observation_points[:, 1],\n",
    "                                 })\n",
    "    data_to_save['TradedTime'] = pd.Series()\n",
    "\n",
    "    # Now calculate the Traded prices and traded times in reverse order as to what would happen\n",
    "    # with real data.\n",
    "    # data_to_save.loc[0, 'TradedTime'] = dt.datetime.combine(dd.date(), morning_start)\n",
    "    data_to_save['TradedTime'] = data_to_save['Duration'].cumsum().apply(lambda dur:\n",
    "                                                                         (dt.datetime.combine(dd.date(), morning_start)+\\\n",
    "                                                                                     dt.timedelta(seconds=dur)).time())\n",
    "\n",
    "    data_to_save['TradedPrice'] = initial_price * (1. + data_to_save['ReturnTradedPrice']).cumprod()\n",
    "    data_to_save.to_csv(os.path.join(file_path, file_name), index=False)\n",
    "\n",
    "print \"ok\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Creation ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_hidden_states = no_states\n",
    "\n",
    "init_params = {\n",
    "    \"obs_model_params\": {\n",
    "                                'obs_model_name': 'ExpIndMixDiracGauss',\n",
    "                                'em_init_method': InitialisationMethod.cluster\n",
    "\n",
    "    },\n",
    "    \"hidden_model_params\": {\n",
    "                                'no_hidden_states': no_states,\n",
    "                                'pi':pi,\n",
    "                                'tpm': tpm,\n",
    "                                'em_init_method': InitialisationMethod.uniform\n",
    "    },\n",
    "    \"update_tag\": 'tpsml'\n",
    "}\n",
    "\n",
    "\n",
    "data = load_data(ticker, which_trading_hours=TradingHours.all_trading_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trd_hours_filter = TradingHours.all_trading_day\n",
    "hmm_calibration_engine = hmm_calibration(no_parallel_procs=None,\n",
    "                                         init_params=init_params)\n",
    "\n",
    "\n",
    "trained_hmms = hmm_calibration_engine.hmm_fit_func(ticker, data, trd_hours_filter,\n",
    "                                                   force_recalc=False)\n",
    "\n",
    "\n",
    "for date, date_hmm in trained_hmms.iteritems():\n",
    "    feature_engine = hmm_features(date_hmm)\n",
    "    features = feature_engine.generate_features(data[date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## uncomment below:  for saving hmm models ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###saving trained model hmms###\n",
    "# seq_model = \"_\".join((str(ticker),str(n_hidden_states),'state',\"trained\",\"hmm\",\"models\", \".pickle\"))\n",
    "# print(\"saving the model:\", seq_model)\n",
    "# pickle.dump(init_params, open(os.path.join(models_path,seq_model), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create labels ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rolling_window': 25, 'labelling_method': <LabellingChoice.price_move_in_window: 'PrMov'>, 'updown_threshold': 0.1, 'threshold_method': <ThresholdMethod.arbitrary: 'arbitrary'>}\n"
     ]
    }
   ],
   "source": [
    "window =25\n",
    "threshold =0.1\n",
    "\n",
    "\n",
    "labelling_method_params = [{\n",
    "\n",
    "    'labelling_method': LabellingChoice.price_move_in_window,\n",
    "    'rolling_window': window,\n",
    "    # Uncomment below if you want to check a price move only above a certain level\n",
    "    'updown_threshold': threshold, #this is multiplied by 100\n",
    "    'threshold_method': ThresholdMethod.arbitrary,\n",
    "}]\n",
    "\n",
    "for label_init in labelling_method_params:\n",
    "    print label_init\n",
    "    labeller = DataLabellingSimple(label_init)\n",
    "    labeller.label_training_data(data)\n",
    "\n",
    "keys_ = data.keys()\n",
    "\n",
    "for key_, _ in enumerate(keys_):\n",
    "    data[keys_[key_]].to_csv(ticker_labels_path+'/'+str(keys_[key_])+'.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ak/Data/features_models/labels/test_SYNT_2states/NON_DIRECTIONAL'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_labels_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Labels: Locations we need ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ticker = 'SYNT_2states'\n",
    "# ticker = 'SYNT_4states'\n",
    "\n",
    "\n",
    "ticker_features_path = os.path.join(features_path, ticker)\n",
    "\n",
    "###\n",
    "\n",
    "# list of files    \n",
    "labels_list = os.listdir(ticker_labels_path)\n",
    "\n",
    "# features_list = os.listdir(ticker_features_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Labels###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NON_DIRECTIONAL', '20170605.csv', '20170602.csv', '20170601.csv']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "non_directional = os.path.join(ticker_labels_path, labels_list[0])\n",
    "labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('saving the classifiers:', 'synthetic_data_20170601_label_PrMov__window_25__thres_arbitrary__10.0_clfs_.pickle')\n",
      "('saving the classifiers:', 'synthetic_data_20170602_label_PrMov__window_25__thres_arbitrary__10.0_clfs_.pickle')\n",
      "('saving the classifiers:', 'synthetic_data_20170605_label_PrMov__window_25__thres_arbitrary__10.0_clfs_.pickle')\n"
     ]
    }
   ],
   "source": [
    "data_dic = load_data(ticker, which_trading_hours=TradingHours.all_trading_day)\n",
    "## clf fitting##\n",
    "for date, date_hmm in trained_hmms.iteritems():\n",
    "    feature_engine = hmm_features(date_hmm)\n",
    "    features_load = feature_engine.generate_features(data_dic[date])\n",
    "    labels_load= pd.read_csv(os.path.join(non_directional,str(date)+'.csv'))\n",
    "    features, labels_clean = remove_nans(features_load, labels_load)\n",
    "    x_std = sc.fit_transform(features.values.astype(np.float)) #fit & transform the features\n",
    "    X_train, X_test, y_train, y_test = train_test_split( \\\n",
    "        x_std, labels_clean, test_size=0.05, random_state=1, stratify=labels_clean) #probably can get rid of this\n",
    "    models_cls = FitModels(X_train, y_train)\n",
    "    best_clfs = {#'SVC': models_cls.svm_clf(kernel_choice=\"rbf\"),\n",
    "                 'RIDGE_clf': models_cls.ridge_clf()\n",
    "                 #'GBOOST': models_cls.gradient_boost_clf(),\n",
    "                 'GP_clf': models_cls.gp_clf(),\n",
    "                 #'RF_clf': models_cls.random_forest_clf(),\n",
    "                 }\n",
    "    # This is sequence for the name of the best classifiers.\n",
    "    seq_clf = \"_\".join((\"synthetic_data\",str(date),labels_clean.columns.values[0],\"clfs\", \".pickle\"))\n",
    "    print(\"saving the classifiers:\",seq_clf)\n",
    "    pickle.dump(best_clfs, open(os.path.join(ticker_models_path,seq_clf), 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RIDGE_clf': RidgeClassifierCV(alphas=array([1.000e-01, 2.000e-01, ..., 9.998e+02, 9.999e+02]),\n",
       "          class_weight=None,\n",
       "          cv=KFold(n_splits=5, random_state=None, shuffle=False),\n",
       "          fit_intercept=True, normalize=True, scoring=None),\n",
       " 'SVC': GridSearchCV(cv=None, error_score='raise',\n",
       "        estimator=SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "   decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "   max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "   tol=0.001, verbose=False),\n",
       "        fit_params=None, iid=True, n_jobs=1,\n",
       "        param_grid={'kernel': ['rbf'], 'C': [1, 5, 10, 25, 50, 100], 'gamma': [0.0001, 0.001, 0.01, 0.02, 0.05, 0.01]},\n",
       "        pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "        scoring=None, verbose=0)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_labels_path,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(os.path.join(ticker_labels_path,str(date)+'.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
