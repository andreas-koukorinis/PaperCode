{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_recall_fscore_support\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.multiclass import OneVsRestClassifier #support from multiclass\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20171006',\n",
       " '20170922',\n",
       " '20180405',\n",
       " '20180227',\n",
       " '20170711',\n",
       " '20170216',\n",
       " '20171020',\n",
       " '20170904',\n",
       " '20180418',\n",
       " '20170824',\n",
       " '20180404',\n",
       " '20170905',\n",
       " '20170703',\n",
       " '20170918',\n",
       " '20180219',\n",
       " '20180419',\n",
       " '20170914',\n",
       " '20171002',\n",
       " '20170915',\n",
       " '20170214',\n",
       " '20180412',\n",
       " '20170829',\n",
       " '20170817',\n",
       " '20170202',\n",
       " '20171024',\n",
       " '20170719',\n",
       " '20170809',\n",
       " '20180207',\n",
       " '20170807',\n",
       " '20180403',\n",
       " '20171010',\n",
       " '20170727',\n",
       " '20170206',\n",
       " '20170912',\n",
       " '20170821',\n",
       " '20180221',\n",
       " '20180209',\n",
       " '20170823',\n",
       " '20170814',\n",
       " '20170811',\n",
       " '20170704',\n",
       " '20170228',\n",
       " '20170818',\n",
       " '20170203',\n",
       " '20170223',\n",
       " '20170816',\n",
       " '20170215',\n",
       " '20180406',\n",
       " '20170717',\n",
       " '20170830',\n",
       " '20171027',\n",
       " '20170705',\n",
       " '20180205',\n",
       " '20180208',\n",
       " '20170927',\n",
       " '20170710',\n",
       " '20170731',\n",
       " '20171018',\n",
       " '20170803',\n",
       " '20170928',\n",
       " '20170804',\n",
       " '20180413',\n",
       " '20170227',\n",
       " '20180409',\n",
       " '20171017',\n",
       " '20170911',\n",
       " '20170201',\n",
       " '20180216',\n",
       " '20170210',\n",
       " '20170209',\n",
       " '20171013',\n",
       " '20180214',\n",
       " '20170831',\n",
       " '20170913',\n",
       " '20171019',\n",
       " '20171005',\n",
       " '20170217',\n",
       " '20170207',\n",
       " '20170925',\n",
       " '20180213',\n",
       " '20170726',\n",
       " '20170920',\n",
       " '20170220',\n",
       " '20170908',\n",
       " '20171025',\n",
       " '20170825',\n",
       " '20170718',\n",
       " '20170808',\n",
       " '20180212',\n",
       " '20180226',\n",
       " '20170706',\n",
       " '20180228',\n",
       " '20170221',\n",
       " '20170724',\n",
       " '20171031',\n",
       " '20170713',\n",
       " '20170815',\n",
       " '20171030',\n",
       " '20170901',\n",
       " '20180410',\n",
       " '20170721',\n",
       " '20171023',\n",
       " '20180215',\n",
       " '20170208',\n",
       " '20171009',\n",
       " '20180201',\n",
       " '20170906',\n",
       " '20170907',\n",
       " '20180202',\n",
       " '20180222',\n",
       " '20170929',\n",
       " '20180220',\n",
       " '20171003',\n",
       " '20180223',\n",
       " '20170224',\n",
       " '20170720',\n",
       " '20170728',\n",
       " '20170714',\n",
       " '20171026',\n",
       " '20170802',\n",
       " '20170222',\n",
       " '20170712',\n",
       " '20170707',\n",
       " '20180417',\n",
       " '20180411',\n",
       " '20180420',\n",
       " '20180416',\n",
       " '20170810',\n",
       " '20170725',\n",
       " '20180206',\n",
       " '20170921',\n",
       " '20170822',\n",
       " '20170213',\n",
       " '20171016',\n",
       " '20170801',\n",
       " '20171004',\n",
       " '20170926',\n",
       " '20171012',\n",
       " '20171011',\n",
       " '20170919']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "data_dir = os.getenv('FINANCE_DATA')\n",
    "data_only_drive = '/mnt/usb-Seagate_Expansion_Desk_NA8XEHR6-0:0-part2' #external date only drive\n",
    "\n",
    "ftse_symbols = [s for s in os.listdir(data_dir) if s.endswith('.L')]\n",
    "features_models = os.path.join(data_dir, 'features_models')\n",
    "labels = os.path.join(features_models, 'labels')\n",
    "features = os.path.join(features_models, 'features')\n",
    "hmm_models = os.path.join(features_models, 'models', 'HMM')\n",
    "\n",
    "os.listdir(os.path.join(features,ftse_symbols[1],'MODEL_BASED')) #list of all the model -dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_labels_path= os.path.join(labels, ftse_symbols[1])\n",
    "# dates_features =os.listdir(os.path.join(features, ftse_symbols[1],'MODEL_BASED'))\n",
    "# test= os.path.join(symbol_features_path, dates_features[1])\n",
    "# features_for_date=os.listdir(test)\n",
    "# \"_\".join((features_for_date[1].split('_')[0],features_for_date[1].split('_')[1],features_for_date[1].split('_')[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20170928.csv',\n",
       " '20171027.csv',\n",
       " '20170803.csv',\n",
       " '20170711.csv',\n",
       " '20170824.csv',\n",
       " '20170808.csv',\n",
       " '20170210.csv',\n",
       " '20171004.csv',\n",
       " '20170705.csv',\n",
       " '20171024.csv',\n",
       " '20180220.csv',\n",
       " '20170904.csv',\n",
       " '20170224.csv',\n",
       " '20170915.csv',\n",
       " '20170913.csv',\n",
       " '20171020.csv',\n",
       " '20170802.csv',\n",
       " '20171009.csv',\n",
       " '20170830.csv',\n",
       " '20170223.csv',\n",
       " '20171003.csv',\n",
       " '20170810.csv',\n",
       " '20170727.csv',\n",
       " '20170213.csv',\n",
       " '20170712.csv',\n",
       " '20170831.csv',\n",
       " '20170829.csv',\n",
       " '20180411.csv',\n",
       " '20170801.csv',\n",
       " '20170704.csv',\n",
       " '20170901.csv',\n",
       " '20171018.csv',\n",
       " '20170821.csv',\n",
       " '20180201.csv',\n",
       " '20170908.csv',\n",
       " '20170228.csv',\n",
       " '20170914.csv',\n",
       " '20180221.csv',\n",
       " '20170725.csv',\n",
       " '20170728.csv',\n",
       " '20170227.csv',\n",
       " '20170222.csv',\n",
       " '20171026.csv',\n",
       " '20180205.csv',\n",
       " '20170922.csv',\n",
       " '20170209.csv',\n",
       " '20180419.csv',\n",
       " '20180405.csv',\n",
       " '20170703.csv',\n",
       " '20170825.csv',\n",
       " '20170207.csv',\n",
       " '20180212.csv',\n",
       " '20170719.csv',\n",
       " '20171010.csv',\n",
       " '20170221.csv',\n",
       " '20170816.csv',\n",
       " '20180417.csv',\n",
       " '20180219.csv',\n",
       " '20170815.csv',\n",
       " '20180413.csv',\n",
       " '20170907.csv',\n",
       " '20170706.csv',\n",
       " '20170710.csv',\n",
       " '20171031.csv',\n",
       " '20170217.csv',\n",
       " '20171016.csv',\n",
       " '20180216.csv',\n",
       " '20180416.csv',\n",
       " '20170811.csv',\n",
       " '20170921.csv',\n",
       " '20180420.csv',\n",
       " '20170203.csv',\n",
       " '20171013.csv',\n",
       " '20180227.csv',\n",
       " '20180213.csv',\n",
       " '20180206.csv',\n",
       " '20170720.csv',\n",
       " '20170920.csv',\n",
       " '20170714.csv',\n",
       " '20170906.csv',\n",
       " '20170215.csv',\n",
       " '20170724.csv',\n",
       " '20170216.csv',\n",
       " '20180403.csv',\n",
       " '20171019.csv',\n",
       " '20170718.csv',\n",
       " '20170912.csv',\n",
       " '20180412.csv',\n",
       " '20180207.csv',\n",
       " '20170208.csv',\n",
       " '20171017.csv',\n",
       " '20171002.csv',\n",
       " '20170925.csv',\n",
       " '20171023.csv',\n",
       " '20180215.csv',\n",
       " '20180202.csv',\n",
       " '20170919.csv',\n",
       " '20180228.csv',\n",
       " '20170818.csv',\n",
       " '20180209.csv',\n",
       " '20170721.csv',\n",
       " '20171005.csv',\n",
       " '20180406.csv',\n",
       " '20170807.csv',\n",
       " '20171030.csv',\n",
       " '20170905.csv',\n",
       " '20170214.csv',\n",
       " '20180208.csv',\n",
       " '20180223.csv',\n",
       " '20180404.csv',\n",
       " '20171011.csv',\n",
       " '20180410.csv',\n",
       " '20171025.csv',\n",
       " '20180418.csv',\n",
       " '20170929.csv',\n",
       " '20170804.csv',\n",
       " '20170809.csv',\n",
       " '20170822.csv',\n",
       " '20170202.csv',\n",
       " '20171012.csv',\n",
       " '20170731.csv',\n",
       " '20180226.csv',\n",
       " '20170927.csv',\n",
       " '20170814.csv',\n",
       " '20170911.csv',\n",
       " '20170726.csv',\n",
       " '20170220.csv',\n",
       " '20170206.csv',\n",
       " '20170918.csv',\n",
       " '20180214.csv',\n",
       " '20180222.csv',\n",
       " '20170713.csv',\n",
       " '20170201.csv',\n",
       " '20170823.csv',\n",
       " '20170707.csv',\n",
       " '20170926.csv',\n",
       " '20171006.csv',\n",
       " '20170717.csv',\n",
       " '20170817.csv',\n",
       " '20180409.csv']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbol_features_path=os.path.join(features, ftse_symbols[1],'MODEL_BASED')\n",
    "dates_for_symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####OOP#####\n",
    "class DataLoader(object):\n",
    "    def __init__(self, path_main, ticker):\n",
    "        self.main_path = path_main\n",
    "        self.ticker = ticker\n",
    "        \n",
    "        self.features_labels_path = os.path.join(self.main_path, 'features_models')\n",
    "        self.features_path = os.path.join(self.features_labels_path,'features')\n",
    "        # collection of per symbol non directional labels\n",
    "        self.labels_path = os.path.join(self.features_labels_path, 'labels', self.ticker, 'NON_DIRECTIONAL')\n",
    "        self.symbol_features_path = os.path.join(self.features_labels_path,'features', self.ticker, 'MODEL_BASED')\n",
    "         #list of all the model -oos hmm feature dates - each folder is a collection of oos feature dates\n",
    "        self.oos_dates_list = os.listdir(self.symbol_features_path)\n",
    "        \n",
    "    def ticker_features(self, model_date, date):\n",
    "        #need to make this a lot more flexible with number of states\n",
    "        if model_date < date:\n",
    "            file_name = \"_\".join((self.ticker, '3', 'states','features','date:',date,'now:', '20181224','.pickle'))\n",
    "            file_loc = os.path.join(self.symbol_features_path,str(model_date),file_name)\n",
    "            with open(file_loc, 'rb') as handle:\n",
    "                ticker_features = pickle.load(handle)\n",
    "        else:\n",
    "            print('Loading Feature Date which is in-sample. Change your Model Date')\n",
    "        return ticker_features\n",
    "\n",
    "    def ticker_labels_csv(self, date):\n",
    "        file_loc = os.path.join(self.labels_path, str(date)+'.csv')\n",
    "        ticker_labels = pd.read_csv(file_loc, index_col=0)\n",
    "        return ticker_labels\n",
    "\n",
    "    @staticmethod\n",
    "    def open_pickle_file(path, pickle_file):\n",
    "        file_loc = os.path.join(path, pickle_file)\n",
    "        pickle_to_file = pickle.load(open(file_loc, \"rb\"))\n",
    "        return pickle_to_file\n",
    "\n",
    "    @staticmethod\n",
    "    def get_date_from_file(file_, numb_):\n",
    "        return os.path.splitext(file_[numb_])[0]\n",
    "\n",
    "\n",
    "class MarketFeatures(object):\n",
    "    # a class to be expanded that uses features for base case -market based only-indicators/features\n",
    "    \"\"\"\"Requires:\n",
    "    a dataframe that has TradedPrice And Volume columns\n",
    "    symbol - A stock symbol on which to form a strategy on.\n",
    "    short_window - Lookback period for short moving average.\n",
    "    long_window - Lookback period for long moving average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "#         self.ticker = ticker\n",
    "        self.df = df\n",
    "\n",
    "    def load_data(self):\n",
    "        pass\n",
    "\n",
    "    def ma_spread(self, short_window=5, long_window=20):\n",
    "        # function that produces the MA spread, which can be used on its own or as an input for MACD\n",
    "        short_rolling_px = self.df['TradedPrice'].rolling(window=short_window).mean()\n",
    "        long_rolling_px = self.df['TradedPrice'].rolling(window=long_window).mean()\n",
    "        px_name = \"_\".join(('px_indx', str(short_window), str(long_window)))\n",
    "        self.df[px_name] = long_rolling_px - short_rolling_px\n",
    "        return self.df\n",
    "\n",
    "    def obv_calc(self):\n",
    "        # on balance volume indicator\n",
    "        self.df['SignedVolume'] = self.df['Volume'] * np.sign(self.df['TradedPrice'].diff()).cumsum()\n",
    "        self.df['SignedVolume'].iat[1] = 0\n",
    "        self.df['OBV'] = self.df['SignedVolume'] #.cumsum()\n",
    "        self.df = df.drop(columns=['SignedVolume'])\n",
    "        return self.df\n",
    "\n",
    "    def chaikin_mf(self, period=5):\n",
    "        # chaiking money flow indicator\n",
    "        self.df[\"MF Multiplier\"] = (self.df['TradedPrice'] - (self.df['TradedPrice'].expanding(period).min()) \\\n",
    "                               - (self.df['TradedPrice'].expanding(period).max() \\\n",
    "                                  - self.df['TradedPrice'])) / (\n",
    "                                          self.df['TradedPrice'].expanding(period).max() - self.df[\n",
    "                                      'TradedPrice'].expanding(period).min())\n",
    "        self.df[\"MF Volume\"] = self.df['MF Multiplier'] * df['Volume']\n",
    "        self.df['CMF_' + str(period)] = self.df['MF Volume'].sum() / self.df[\"Volume\"].rolling(period).sum()\n",
    "        self.df = self.df.drop(columns=['MF Multiplier', 'MF Volume'])\n",
    "        return self.df\n",
    "\n",
    "\n",
    "class FitModels(object):\n",
    "\n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "    # # Train a SVM classification model\n",
    "\n",
    "    def best_kernel_ridge(self, kernel_choice):\n",
    "\n",
    "        kr_clf =OneVsRestClassifier(GridSearchCV(KernelRidge(kernel=str(kernel_choice)), cv=10,\n",
    "                          param_grid={\"alpha\": [1e0, 0.1, 1e-2, 1e-3],\n",
    "                                      \"gamma\": np.logspace(-2, 2, 5)})).fit(self.X_train, self.y_train)\n",
    "\n",
    "        return kr_clf\n",
    "\n",
    "    def best_svm_clf(self, kernel_choice):\n",
    "\n",
    "        param_grid = dict(kernel=[str(kernel_choice)],\n",
    "                          C=[1, 5, 10, 25, 50, 100],\n",
    "                          gamma=[0.0001, 0.001, 0.01, 0.02, 0.05, 0.01])\n",
    "\n",
    "        clf = OneVsRestClassifier(\n",
    "            GridSearchCV(SVC(class_weight='balanced'), param_grid, verbose=1, n_jobs=-1, cv=10))\\\n",
    "            .fit(self.X_train, self.y_train)\n",
    "        return clf\n",
    "\n",
    "    def best_gradient_boost_clf(self):\n",
    "        #this needs to be written properly- just a baseline placeholder here!\n",
    "        GBR = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                           max_depth=4, max_features='sqrt',\n",
    "                                           min_samples_leaf=15, min_samples_split=10,loss='huber',\n",
    "                                        random_state=5)\n",
    "\n",
    "        gb_boost_clf = OneVsRestClassifier(GBR).fit(self.X_train, self.y_train)\n",
    "\n",
    "        return gb_boost_clf\n",
    "    def best_MKL_clf(self):\n",
    "        pass\n",
    "    def best_knn_clf(self):\n",
    "        pass\n",
    "    def best_random_forest_clf(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def run_cv(self, clf_class, **kwargs):\n",
    "        # Construct a kfolds object\n",
    "        kf = KFold(len(self.y_train), n_folds=10, shuffle=True)\n",
    "        y_pred = self.y_train.copy()\n",
    "\n",
    "        # Iterate through folds\n",
    "        for train_index, test_index in kf:\n",
    "            X_train_local, X_test_local = self.X_train[train_index], self.X_train[test_index]\n",
    "            y_train_local = self.y_train[train_index]\n",
    "            # Initialize a classifier with key word arguments\n",
    "            clf = clf_class(**kwargs)\n",
    "            clf.fit(self.X_train, self.y_train)\n",
    "            y_pred[test_index] = clf.predict(X_test_local)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "class PredictModels(FitModels):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "def no_nans(label):\n",
    "    return np.sum(np.isnan(label))\n",
    "\n",
    "\n",
    "def remove_last_element(arr):\n",
    "    return arr[np.arange(arr.size - 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_dates(_dates_list, _key_date):\n",
    "    #returns a list of dates that are forward from the key_date\n",
    "    fwd_dates_list  = [i for i in _dates_list if i > _key_date]\n",
    "    return fwd_dates_list\n",
    "\n",
    "def prec_recall_report(y_true_, y_predict_):\n",
    "    #function to ge the sci-kit learn classification metrics into a pretty DF for csv!\n",
    "    report = pd.DataFrame(list(precision_recall_fscore_support(y_true_, y_predict_)),\n",
    "                          index=['Precision', 'Recall', 'F1-score', 'Support']).T\n",
    "    # Now add the 'Avg/Total' row\n",
    "    report.loc['Avg/Total', :] = precision_recall_fscore_support(y_true_, y_predict_, average='weighted')\n",
    "    report.loc['Avg/Total', 'Support'] = report['Support'].sum()\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__' :\n",
    "\n",
    "    data_dir = os.getenv('FINANCE_DATA')\n",
    "    data_only_drive = '/mnt/usb-Seagate_Expansion_Desk_NA8XEHR6-0:0-part2' #external date only drive\n",
    "\n",
    "    ftse_symbols = [s for s in os.listdir(data_dir) if s.endswith('.L')]\n",
    "    features_models = os.path.join(data_dir, 'features_models')\n",
    "    labels = os.path.join(features_models, 'labels')\n",
    "    features = os.path.join(features_models, 'features')\n",
    "    hmm_models = os.path.join(features_models, 'models', 'HMM')\n",
    "    model_save_loc = os.path.join(data_only_drive, 'Data','features_models','models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ak/virtualenvs/DataAnalysis/lib/python2.7/site-packages/ipykernel_launcher.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "test_date='20170928'\n",
    "datacls = DataLoader(path_main=data_dir, ticker=ftse_symbols[1])\n",
    "dates_for_symbol =os.listdir(datacls.labels_path)\n",
    "df =datacls.ticker_labels_csv(date='20180215')\n",
    "df_final =MarketFeatures(df=MarketFeatures(df=MarketFeatures(df=df).obv_calc()).chaikin_mf()).ma_spread()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['Volume'].iat[0]=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        100.0\n",
       "1       1000.0\n",
       "2       2000.0\n",
       "3       1556.0\n",
       "4       1884.0\n",
       "5       4314.0\n",
       "6         30.0\n",
       "7        757.0\n",
       "8       2000.0\n",
       "9        352.0\n",
       "10      1956.0\n",
       "11       315.0\n",
       "12      3682.0\n",
       "13       993.0\n",
       "14      1808.0\n",
       "15      2000.0\n",
       "16      4621.0\n",
       "17      1479.0\n",
       "18      2077.0\n",
       "19       853.0\n",
       "20      3040.0\n",
       "21       484.0\n",
       "22      2502.0\n",
       "23       539.0\n",
       "24       525.0\n",
       "25      2600.0\n",
       "26      2412.0\n",
       "27      1850.0\n",
       "28      2800.0\n",
       "29      1669.0\n",
       "         ...  \n",
       "1699     474.0\n",
       "1700    2000.0\n",
       "1701    1655.0\n",
       "1702    2133.0\n",
       "1703     554.0\n",
       "1704     400.0\n",
       "1705    1916.0\n",
       "1706    2929.0\n",
       "1707     193.0\n",
       "1708    1010.0\n",
       "1709    2946.0\n",
       "1710     273.0\n",
       "1711     299.0\n",
       "1712    2488.0\n",
       "1713     454.0\n",
       "1714    1266.0\n",
       "1715    3775.0\n",
       "1716     248.0\n",
       "1717     447.0\n",
       "1718    2775.0\n",
       "1719     127.0\n",
       "1720    1600.0\n",
       "1721    3300.0\n",
       "1722    1306.0\n",
       "1723     800.0\n",
       "1724     800.0\n",
       "1725     400.0\n",
       "1726    1500.0\n",
       "1727    4941.0\n",
       "1728     577.0\n",
       "Name: Volume, Length: 1729, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final['Volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dates = datacls.oos_dates_list #list of dates that hmm models have been fitted and used out of sample\n",
    "model_date_idx = 1 #picking one such hmm model\n",
    "#path for hmm model features\n",
    "feature_files_path =os.path.join(symbol_features_path,model_dates[model_date_idx])\n",
    "#get all the feature files for each model date\n",
    "list_of_feature_files_for_model_date= os.listdir(os.path.join(symbol_features_path,model_dates[model_date_idx]))\n",
    "#get the dates for those feature files\n",
    "feature_dt_model_date=[list_of_feature_files_for_model_date[idx].split(\"_\")[5] for idx,_ in enumerate(list_of_feature_files_for_model_date)]\n",
    "#pick a feature file\n",
    "test_pickle_file =list_of_feature_files_for_model_date[1]\n",
    "#get all the features in one go\n",
    "features_tuple = datacls.open_pickle_file(path= feature_files_path, pickle_file=test_pickle_file)\n",
    "#get labels file\n",
    "df =datacls.ticker_labels_csv(date=list_of_feature_files_for_model_date[1].split(\"_\")[5])\n",
    "df_w_market_features =MarketFeatures(df=MarketFeatures(df=MarketFeatures(df=df).obv_calc()).chaikin_mf()).ma_spread()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.concat([features_tuple[0], features_tuple[1], features_tuple[2], \\\n",
    "                             features_tuple[3]], axis=1, sort=False)\n",
    "\n",
    "df_concat = pd.concat([features_df, df_w_market_features], axis=1, sort='False').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#right order of things\n",
    "\n",
    "y=np.asanyarray(df_concat['Duration'])\n",
    "df_final=df_concat.drop(columns=['TradedPrice','Duration','TradedTime'])\n",
    "X = df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "np.random.seed(0)\n",
    "# svr = GridSearchCV(SVR(kernel='rbf', gamma=0.1), cv=5,\n",
    "#                    param_grid={\"C\": [1e0, 1e1, 1e2, 1e3],\n",
    "#                                \"gamma\": np.logspace(-2, 2, 5)})\n",
    "# svr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_svr = svr.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "train_time = []\n",
    "# #############################################################################\n",
    "# Fit regression model\n",
    "t0 = time.time()\n",
    "# svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "svr_lin = SVR(kernel='linear', C=1e3)\n",
    "# svr_poly = SVR(kernel='poly', C=1e3, degree=2)\n",
    "# y_rbf = svr_rbf.fit(X, y).predict(X)\n",
    "y_lin = svr_lin.fit(X, y).predict(X)\n",
    "# y_poly = svr_poly.fit(X, y).predict(X)\n",
    "test_time.append(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features= datacls.ticker_features(date='20180215',model_date=test_date)\n",
    "fisher = all_features[0]\n",
    "info= all_features[1]\n",
    "gamma = all_features[2]\n",
    "ksi=all_features[3]\n",
    "print(fisher.shape, info.shape, gamma.shape, ksi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now need to write a function that concatenates all this shit and fits a model\n",
    "print gamma.shape\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datex=0 #which date we pick\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "for datex in range(0,10):\n",
    "    print(\"the model date is:\", list_models_dates[datex])\n",
    "    fwd_dates_list_for_date = fwd_dates(list_models_dates,list_models_dates[datex]) #dates for OOS\n",
    "\n",
    "    model_= models_list[datex] #model we will test\n",
    "\n",
    "    loaded_models=data_cls.open_pickle_file(ticker_models_path, model_)#load models\n",
    "\n",
    "    best_gboost=loaded_models['GBOOST']#best gradient boosting model\n",
    "    best_kr = loaded_models['KR'] #best kernel ridge\n",
    "    best_svc= loaded_models['SVC'] #best svc\n",
    "\n",
    "    #######start testing\n",
    "    for test_date_x in range(0,10):\n",
    "        test_date = fwd_dates_list_for_date[test_date_x]\n",
    "        print(\"you are testing on:\", test_date)\n",
    "        \n",
    "        test_features_pickle=data_cls.ticker_features(test_date)\n",
    "        test_labels = data_cls.ticker_labels_csv(date=test_date)\n",
    "        #one liner converting tuple into pandas dataframe of all the features\n",
    "        df_features_ = pd.concat([test_features_pickle[item]for item in range(0,4)], axis=1,sort=False) \n",
    "        #abels\n",
    "        all_labels=test_labels.drop(columns=['Duration','ReturnTradedPrice','states','TradedTime','TradedPrice','ticker'], axis=1)\n",
    "        idx=1\n",
    "        df_labels = all_labels.iloc[:, 0:idx]\n",
    "        df_concat=pd.concat([df_features_, df_labels], axis=1, sort=False)\n",
    "        df= df_concat.dropna()\n",
    "        label_column_loc = df.shape[1] - 1 #location of labels column\n",
    "        labels = df.iloc[:, label_column_loc:label_column_loc + 1]\n",
    "        features = df.drop(df.columns[label_column_loc], axis=1)\n",
    "        y_predict_gboost =best_gboost.predict(features)\n",
    "        y_predict_svc = best_svc.predict(features)\n",
    "        y_predict_kr =best_kr.predict(features)\n",
    "        print(prec_recall_report(labels,y_predict_gboost))\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "put this all below in a dictionary so you can manipulate it:\n",
    "    print('ROC AUC: %.3f' % roc_auc_score(y, y_predict))\n",
    "        print('Accuracy: %.2f' % accuracy_score(y, y_predict))\n",
    "        print('Precision: %.3f' % precision_score(y_true=y, y_pred=y_predict))\n",
    "        print('Recall: %.3f' % recall_score(y_true=y, y_pred=y_predict))\n",
    "        print('F1: %.3f' % f1_score(y_true=y, y_pred=y_predict))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_model_results= {\n",
    "    'clfs': np.empty((M, T)),\n",
    "    'model_date': np.empty((M, T)),\n",
    "    'data_date': np.empty((M, T)),\n",
    "    'ROC': np.empty((M, T)),\n",
    "    'accuracy' :np.empty((M, T))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##unit test for sequential prediction in sklearn###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_steps =features.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gboost.predict(features[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(1,5):\n",
    "    y_predict=best_svc.predict(features[:step])\n",
    "    print y_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_svc.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
