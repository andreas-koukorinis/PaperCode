{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_recall_fscore_support\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.multiclass import OneVsRestClassifier #support from multiclass\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####paths####\n",
    "labels_path = '/home/ak/Documents/Data/features_models/labels'\n",
    "main_path = '/home/ak/Documents/Data/features_models/'\n",
    "\n",
    "models_path=os.path.join(main_path,'models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 2] No such file or directory: '/media/ak/WorkDrive/Data/SYNT_2states'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5ef0dd027d15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/media/ak/WorkDrive/Data/SYNT_2states'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory: '/media/ak/WorkDrive/Data/SYNT_2states'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "os.listdir('/media/ak/WorkDrive/Data/SYNT_2states')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####OOP#####\n",
    "class DataLoader(object):\n",
    "    def __init__(self, path_, ticker):\n",
    "        self.main_path = path_\n",
    "        self.ticker = ticker\n",
    "        self.labels_path = os.path.join(self.main_path, 'labels')\n",
    "        self.features_path = os.path.join(self.main_path, 'features')\n",
    "        self.ticker_labels_path = os.path.join(self.labels_path, self.ticker)\n",
    "        self.ticker_features_path = os.path.join(self.features_path, self.ticker)\n",
    "\n",
    "    def ticker_features(self, date):\n",
    "        file_loc = os.path.join(self.ticker_features_path, str(date) + '.pickle')\n",
    "        with open(file_loc, 'rb') as handle:\n",
    "            ticker_features = pickle.load(handle)\n",
    "        return ticker_features\n",
    "\n",
    "    def ticker_labels_pickle(self, date):\n",
    "        file_loc = os.path.join(self.ticker_labels_path, str(date) + '.pickle')\n",
    "        with open(file_loc, 'rb') as handle:\n",
    "            ticker_labels = pickle.load(handle)\n",
    "        return ticker_labels\n",
    "\n",
    "    def ticker_labels_csv(self, date):\n",
    "        file_loc = os.path.join(self.ticker_labels_path, str(date) + '.csv')\n",
    "        ticker_labels = pd.read_csv(file_loc, index_col=0)\n",
    "        return ticker_labels\n",
    "\n",
    "    @staticmethod\n",
    "    def open_pickle_file(path, pickle_file):\n",
    "        file_loc = os.path.join(path, pickle_file)\n",
    "        pickle_to_file = pickle.load(open(file_loc, \"rb\"))\n",
    "        return pickle_to_file\n",
    "\n",
    "    @staticmethod\n",
    "    def get_date_from_file(file_, numb_):\n",
    "        return os.path.splitext(file_[numb_])[0]\n",
    "\n",
    "\n",
    "class PriceIndicators(object):\n",
    "    # a class to be expanded that uses features for base case -price only-indicators\n",
    "    \"\"\"\"Requires:\n",
    "    symbol - A stock symbol on which to form a strategy on.\n",
    "    short_window - Lookback period for short moving average.\n",
    "    long_window - Lookback period for long moving average.\"\"\"\n",
    "\n",
    "    def __init__(self, symbol, labels_df):\n",
    "        self.symbol = symbol\n",
    "        self.labels = labels_df\n",
    "\n",
    "    def MACD(self, short_window=5, long_window=20):\n",
    "        short_rolling_px = self.labels['TradedPrice'].rolling(window=short_window).mean()\n",
    "        long_rolling_px = self.labels['TradedPrice'].rolling(window=long_window).mean()\n",
    "        px_indx = long_rolling_px - short_rolling_px\n",
    "        return px_indx\n",
    "\n",
    "\n",
    "class FitModels(object):\n",
    "\n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "    # # Train a SVM classification model\n",
    "\n",
    "    def best_kernel_ridge(self, kernel_choice):\n",
    "\n",
    "        kr_clf =OneVsRestClassifier(GridSearchCV(KernelRidge(kernel=str(kernel_choice)), cv=10,\n",
    "                          param_grid={\"alpha\": [1e0, 0.1, 1e-2, 1e-3],\n",
    "                                      \"gamma\": np.logspace(-2, 2, 5)})).fit(self.X_train, self.y_train)\n",
    "\n",
    "        return kr_clf\n",
    "\n",
    "    def best_svm_clf(self, kernel_choice):\n",
    "\n",
    "        param_grid = dict(kernel=[str(kernel_choice)],\n",
    "                          C=[1, 5, 10, 25, 50, 100],\n",
    "                          gamma=[0.0001, 0.001, 0.01, 0.02, 0.05, 0.01])\n",
    "\n",
    "        clf = OneVsRestClassifier(\n",
    "            GridSearchCV(SVC(class_weight='balanced'), param_grid, verbose=1, n_jobs=-1, cv=10))\\\n",
    "            .fit(self.X_train, self.y_train)\n",
    "        return clf\n",
    "\n",
    "    def best_gradient_boost_clf(self):\n",
    "        #this needs to be written properly- just a baseline placeholder here!\n",
    "        GBR = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                           max_depth=4, max_features='sqrt',\n",
    "                                           min_samples_leaf=15, min_samples_split=10,loss='huber',\n",
    "                                        random_state=5)\n",
    "\n",
    "        gb_boost_clf = OneVsRestClassifier(GBR).fit(self.X_train, self.y_train)\n",
    "\n",
    "        return gb_boost_clf\n",
    "    def best_MKL_clf(self):\n",
    "        pass\n",
    "    def best_knn_clf(self):\n",
    "        pass\n",
    "    def best_random_forest_clf(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def run_cv(self, clf_class, **kwargs):\n",
    "        # Construct a kfolds object\n",
    "        kf = KFold(len(self.y_train), n_folds=10, shuffle=True)\n",
    "        y_pred = self.y_train.copy()\n",
    "\n",
    "        # Iterate through folds\n",
    "        for train_index, test_index in kf:\n",
    "            X_train_local, X_test_local = self.X_train[train_index], self.X_train[test_index]\n",
    "            y_train_local = self.y_train[train_index]\n",
    "            # Initialize a classifier with key word arguments\n",
    "            clf = clf_class(**kwargs)\n",
    "            clf.fit(self.X_train, self.y_train)\n",
    "            y_pred[test_index] = clf.predict(X_test_local)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "class PredictModels(FitModels):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "def no_nans(label):\n",
    "    return np.sum(np.isnan(label))\n",
    "\n",
    "\n",
    "def remove_last_element(arr):\n",
    "    return arr[np.arange(arr.size - 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_dates(_dates_list, _key_date):\n",
    "    #returns a list of dates that are forward from the key_date\n",
    "    fwd_dates_list  = [i for i in _dates_list if i > _key_date]\n",
    "    return fwd_dates_list\n",
    "\n",
    "def prec_recall_report(y_true_, y_predict_):\n",
    "    #function to ge the sci-kit learn classification metrics into a pretty DF for csv!\n",
    "    report = pd.DataFrame(list(precision_recall_fscore_support(y_true_, y_predict_)),\n",
    "                          index=['Precision', 'Recall', 'F1-score', 'Support']).T\n",
    "    # Now add the 'Avg/Total' row\n",
    "    report.loc['Avg/Total', :] = precision_recall_fscore_support(y_true_, y_predict_, average='weighted')\n",
    "    report.loc['Avg/Total', 'Support'] = report['Support'].sum()\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "###below may be unnecessary\n",
    "\n",
    "ticker = 'SYNT_3states'\n",
    "\n",
    "features_path = os.path.join(main_path, 'features')\n",
    "\n",
    "ticker_labels_path = os.path.join(labels_path, ticker)\n",
    "ticker_models_path = os.path.join(models_path, ticker)\n",
    "\n",
    "ticker_features_path = os.path.join(features_path, ticker)\n",
    "\n",
    "###\n",
    "\n",
    "# list of files    # list of files\n",
    "labels_list = os.listdir(ticker_labels_path)\n",
    "# clfs=[LR, GBC, RF, KNN]\n",
    "\n",
    "features_list = os.listdir(ticker_features_path)\n",
    "models_list= os.listdir(ticker_models_path)\n",
    "\n",
    "####\n",
    "data_cls = DataLoader(path_=main_path, ticker=ticker)\n",
    "#     idx = 1  # take first label-index for the data-frame of labels\n",
    "#     no_files=len(features_list)\n",
    "#     ticker_models_path = os.path.join(models_path, ticker)\n",
    "#     ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_models_dates=[]\n",
    "for model_id, _ in enumerate(models_list):\n",
    "    list_models_dates.append(models_list[model_id].split(\"_\")[0])\n",
    "\n",
    "# [mm.split(\"_\")[0] for mm in models_list]\n",
    "# result=fwd_dates(list_models_dates,list_models_dates[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-0cf68fd07416>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdatex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the model date is:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_models_dates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mfwd_dates_list_for_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfwd_dates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_models_dates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist_models_dates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#dates for OOS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#datex=0 #which date we pick\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "for datex in range(0,10):\n",
    "    print(\"the model date is:\", list_models_dates[datex])\n",
    "    fwd_dates_list_for_date = fwd_dates(list_models_dates,list_models_dates[datex]) #dates for OOS\n",
    "\n",
    "    model_= models_list[datex] #model we will test\n",
    "\n",
    "    loaded_models=data_cls.open_pickle_file(ticker_models_path, model_)#load models\n",
    "\n",
    "    best_gboost=loaded_models['GBOOST']#best gradient boosting model\n",
    "    best_kr = loaded_models['KR'] #best kernel ridge\n",
    "    best_svc= loaded_models['SVC'] #best svc\n",
    "\n",
    "    #######start testing\n",
    "    for test_date_x in range(0,10):\n",
    "        test_date = fwd_dates_list_for_date[test_date_x]\n",
    "        print(\"you are testing on:\", test_date)\n",
    "        \n",
    "        test_features_pickle=data_cls.ticker_features(test_date)\n",
    "        test_labels = data_cls.ticker_labels_csv(date=test_date)\n",
    "        #one liner converting tuple into pandas dataframe of all the features\n",
    "        df_features_ = pd.concat([test_features_pickle[item]for item in range(0,4)], axis=1,sort=False) \n",
    "        #abels\n",
    "        all_labels=test_labels.drop(columns=['Duration','ReturnTradedPrice','states','TradedTime','TradedPrice','ticker'], axis=1)\n",
    "        idx=1\n",
    "        df_labels = all_labels.iloc[:, 0:idx]\n",
    "        df_concat=pd.concat([df_features_, df_labels], axis=1, sort=False)\n",
    "        df= df_concat.dropna()\n",
    "        label_column_loc = df.shape[1] - 1 #location of labels column\n",
    "        labels = df.iloc[:, label_column_loc:label_column_loc + 1]\n",
    "        features = df.drop(df.columns[label_column_loc], axis=1)\n",
    "        y_predict_gboost =best_gboost.predict(features)\n",
    "        y_predict_svc = best_svc.predict(features)\n",
    "        y_predict_kr =best_kr.predict(features)\n",
    "        print(prec_recall_report(labels,y_predict_gboost))\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "put this all below in a dictionary so you can manipulate it:\n",
    "    print('ROC AUC: %.3f' % roc_auc_score(y, y_predict))\n",
    "        print('Accuracy: %.2f' % accuracy_score(y, y_predict))\n",
    "        print('Precision: %.3f' % precision_score(y_true=y, y_pred=y_predict))\n",
    "        print('Recall: %.3f' % recall_score(y_true=y, y_pred=y_predict))\n",
    "        print('F1: %.3f' % f1_score(y_true=y, y_pred=y_predict))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_model_results= {\n",
    "    'clfs': np.empty((M, T)),\n",
    "    'model_date': np.empty((M, T)),\n",
    "    'data_date': np.empty((M, T)),\n",
    "    'ROC': np.empty((M, T)),\n",
    "    'accuracy' :np.empty((M, T))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##unit test for sequential prediction in sklearn###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_steps =features.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_gboost.predict(features[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n",
      "[0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for step in range(1,5):\n",
    "    y_predict=best_svc.predict(features[:step])\n",
    "    print y_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_svc.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
