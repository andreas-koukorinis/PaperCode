{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 2,
>>>>>>> ba910a9cf42d30c0b7bfd8e0b793f20d062d1b98
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "sc = StandardScaler()\n",
    "import os\n",
    "import pickle\n",
    "import fnmatch\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, recall_score, precision_score\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 3,
>>>>>>> ba910a9cf42d30c0b7bfd8e0b793f20d062d1b98
   "metadata": {},
   "outputs": [],
   "source": [
    "##useful functions\n",
    "\n",
    "def fwd_dates(_dates_list, _key_date):\n",
    "    # returns a list of dates that are forward from the key_date\n",
    "    fwd_dates_list = [i for i in _dates_list if i > _key_date]\n",
    "    return fwd_dates_list\n",
    "\n",
    "def common_member(a, b): \n",
    "      \n",
    "    a_set = set(a) \n",
    "    b_set = set(b) \n",
    "      \n",
    "    # check length  \n",
    "    if len(a_set.intersection(b_set)) > 0: \n",
    "        return(a_set.intersection(b_set))   \n",
    "    else: \n",
    "        return(\"no common elements\") \n",
    "\n",
    "\n",
    "def remove_nans(features_tuple, labels, idx=1):\n",
    "    # not the cleanest but useful\n",
    "    # function to clean up nans as I seem to use it a lot, so better to have one function\n",
    "    # combines the features and labels and removes rows with nans across so we dont lose the ordering\n",
    "    # returns features and labels\n",
    "    features_df = pd.concat([features_tuple[0], features_tuple[1], features_tuple[2], \\\n",
    "                             features_tuple[3]], axis=1, sort=False)\n",
    "    labels_only = labels.drop(columns=['ReturnTradedPrice', 'Duration', 'states', 'TradedTime',\n",
    "                                       'TradedPrice'], axis=1)\n",
    "    df_concat = pd.concat([features_df, labels_only.iloc[:, 0:idx]], axis=1, sort='False')\n",
    "    # only using 1st set of labels- but we can re-write this a bit\n",
    "    df_x_nan = df_concat.dropna()  # dropping all nans\n",
    "    label_column_loc_ = df_x_nan.shape[1] - 1  # location of labels column in the clean df\n",
    "    labels_ = df_x_nan.iloc[:, label_column_loc_:label_column_loc_ + 1]  # keep pure labels\n",
    "    features_ = df_x_nan.drop(df_x_nan.columns[label_column_loc_], axis=1)  # keeping the features only\n",
    "    return features_, labels_\n",
    "\n",
    "\n",
    "def prec_recall_report(y_true, y_predict):\n",
    "    # function to ge the sci-kit learn classification metrics into a pretty DF for csv!\n",
    "    report = pd.DataFrame(list(precision_recall_fscore_support(y_true, y_predict)),\n",
    "                          index=['Precision', 'Recall', 'F1-score', 'Support']).T\n",
    "    # Now add the 'Avg/Total' row\n",
    "    report.loc['Avg/Total', :] = precision_recall_fscore_support(y_true, y_predict, average='weighted')\n",
    "    report.loc['Avg/Total', 'Support'] = report['Support'].sum()\n",
    "    return report\n",
    "\n",
    "\n",
    "class DataLoader(object):\n",
    "    def __init__(self, path_main, ticker):\n",
    "        self.main_path = path_main\n",
    "        self.ticker = ticker\n",
    "\n",
    "        self.features_labels_path = os.path.join(self.main_path, 'features_models')\n",
    "        self.features_path = os.path.join(self.features_labels_path, 'features')\n",
    "        # collection of per symbol non directional labels\n",
    "        self.labels_path = os.path.join(self.features_labels_path, 'labels', self.ticker, 'NON_DIRECTIONAL')\n",
    "        self.symbol_features_path = os.path.join(self.features_labels_path, 'features', self.ticker, 'MODEL_BASED')\n",
    "        # list of all the model -oos hmm feature dates - each folder is a collection of oos feature dates\n",
    "        self.hmm_dates_list = os.listdir(self.symbol_features_path)  # each folder are the OOS features from each HMM\n",
    "        self.compute_date = os.listdir(os.path.join( \\\n",
    "            self.symbol_features_path, \\\n",
    "            os.listdir(self.symbol_features_path)[1]))[1].split(\"_\")[7]\n",
    "\n",
    "    def ticker_features(self, model_date, date):\n",
    "        # need to make this a lot more flexible with number of states\n",
    "        if model_date < date:\n",
    "            file_name = \"_\".join(\n",
    "                (self.ticker, '3', 'states', 'features', 'date:', date, 'now:', self.compute_date, '.pickle'))\n",
    "            file_loc = os.path.join(self.symbol_features_path, str(model_date), file_name)\n",
    "            with open(file_loc, 'rb') as handle:\n",
    "                ticker_features = pickle.load(handle)\n",
    "        else:\n",
    "            print('Loading Feature Date which is in-sample. Change your Model Date')\n",
    "        return ticker_features\n",
    "\n",
    "    def ticker_labels_csv(self, date):\n",
    "        file_loc = os.path.join(self.labels_path, str(date) + '.csv')\n",
    "        ticker_labels = pd.read_csv(file_loc, index_col=0)\n",
    "        return ticker_labels\n",
    "\n",
    "    @staticmethod\n",
    "    def open_pickle_file(path, pickle_file):\n",
    "        file_loc = os.path.join(path, pickle_file)\n",
    "        pickle_to_file = pickle.load(open(file_loc, \"rb\"))\n",
    "        return pickle_to_file\n",
    "\n",
    "    @staticmethod\n",
    "    def get_date_from_file(file_, numb_):\n",
    "        return os.path.splitext(file_[numb_])[0]\n",
    "\n",
    "class MarketFeatures(object):\n",
    "    # a class to be expanded that uses features for base case -market based only-indicators/features\n",
    "    \"\"\"\"Requires:\n",
    "    a dataframe that has TradedPrice And Volume columns\n",
    "    symbol - A stock symbol on which to form a strategy on.\n",
    "    short_window - Lookback period for short moving average.\n",
    "    long_window - Lookback period for long moving average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        #         self.ticker = ticker\n",
    "        self.df = df\n",
    "\n",
    "    def load_data(self):\n",
    "        pass\n",
    "\n",
    "    def ma_spread(self, short_window=5, long_window=20):\n",
    "        # function that produces the MA spread, which can be used on its own or as an input for MACD\n",
    "        short_rolling_px = self.df['TradedPrice'].rolling(window=short_window).mean()\n",
    "        long_rolling_px = self.df['TradedPrice'].rolling(window=long_window).mean()\n",
    "        px_name = \"_\".join(('px_indx', str(short_window), str(long_window)))\n",
    "        self.df[px_name] = long_rolling_px - short_rolling_px\n",
    "        return self.df\n",
    "\n",
    "    def ma_spread_duration(self, short_window=5, long_window=20):\n",
    "        # function that produces the MA spread, which can be used on its own or as an input for MACD\n",
    "        short_rolling_px = self.df['Duration'].rolling(window=short_window).mean()\n",
    "        long_rolling_px = self.df['Duration'].rolling(window=long_window).mean()\n",
    "        dur_name = \"_\".join(('dur_indx', str(short_window), str(long_window)))\n",
    "        self.df[dur_name] = long_rolling_px - short_rolling_px\n",
    "        return self.df\n",
    "\n",
    "    def obv_calc(self):\n",
    "        # on balance volume indicator\n",
    "        self.df['SignedVolume'] = self.df['Volume'] * np.sign(self.df['TradedPrice'].diff()).cumsum()\n",
    "        self.df['SignedVolume'].iat[1] = 0\n",
    "        self.df['OBV'] = self.df['SignedVolume']  # .cumsum()\n",
    "        self.df = self.df.drop(columns=['SignedVolume'])\n",
    "        return self.df\n",
    "\n",
    "    def chaikin_mf(self, period=5):\n",
    "        # Chaikin money flow indicator\n",
    "        self.df[\"MF Multiplier\"] = (self.df['TradedPrice'] - (self.df['TradedPrice'].expanding(period).min()) \\\n",
    "                                    - (self.df['TradedPrice'].expanding(period).max() \\\n",
    "                                       - self.df['TradedPrice'])) / (\n",
    "                                           self.df['TradedPrice'].expanding(period).max() - self.df[ \\\n",
    "                                       'TradedPrice'].expanding(period).min())\n",
    "        self.df[\"MF Volume\"] = self.df['MF Multiplier'] * self.df['Volume']\n",
    "        self.df['CMF_' + str(period)] = self.df['MF Volume'].sum() / self.df[\"Volume\"].rolling(period).sum()\n",
    "        self.df = self.df.drop(columns=['MF Multiplier', 'MF Volume'])\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": 4,
>>>>>>> ba910a9cf42d30c0b7bfd8e0b793f20d062d1b98
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['metrics', 'models']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# passport locations\n",
    "\n",
    "# data_oos_loc= ('/media/ak/My Passport/Experiment Data')\n",
    "# passport = ('/media/ak/My Passport')\n",
    "# experimental_data = os.path.join(passport, 'Experiment Data')\n",
    "# features_loc = os.path.join(data_oos_loc,'features') #\n",
    "# labels_loc = os.path.join(data_oos_loc,'labels')\n",
    "# models_loc = os.path.join(data_oos_loc,'models')\n",
    "# # symbols to use as a starting point\n",
    "# good_symbols = os.listdir(features_loc) \n",
    "# folder= '/media/ak/'\n",
    "mnt= '/mnt'\n",
    "data_drive = os.path.join('/mnt/',os.listdir(mnt)[1])\n",
    "os.listdir(data_drive)\n",
    "data_oos = os.path.join(data_drive, 'Data')\n",
    "os.listdir(data_oos)\n",
    "data_oos_loc = os.path.join(data_oos,'features_models')\n",
    "os.listdir(data_oos_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LAND.L',\n",
       " 'AAL.L',\n",
       " 'APF.L',\n",
       " 'AV.L',\n",
       " 'AZN.L',\n",
       " 'BARC.L',\n",
       " 'BATS.L',\n",
       " 'BLT.L',\n",
       " 'CCL.L',\n",
       " 'CEY.L',\n",
       " 'clfs_model_hash.pickle',\n",
       " 'CNA.L',\n",
       " 'CPG.L',\n",
       " 'CPI.L',\n",
       " 'CRH.I',\n",
       " 'DGE.L',\n",
       " 'DMGOa.L',\n",
       " 'ECM.L',\n",
       " 'EGS.L',\n",
       " 'GKN.L',\n",
       " 'HSBA.L',\n",
       " 'IEER.L',\n",
       " 'III.L',\n",
       " 'IOG.L',\n",
       " 'ITV.L',\n",
       " 'KGF.L',\n",
       " 'LGEN.L',\n",
       " 'LLOY.L',\n",
       " 'MAB.L',\n",
       " 'MKS.L',\n",
       " 'NG.L',\n",
       " 'PRU.L',\n",
       " 'PSON.L',\n",
       " 'RB.L',\n",
       " 'RBS.L',\n",
       " 'RDSa.L',\n",
       " 'RDSb.L',\n",
       " 'REL.L',\n",
       " 'RR.L',\n",
       " 'RSA.L',\n",
       " 'RTO.L',\n",
       " 'SDR.L',\n",
       " 'SGE.L',\n",
       " 'SHP.L',\n",
       " 'SMIN.L',\n",
       " 'SPT.L',\n",
       " 'STAN.L',\n",
       " 'TSCO.L',\n",
       " 'ULVR.L',\n",
       " 'UU.L',\n",
       " 'VOD.L',\n",
       " 'WPP.L']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(models_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a symbol that works\n",
    "symbol ='MKS.L'\n",
    "\n",
    "syml_features_loc = os.path.join(features_loc,symbol,'MODEL_BASED') #create the symbol feature locations\n",
    "syml_models_loc = os.path.join(models_loc,symbol,'SINGLE_KERNEL') #create the symbol model locations\n",
    "syml_labels_loc = os.path.join(labels_loc,symbol,'NON_DIRECTIONAL')# create the symbol labels location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists of dates\n",
    "labels_dates = [os.listdir(syml_labels_loc)[idx].split(\".\")[0] for idx, _ in enumerate(os.listdir(syml_labels_loc))]\n",
    "features_dates= os.listdir(syml_features_loc)\n",
    "model_dates= [os.listdir(syml_models_loc)[idx].split(\"_\")[1] for idx,_ in enumerate(os.listdir(syml_models_loc))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_dates=sorted(list(common_member(sorted(list(common_member(features_dates, model_dates))),labels_dates)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#symbol_compute_date\n",
    "symbol_compute_date = os.listdir(os.path.join(syml_features_loc,os.listdir(syml_features_loc)[1]))[1].split(\"_\")[7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model date- this date corresponds to the model which was used for out of sample\n",
    "date_idx = 1 \n",
    "# #contains the list of detailed features for the specific model date\n",
    "# os.listdir(os.path.join(syml_features_loc, common_dates[date_idx])) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MKS.L\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20170920')\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20170921')\n",
      "skipping\n",
      "skipping\n",
      "skipping\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20170922')\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20170925')\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20170926')\n",
      "skipping\n",
      "skipping\n",
      "skipping\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20170927')\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20170928')\n",
      "skipping\n",
      "skipping\n",
      "skipping\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20170929')\n",
      "skipping\n",
      "skipping\n",
      "skipping\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180201')\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180202')\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180205')\n",
      "skipping\n",
      "skipping\n",
      "skipping\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180206')\n",
      "skipping\n",
      "skipping\n",
      "skipping\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180207')\n",
      "skipping\n",
      "skipping\n",
      "skipping\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180208')\n",
      "skipping\n",
      "skipping\n",
      "skipping\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180209')\n",
      "skipping\n",
      "skipping\n",
      "skipping\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180212')\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180213')\n",
      "skipping\n",
      "skipping\n",
      "skipping\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180214')\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180215')\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180216')\n",
      "skipping\n",
      "skipping\n",
      "skipping\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180219')\n",
      "skipping\n",
      "skipping\n",
      "skipping\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180220')\n",
      "skipping\n",
      "skipping\n",
      "skipping\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180221')\n",
      "skipping\n",
      "skipping\n",
      "skipping\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180222')\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180223')\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180226')\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180227')\n",
      "skipping\n",
      "skipping\n",
      "skipping\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180228')\n",
      "skipping\n",
      "skipping\n",
      "skipping\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180403')\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180404')\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180405')\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180406')\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180409')\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180410')\n",
      "skipping\n",
      "skipping\n",
      "skipping\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180411')\n",
      "skipping\n",
      "skipping\n",
      "skipping\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180412')\n",
      "skipping\n",
      "skipping\n",
      "skipping\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180413')\n",
      "skipping\n",
      "skipping\n",
      "skipping\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180416')\n",
      "skipping\n",
      "skipping\n",
      "skipping\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180417')\n",
      "skipping\n",
      "skipping\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180418')\n",
      "skipping\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180419')\n",
      "('Your symbol is:', 'MKS.L', 'and the model date is:', '20180420')\n"
     ]
    }
   ],
   "source": [
    "print symbol\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "#create a time-delta which will be needed for the common date\n",
    "daysdelta=timedelta(days=1)\n",
    "\n",
    "accuracy_results=defaultdict(dict)\n",
    "accuracy_models_results=defaultdict(dict)\n",
    "recall_models_results=defaultdict(dict)\n",
    "f1_models_results=defaultdict(dict)\n",
    "\n",
    "# now lets take all the model directories and locations##\n",
    "\n",
    "for common_date in common_dates:\n",
    "    #move the common date (which is the hmm date, and folder name) one dat forward\n",
    "    common_day_start=datetime.strptime(common_date, '%Y%m%d') +daysdelta\n",
    "    #convert it back to a string (prob we can do in one go)\n",
    "    first_oos_day= common_day_start.strftime('%Y%m%d')\n",
    "    # getting the model\n",
    "    model_name =\"_\".join((symbol,common_date,'label_PrMov__window_5__thres_arbitrary__0.1_clf_.pickle'))\n",
    "    model_pickle=os.path.join(syml_models_loc, model_name)\n",
    "\n",
    "    pickle_to_file = pickle.load(open(model_pickle, \"rb\")) #load your model\n",
    "\n",
    "    best_estimator = pickle_to_file['SVC'] #.best_estimator_\n",
    "    print('Your symbol is:', symbol, 'and the model date is:' ,common_date)\n",
    "    # set a few OOS dates\n",
    "    \n",
    "    fwd_dates_list = sorted([i for i in common_dates if i > first_oos_day])[:3]\n",
    "    \n",
    "    fitted_models_results = {\n",
    "            'accuracy': defaultdict(dict),\n",
    "            'recall': defaultdict(dict),\n",
    "            'F1-score': defaultdict(dict)\n",
    "    }\n",
    "    oos_features_path= os.path.join(syml_features_loc, common_date)\n",
    "    for dic_idx, fwd_date in enumerate(fwd_dates_list):\n",
    "        feature_file = \"_\".join((symbol,'3_states_features_date:',fwd_date,'now:',symbol_compute_date,'.pickle'))\n",
    "        features_loc = os.path.join(syml_features_loc,common_date, feature_file)\n",
    "        features_tuple=pickle.load(open(features_loc, \"rb\"))\n",
    "        market_data_oos= pd.read_csv(os.path.join(syml_labels_loc, \n",
    "                                                  '.'.join((fwd_date,'csv'))),index_col=0)\n",
    "        features_df = pd.concat([features_tuple[0], features_tuple[1],\n",
    "                             features_tuple[2], features_tuple[3]], axis=1)\n",
    "        df_w_market_features = MarketFeatures(df=MarketFeatures(\\\n",
    "                                                                df=MarketFeatures(\n",
    "                        df=MarketFeatures(df=market_data_oos).obv_calc()).chaikin_mf()).ma_spread()).ma_spread_duration()\n",
    "# fix the sort issue!!\n",
    "        df_concat = pd.concat([features_df, df_w_market_features], axis=1).dropna()\n",
    "\n",
    "        label_name = str(df_concat.columns[df_concat.columns.str.contains(pat='label')].values[0])\n",
    "\n",
    "        df_final = df_concat.drop(columns=['TradedPrice', 'Duration', 'TradedTime', 'ReturnTradedPrice', \\\n",
    "                                           'Volume', label_name])\n",
    "        if len(df_final)> 5:\n",
    "            X = MinMaxScaler().fit_transform(df_final)\n",
    "\n",
    "            y_labels = df_concat[df_concat.columns[df_concat.columns.str.contains(pat='label')]].iloc[:, 0]\n",
    "            y_predict = best_estimator.predict(X)\n",
    "            accuracy_models_results[common_date][datetime.strptime(fwd_date, '%Y%m%d').strftime('%Y%m%d')] =(accuracy_score(y_labels, y_predict))\n",
    "            recall_models_results[common_date][datetime.strptime(fwd_date, '%Y%m%d').strftime('%Y%m%d')] =(recall_score(y_labels,y_predict))\n",
    "            f1_models_results[common_date][datetime.strptime(fwd_date, '%Y%m%d').strftime('%Y%m%d')]=f1_score(y_true= y_labels, y_pred=y_predict)\n",
    "        else:\n",
    "            print ('skipping')\n",
    "fitted_models_results['accuracy']= accuracy_models_results\n",
    "fitted_models_results['recall']= recall_models_results\n",
    "fitted_models_results['F1-score']= f1_models_results\n",
    "symbol_results_metrics.pickle\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df=pd.DataFrame(fitted_models_results['accuracy'])\n",
    "df1 = results_df.replace(np.nan, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_ids = []\n",
    "# frames = []\n",
    "\n",
    "# for user_id, d in user_dict.iteritems():\n",
    "#     user_ids.append(user_id)\n",
    "#     frames.append(pd.DataFrame.from_dict(d, orient='index'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20180215\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-af0fe7af8059>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfitted_models_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mkey1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0mkey2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'mean'"
     ]
    }
   ],
   "source": [
    "date_1 = []\n",
    "date_2 = []\n",
    "\n",
    "# for user_id, d in user_dict.iteritems():\n",
    "#     date_1.append(user_id)\n",
    "#     frames.append(pd.DataFrame.from_dict(d, orient='index'))\n",
    "for key1, key2 in fitted_models_results['accuracy'].iteritems():\n",
    "    print key1\n",
    "    print key2.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
