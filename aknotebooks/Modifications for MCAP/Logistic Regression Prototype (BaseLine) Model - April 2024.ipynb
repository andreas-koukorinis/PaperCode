{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "2cf20697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from collections import defaultdict\n",
    "##################################\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import the class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, f1_score, recall_score, accuracy_score, classification_report\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report, roc_auc_score, confusion_matrix, matthews_corrcoef, log_loss\n",
    "from sklearn.preprocessing import LabelBinarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "3a13ef91",
   "metadata": {},
   "outputs": [],
   "source": [
    "### class with features \n",
    "\n",
    "class CreateMarketFeatures:\n",
    "    \"\"\"A class to compute market-based features for trading strategies.\n",
    "\n",
    "    Requires:\n",
    "    df: pandas DataFrame with columns TradedPrice, Volume, and optionally Duration.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def ma_spread(self, short_window=5, long_window=10):\n",
    "        \"\"\"Calculate moving average spread between short and long windows for TradedPrice.\"\"\"\n",
    "        short_ma = self.df['TradedPrice'].rolling(window=short_window, min_periods=1).mean()\n",
    "        long_ma = self.df['TradedPrice'].rolling(window=long_window, min_periods=1).mean()\n",
    "        self.df['ma_spread'] = long_ma - short_ma\n",
    "        return self.df\n",
    "    def include_label_columns(self):\n",
    "        \"\"\"Include columns that contain 'label' in their name into the DataFrame.\"\"\"\n",
    "        # Find all columns that have 'label' in their name\n",
    "        label_cols = [col for col in self.df.columns if 'label' in col]\n",
    "        # Check if label columns are found and handle them\n",
    "        if label_cols:\n",
    "            # The columns are already part of the DataFrame, so this function might just ensure they are treated correctly\n",
    "            # Here you can add additional handling or processing of label columns if necessary\n",
    "            print(f\"Label columns included: {label_cols}\")\n",
    "        else:\n",
    "            # Assuming default action if no label columns are present\n",
    "            print(\"No label columns found.\")\n",
    "\n",
    "    def obv_calc(self):\n",
    "        \"\"\"Calculate On-Balance Volume (OBV) to measure buying and selling pressure.\"\"\"\n",
    "        price_diff = self.df['TradedPrice'].diff()\n",
    "        volume_direction = self.df['Volume'] * np.sign(price_diff)\n",
    "        self.df['OBV'] = volume_direction.cumsum()\n",
    "        return self.df\n",
    "\n",
    "    def chaikin_mf(self, period=5):\n",
    "        \"\"\"Calculate Chaikin Money Flow (CMF) to measure money flow volume over a set period.\"\"\"\n",
    "        high = self.df['TradedPrice'].rolling(window=period, min_periods=1).max()\n",
    "        low = self.df['TradedPrice'].rolling(window=period, min_periods=1).min()\n",
    "        close = self.df['TradedPrice']\n",
    "        \n",
    "        cmf_multiplier = ((close - low) - (high - close)) / (high - low).replace(0, np.nan)\n",
    "        cmf_volume = cmf_multiplier * self.df['Volume']\n",
    "        self.df['CMF'] = cmf_volume.rolling(window=period, min_periods=1).sum() / \\\n",
    "                         self.df['Volume'].rolling(window=period, min_periods=1).sum()\n",
    "        self.df['CMF'].fillna(0, inplace=True)  # Handle NaN divisions\n",
    "        return self.df\n",
    "\n",
    "    def add_labels(self):\n",
    "        \"\"\"Placeholder to demonstrate how to add labels if needed.\"\"\"\n",
    "        # Assuming label logic or addition is done outside or here based on certain conditions\n",
    "        pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "cd48337e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of Logistic Regression Classifier and fit the data.\n",
    "logreg = LogisticRegression(C=1e5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d9ee6f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "FeaturesDir = '/media/ak/DataOnly1/SymbolFeatureDirectories/'\n",
    "LabelOne='/media/ak/DataOnly1/ExperimentCommonLocs/LabelsAlternateOne'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "826125a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common elements: ['AAL.L', 'APF.L', 'AV.L', 'AZN.L', 'BARC.L', 'BATS.L', 'BLT.L', 'CCL.L', 'CEY.L', 'CPG.L', 'ITV.L', 'KGF.L', 'LAND.L', 'LGEN.L', 'LLOY.L', 'MAB.L', 'MKS.L', 'NG.L', 'PRU.L', 'PSON.L', 'RB.L', 'RBS.L', 'RDSa.L', 'RDSb.L', 'REL.L', 'RR.L', 'RSA.L', 'RTO.L', 'SDR.L', 'SGE.L', 'SHP.L', 'SMIN.L', 'SPT.L', 'STAN.L', 'TSCO.L', 'ULVR.L', 'UU.L', 'VOD.L', 'WPP.L']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Paths to the directories\n",
    "FeaturesDir = '/media/ak/DataOnly1/SymbolFeatureDirectories/'\n",
    "LabelOne = '/media/ak/DataOnly1/ExperimentCommonLocs/LabelsAlternateOne'\n",
    "\n",
    "# Get list of files in each directory\n",
    "features_files = set(os.listdir(FeaturesDir))\n",
    "labels_files = set(os.listdir(LabelOne))\n",
    "\n",
    "# Find common elements\n",
    "common_elements = features_files.intersection(labels_files)\n",
    "# Convert set to list\n",
    "common_elements_list = sorted(list(common_elements))\n",
    "# Print the common elements\n",
    "print(\"Common elements:\", common_elements_list)\n",
    "symbol = common_elements_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a1e06c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_common_elements(features_dates, labelDates):\n",
    "    # Convert lists to sets to find common elements\n",
    "    common_elements = set(features_dates).intersection(labelDates)\n",
    "    \n",
    "    # Convert the set back to a list to return\n",
    "    return list(common_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8249fe26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "87e81ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_directory_contents_by_date(date, FeaturesDir, LabelOne):\n",
    "    # List the contents of both directories\n",
    "    features_files = os.listdir(FeaturesDir)\n",
    "    labels_files = os.listdir(LabelOne)\n",
    "    \n",
    "    # Prepare the paths\n",
    "    features_date_path = os.path.join(FeaturesDir, date)\n",
    "    labels_date_file = f\"{date}.csv\"\n",
    "    labels_date_path = os.path.join(LabelOne, labels_date_file)\n",
    "    \n",
    "    # Verify the existence of the date directory in features and date file in labels\n",
    "    if os.path.isdir(features_date_path) and labels_date_file in labels_files:\n",
    "        return {\n",
    "            'Features Directory Content': os.listdir(features_date_path),\n",
    "            'Labels File Path': labels_date_path\n",
    "        }\n",
    "    else:\n",
    "        return \"The specified date directory or file does not exist in one or both locations.\"\n",
    "\n",
    "# Example usage\n",
    "FeaturesDir = '/media/ak/DataOnly1/SymbolFeatureDirectories/'\n",
    "LabelOne = '/media/ak/DataOnly1/ExperimentCommonLocs/LabelsAlternateOne/'\n",
    "date = '20170829'  # Change this date to your specific requirement\n",
    "\n",
    "# # Call the function\n",
    "# result = get_directory_contents_by_date(date, FeaturesDir, LabelOne)\n",
    "# print(result)\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "# FeaturesDir = '/media/ak/DataOnly1/SymbolFeatureDirectories/'\n",
    "# LabelOne = '/media/ak/DataOnly1/ExperimentCommonLocs/LabelsAlternateOne'\n",
    "\n",
    "\n",
    "# # # Call the function\n",
    "# # result = find_common_subdirectories(index, FeaturesDir, LabelOne)\n",
    "# # FeaturesDirectoryContent = result['Features Directory Content']\n",
    "# # IndexDate =1 \n",
    "# # DateId = FeaturesDirectoryContent[IndexDate]\n",
    "# # FeaturesDateDir = os.path.join(FeaturesDir, DateId)\n",
    "\n",
    "def check_date_in_list(common_dates, DatesIdx, check_list):\n",
    "    try:\n",
    "        # Build the string from the common_dates list at index DatesIdx\n",
    "        date_with_extension = common_dates[DatesIdx] + '.csv'\n",
    "        \n",
    "        # Check if this string is in the provided list\n",
    "        is_present = date_with_extension in check_list\n",
    "        \n",
    "        return is_present\n",
    "    except IndexError:\n",
    "        return \"Index is out of range\"  # Handle the case where DatesIdx is out of the list bounds\n",
    "\n",
    "# # Example usage\n",
    "# common_dates = ['2023-04-04', '2023-04-05']\n",
    "# DatesIdx = 1\n",
    "# check_list = ['2023-04-05.csv', '2023-04-06.csv']\n",
    "\n",
    "# # Call the function and print the result\n",
    "# result = check_date_in_list(common_dates, DatesIdx, check_list)\n",
    "# print(\"Is the date file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6fc903c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = list(set(os.listdir(FeaturesDir)).intersection(set(os.listdir(LabelOne))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "62151b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Features Path': '/media/ak/DataOnly1/SymbolFeatureDirectories/APF.L/MODEL_BASED', 'Labels Path': '/media/ak/DataOnly1/ExperimentCommonLocs/LabelsAlternateOne/APF.L'}\n",
      "['20170711', '20170712', '20170713', '20170714', '20170717', '20170718', '20170719', '20170720', '20170721', '20170724', '20170725', '20170726', '20170727', '20170728', '20170731', '20170801', '20170802', '20170803', '20170804', '20170807', '20170808', '20170809', '20170810', '20170811', '20170814', '20170815', '20170816', '20170817', '20170818', '20170821', '20170822', '20170823', '20170824', '20170825', '20170829', '20170830', '20170831', '20170901', '20170904', '20170905', '20170906', '20170907', '20170908', '20170911', '20170912', '20170913', '20170914', '20170915', '20170918', '20170919', '20170920', '20170921', '20170922', '20170925', '20170926', '20170927', '20170928', '20170929']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_paths_for_symbol(index, FeaturesDir, LabelOne):\n",
    "    # Find the common elements between the two directories\n",
    "    symbols = sorted(list(set(os.listdir(FeaturesDir)).intersection(set(os.listdir(LabelOne)))))\n",
    "    \n",
    "    # Check if the index is valid\n",
    "    if index < 0 or index >= len(symbols):\n",
    "        return \"Index is out of range.\"\n",
    "\n",
    "    # Get the symbol at the provided index\n",
    "    selected_symbol = symbols[index]\n",
    "\n",
    "    # Construct the paths for the selected symbol in both directories\n",
    "    features_path = os.path.join(FeaturesDir, selected_symbol, 'MODEL_BASED')\n",
    "    labels_path = os.path.join(LabelOne, selected_symbol)\n",
    "\n",
    "    return {\n",
    "        'Features Path': features_path,\n",
    "        'Labels Path': labels_path\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "FeaturesDir = '/media/ak/DataOnly1/SymbolFeatureDirectories/'\n",
    "LabelOne = '/media/ak/DataOnly1/ExperimentCommonLocs/LabelsAlternateOne/'\n",
    "index = 1  # Adjust this index based on your choice from the list of common symbols\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d6b89f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Call the function\n",
    "# result = get_paths_for_symbol(index, FeaturesDir, LabelOne)\n",
    "# print(result)\n",
    "# datesDirs = os.listdir(result['Features Path'])\n",
    "# dateFiles = os.listdir(result['Labels Path'])\n",
    "# label_dates = [f.split(\".csv\")[0] for f in dateFiles]\n",
    "# datesDirsIdx = 1\n",
    "# datesDirPath = os.path.join(result['Features Path'], datesDirs[datesDirsIdx])\n",
    "# features_files = os.listdir(datesDirPath)\n",
    "# features_dict = {f.split(\"_\")[5]: f for f in features_files if \"_\" in f and len(f.split(\"_\")) > 5}\n",
    "\n",
    "# features_dates= list(features_dict.keys())\n",
    "\n",
    "# common_dates = sorted(list(set(features_dates).intersection(set(label_dates))))\n",
    "# print(common_dates)\n",
    "# DatesIdx = 1\n",
    "# featuresCommonDatePath = os.path.join( datesDirPath, features_dict[common_dates[DatesIdx]])\n",
    "# labelsCommonDatePath = os.path.join(result['Labels Path'], common_dates[DatesIdx]+str('.csv'))\n",
    "# data =pd.read_csv(labelsCommonDatePath) # data\n",
    "# # # Example Usage\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Initialize the feature creation class\n",
    "# market_features = CreateMarketFeatures(df)\n",
    "\n",
    "# # Calculate features\n",
    "# df_with_features = market_features.ma_spread()\n",
    "# df_with_features = market_features.obv_calc()\n",
    "# df_with_features = market_features.chaikin_mf()\n",
    "\n",
    "# print(df_with_features.columns.values)\n",
    "# # Automatically detect label column\n",
    "# label_columns = [col for col in df.columns if 'label' in col]\n",
    "# if not label_columns:\n",
    "#     raise ValueError(\"No label column found.\")\n",
    "# label_column = label_columns[0]  # Use the first label column found\n",
    "\n",
    "# # Features and target variable\n",
    "# X = df[['ma_spread', 'OBV', 'CMF']]\n",
    "# y = df[label_column]\n",
    "\n",
    "# # Checking for NaNs in features and target\n",
    "# print(\"NaNs in X:\", X.isna().sum())\n",
    "# print(\"NaNs in y:\", y.isna().sum())\n",
    "\n",
    "# # Filling NaNs with the median or mean or dropping rows with NaNs\n",
    "# X.fillna(X.median(), inplace=True)\n",
    "# y.dropna(inplace=True)  # Dropping NaNs from y might require re-splitting the data\n",
    "\n",
    "# # If you drop NaNs from y, you need to make sure that X only includes rows that have corresponding y values\n",
    "# X = X.loc[y.index]\n",
    "\n",
    "# # Re-splitting the data if you have dropped any rows\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=16)\n",
    "\n",
    "# # Instantiate the model\n",
    "# logreg = LogisticRegression(random_state=16)\n",
    "\n",
    "# # Fit the model\n",
    "# logreg.fit(X_train, y_train)\n",
    "\n",
    "# # Predicting the test set results\n",
    "# y_pred = logreg.predict(X_test)\n",
    "\n",
    "# # Evaluating the model\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# precision = precision_score(y_test, y_pred, average='binary')\n",
    "# recall = recall_score(y_test, y_pred, average='binary')\n",
    "# f1 = f1_score(y_test, y_pred, average='binary')\n",
    "\n",
    "# print(f\"Accuracy: {accuracy}\")\n",
    "# print(f\"Precision: {precision}\")\n",
    "# print(f\"Recall: {recall}\")\n",
    "# print(f\"F1 Score: {f1}\")\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "8e8d29cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Multi-Day Fitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "73d39323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7406647517439475\n",
      "Precision: 0.7406647517439475\n",
      "Recall: 1.0\n",
      "F1 Score: 0.851013672795851\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       632\n",
      "         1.0       0.74      1.00      0.85      1805\n",
      "\n",
      "    accuracy                           0.74      2437\n",
      "   macro avg       0.37      0.50      0.43      2437\n",
      "weighted avg       0.55      0.74      0.63      2437\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ak/anaconda3/envs/tickData/lib/python3.7/site-packages/pandas/core/series.py:4536: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  downcast=downcast,\n",
      "/home/ak/anaconda3/envs/tickData/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ak/anaconda3/envs/tickData/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ak/anaconda3/envs/tickData/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Assuming `common_dates` and other setup from your code are correctly defined\n",
    "train_dates = common_dates[:6]  # First 6 dates for training\n",
    "test_dates = common_dates[6:]  # Remaining dates for testing\n",
    "\n",
    "def load_data_for_dates(dates, features_path, labels_path, features_dict):\n",
    "    data_frames = []\n",
    "    for date in dates:\n",
    "        features_date_path = os.path.join(features_path, features_dict[date])\n",
    "        labels_date_path = os.path.join(labels_path, date + '.csv')\n",
    "        df = pd.read_csv(labels_date_path)\n",
    "        # Assuming feature calculation happens here or is pre-calculated\n",
    "        market_features = CreateMarketFeatures(df)\n",
    "        df = market_features.ma_spread()\n",
    "        df = market_features.obv_calc()\n",
    "        df = market_features.chaikin_mf()\n",
    "        data_frames.append(df)\n",
    "    return pd.concat(data_frames)\n",
    "\n",
    "# Load training and testing data\n",
    "train_data = load_data_for_dates(train_dates, datesDirPath, result['Labels Path'], features_dict)\n",
    "test_data = load_data_for_dates(test_dates, datesDirPath, result['Labels Path'], features_dict)\n",
    "\n",
    "# Ensure label columns are correctly identified\n",
    "label_columns = [col for col in train_data.columns if 'label' in col]\n",
    "if not label_columns:\n",
    "    raise ValueError(\"No label column found.\")\n",
    "label_column = label_columns[0]  # Use the first label column found\n",
    "\n",
    "# Features and target variable setup\n",
    "X_train = train_data[['ma_spread', 'OBV', 'CMF']]\n",
    "y_train = train_data[label_column]\n",
    "X_test = test_data[['ma_spread', 'OBV', 'CMF']]\n",
    "y_test = test_data[label_column]\n",
    "\n",
    "# Handling missing data\n",
    "X_train.fillna(X_train.median(), inplace=True)\n",
    "y_train.fillna(method='ffill', inplace=True)  # Forward fill for labels, or choose a better imputation method\n",
    "X_test.fillna(X_test.median(), inplace=True)\n",
    "y_test.fillna(method='ffill', inplace=True)  # Forward fill for labels, or choose a better imputation method\n",
    "\n",
    "### Step 2: Train and Evaluate the Model\n",
    "logreg = LogisticRegression(random_state=16)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Model evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='binary')\n",
    "recall = recall_score(y_test, y_pred, average='binary')\n",
    "f1 = f1_score(y_test, y_pred, average='binary')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "501730ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7406647517439475\n",
      "Precision: 0.7406647517439475\n",
      "Recall: 1.0\n",
      "F1 Score: 0.851013672795851\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       632\n",
      "         1.0       0.74      1.00      0.85      1805\n",
      "\n",
      "    accuracy                           0.74      2437\n",
      "   macro avg       0.37      0.50      0.43      2437\n",
      "weighted avg       0.55      0.74      0.63      2437\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ak/anaconda3/envs/tickData/lib/python3.7/site-packages/pandas/core/series.py:4536: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  downcast=downcast,\n",
      "/home/ak/anaconda3/envs/tickData/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ak/anaconda3/envs/tickData/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ak/anaconda3/envs/tickData/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "number_of_training_days = 6\n",
    "# Assuming `common_dates` and other setup from your code are correctly defined\n",
    "train_dates = common_dates[:number_of_training_days]  # First 6 dates for training\n",
    "test_dates = common_dates[number_of_training_days:]  # Remaining dates for testing\n",
    "\n",
    "def load_data_for_dates(dates, features_path, labels_path, features_dict):\n",
    "    data_frames = []\n",
    "    for date in dates:\n",
    "        features_date_path = os.path.join(features_path, features_dict[date])\n",
    "        labels_date_path = os.path.join(labels_path, date + '.csv')\n",
    "        df = pd.read_csv(labels_date_path)\n",
    "        # Assuming feature calculation happens here or is pre-calculated\n",
    "        market_features = CreateMarketFeatures(df)\n",
    "        df = market_features.ma_spread()\n",
    "        df = market_features.obv_calc()\n",
    "        df = market_features.chaikin_mf()\n",
    "        data_frames.append(df)\n",
    "    return pd.concat(data_frames)\n",
    "\n",
    "# Load training and testing data\n",
    "train_data = load_data_for_dates(train_dates, datesDirPath, result['Labels Path'], features_dict)\n",
    "test_data = load_data_for_dates(test_dates, datesDirPath, result['Labels Path'], features_dict)\n",
    "\n",
    "# Ensure label columns are correctly identified\n",
    "label_columns = [col for col in train_data.columns if 'label' in col]\n",
    "if not label_columns:\n",
    "    raise ValueError(\"No label column found.\")\n",
    "label_column = label_columns[0]  # Use the first label column found\n",
    "\n",
    "# Features and target variable setup\n",
    "X_train = train_data[['ma_spread', 'OBV', 'CMF']]\n",
    "y_train = train_data[label_column]\n",
    "X_test = test_data[['ma_spread', 'OBV', 'CMF']]\n",
    "y_test = test_data[label_column]\n",
    "\n",
    "# Handling missing data\n",
    "X_train.fillna(X_train.median(), inplace=True)\n",
    "y_train.fillna(method='ffill', inplace=True)  # Forward fill for labels, or choose a better imputation method\n",
    "X_test.fillna(X_test.median(), inplace=True)\n",
    "y_test.fillna(method='ffill', inplace=True)  # Forward fill for labels, or choose a better imputation method\n",
    "\n",
    "### Step 2: Train and Evaluate the Model\n",
    "logreg = LogisticRegression(random_state=16)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Model evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='binary')\n",
    "recall = recall_score(y_test, y_pred, average='binary')\n",
    "f1 = f1_score(y_test, y_pred, average='binary')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "aea2a848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Micro: 0.74\n",
      "F1 Macro: 0.43\n",
      "F1 Weighted: 0.63\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       632\n",
      "         1.0       0.74      1.00      0.85      1805\n",
      "\n",
      "    accuracy                           0.74      2437\n",
      "   macro avg       0.37      0.50      0.43      2437\n",
      "weighted avg       0.55      0.74      0.63      2437\n",
      "\n",
      "Confusion Matrix:\n",
      " [[   0  632]\n",
      " [   0 1805]]\n",
      "Matthews Correlation Coefficient: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ak/anaconda3/envs/tickData/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ak/anaconda3/envs/tickData/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ak/anaconda3/envs/tickData/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ak/anaconda3/envs/tickData/lib/python3.7/site-packages/sklearn/metrics/_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_prob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6699/3208832813.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Note: ROC-AUC can be computed for binary classification, for multi-class you would adjust the method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Check if it's a binary classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mroc_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# adjust indexing based on your predict_proba output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ROC-AUC Score: {:.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_prob' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, classification_report, roc_auc_score, confusion_matrix, matthews_corrcoef, log_loss\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Assuming y_test and y_pred are available from your logistic regression model predictions\n",
    "# Calculate F1 scores\n",
    "f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(\"F1 Micro: {:.2f}\".format(f1_micro))\n",
    "print(\"F1 Macro: {:.2f}\".format(f1_macro))\n",
    "print(\"F1 Weighted: {:.2f}\".format(f1_weighted))\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Matthews Correlation Coefficient\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "print(\"Matthews Correlation Coefficient: {:.2f}\".format(mcc))\n",
    "\n",
    "# If the logistic regression model outputs probabilities (you need to adjust model prediction if not)\n",
    "# Assuming you have predicted probabilities y_prob for ROC and Log-Loss; if not use logreg.predict_proba(X_test)\n",
    "# y_prob = logreg.predict_proba(X_test)[:, 1]  # Adjust according to your specific model\n",
    "\n",
    "# ROC-AUC score\n",
    "# Note: ROC-AUC can be computed for binary classification, for multi-class you would adjust the method\n",
    "if len(np.unique(y_test)) == 2:  # Check if it's a binary classification\n",
    "    roc_auc = roc_auc_score(y_test, y_prob[:, 1])  # adjust indexing based on your predict_proba output\n",
    "    print(\"ROC-AUC Score: {:.2f}\".format(roc_auc))\n",
    "else:\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test_binarized = lb.transform(y_test)\n",
    "    y_prob = logreg.predict_proba(X_test)\n",
    "    roc_auc = roc_auc_score(y_test_binarized, y_prob, multi_class='ovr')  # ovr: One-vs-Rest\n",
    "    print(\"ROC-AUC Score for Multi-class: {:.2f}\".format(roc_auc))\n",
    "\n",
    "# Log-Loss\n",
    "logloss = log_loss(y_test, y_prob)\n",
    "print(\"Log Loss: {:.2f}\".format(logloss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "0ef51a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Micro: 0.74\n",
      "F1 Macro: 0.43\n",
      "F1 Weighted: 0.63\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       632\n",
      "         1.0       0.74      1.00      0.85      1805\n",
      "\n",
      "    accuracy                           0.74      2437\n",
      "   macro avg       0.37      0.50      0.43      2437\n",
      "weighted avg       0.55      0.74      0.63      2437\n",
      "\n",
      "Confusion Matrix:\n",
      " [[   0  632]\n",
      " [   0 1805]]\n",
      "Matthews Correlation Coefficient: 0.00\n",
      "ROC-AUC Score: 0.51\n",
      "Log Loss: 0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ak/anaconda3/envs/tickData/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ak/anaconda3/envs/tickData/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ak/anaconda3/envs/tickData/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ak/anaconda3/envs/tickData/lib/python3.7/site-packages/sklearn/metrics/_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, classification_report, roc_auc_score, confusion_matrix, matthews_corrcoef, log_loss\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Assuming y_test and y_pred are available from your logistic regression model predictions\n",
    "\n",
    "# Instantiate the model (if not already instantiated)\n",
    "logreg = LogisticRegression(random_state=16)\n",
    "\n",
    "# Train the model\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_pred = logreg.predict(X_test)\n",
    "y_prob = logreg.predict_proba(X_test)  # This will be used for ROC-AUC and log loss\n",
    "\n",
    "# Calculate F1 scores\n",
    "f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(\"F1 Micro: {:.2f}\".format(f1_micro))\n",
    "print(\"F1 Macro: {:.2f}\".format(f1_macro))\n",
    "print(\"F1 Weighted: {:.2f}\".format(f1_weighted))\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Matthews Correlation Coefficient\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "print(\"Matthews Correlation Coefficient: {:.2f}\".format(mcc))\n",
    "\n",
    "# ROC-AUC score\n",
    "# Note: ROC-AUC can be computed for binary classification, for multi-class you would adjust the method\n",
    "if len(np.unique(y_test)) == 2:  # Check if it's a binary classification\n",
    "    roc_auc = roc_auc_score(y_test, y_prob[:, 1])  # Adjust indexing based on your predict_proba output\n",
    "    print(\"ROC-AUC Score: {:.2f}\".format(roc_auc))\n",
    "else:\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test_binarized = lb.transform(y_test)\n",
    "    roc_auc = roc_auc_score(y_test_binarized, y_prob, multi_class='ovr')  # ovr: One-vs-Rest\n",
    "    print(\"ROC-AUC Score for Multi-class: {:.2f}\".format(roc_auc))\n",
    "\n",
    "# Log-Loss\n",
    "logloss = log_loss(y_test, y_prob)\n",
    "print(\"Log Loss: {:.2f}\".format(logloss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db04741e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
