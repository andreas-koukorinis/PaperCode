{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "sc = StandardScaler()\n",
    "import os\n",
    "import pickle\n",
    "import fnmatch\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##useful functions\n",
    "\n",
    "def fwd_dates(_dates_list, _key_date):\n",
    "    # returns a list of dates that are forward from the key_date\n",
    "    fwd_dates_list = [i for i in _dates_list if i > _key_date]\n",
    "    return fwd_dates_list\n",
    "\n",
    "def common_member(a, b): \n",
    "      \n",
    "    a_set = set(a) \n",
    "    b_set = set(b) \n",
    "      \n",
    "    # check length  \n",
    "    if len(a_set.intersection(b_set)) > 0: \n",
    "        return(a_set.intersection(b_set))   \n",
    "    else: \n",
    "        return(\"no common elements\") \n",
    "\n",
    "\n",
    "def remove_nans(features_tuple, labels, idx=1):\n",
    "    # not the cleanest but useful\n",
    "    # function to clean up nans as I seem to use it a lot, so better to have one function\n",
    "    # combines the features and labels and removes rows with nans across so we dont lose the ordering\n",
    "    # returns features and labels\n",
    "    features_df = pd.concat([features_tuple[0], features_tuple[1], features_tuple[2], \\\n",
    "                             features_tuple[3]], axis=1, sort=False)\n",
    "    labels_only = labels.drop(columns=['ReturnTradedPrice', 'Duration', 'states', 'TradedTime',\n",
    "                                       'TradedPrice'], axis=1)\n",
    "    df_concat = pd.concat([features_df, labels_only.iloc[:, 0:idx]], axis=1, sort='False')\n",
    "    # only using 1st set of labels- but we can re-write this a bit\n",
    "    df_x_nan = df_concat.dropna()  # dropping all nans\n",
    "    label_column_loc_ = df_x_nan.shape[1] - 1  # location of labels column in the clean df\n",
    "    labels_ = df_x_nan.iloc[:, label_column_loc_:label_column_loc_ + 1]  # keep pure labels\n",
    "    features_ = df_x_nan.drop(df_x_nan.columns[label_column_loc_], axis=1)  # keeping the features only\n",
    "    return features_, labels_\n",
    "\n",
    "\n",
    "def prec_recall_report(y_true, y_predict):\n",
    "    # function to ge the sci-kit learn classification metrics into a pretty DF for csv!\n",
    "    report = pd.DataFrame(list(precision_recall_fscore_support(y_true, y_predict)),\n",
    "                          index=['Precision', 'Recall', 'F1-score', 'Support']).T\n",
    "    # Now add the 'Avg/Total' row\n",
    "    report.loc['Avg/Total', :] = precision_recall_fscore_support(y_true, y_predict, average='weighted')\n",
    "    report.loc['Avg/Total', 'Support'] = report['Support'].sum()\n",
    "    return report\n",
    "\n",
    "\n",
    "class DataLoader(object):\n",
    "    def __init__(self, path_main, ticker):\n",
    "        self.main_path = path_main\n",
    "        self.ticker = ticker\n",
    "\n",
    "        self.features_labels_path = os.path.join(self.main_path, 'features_models')\n",
    "        self.features_path = os.path.join(self.features_labels_path, 'features')\n",
    "        # collection of per symbol non directional labels\n",
    "        self.labels_path = os.path.join(self.features_labels_path, 'labels', self.ticker, 'NON_DIRECTIONAL')\n",
    "        self.symbol_features_path = os.path.join(self.features_labels_path, 'features', self.ticker, 'MODEL_BASED')\n",
    "        # list of all the model -oos hmm feature dates - each folder is a collection of oos feature dates\n",
    "        self.hmm_dates_list = os.listdir(self.symbol_features_path)  # each folder are the OOS features from each HMM\n",
    "        self.compute_date = os.listdir(os.path.join( \\\n",
    "            self.symbol_features_path, \\\n",
    "            os.listdir(self.symbol_features_path)[1]))[1].split(\"_\")[7]\n",
    "\n",
    "    def ticker_features(self, model_date, date):\n",
    "        # need to make this a lot more flexible with number of states\n",
    "        if model_date < date:\n",
    "            file_name = \"_\".join(\n",
    "                (self.ticker, '3', 'states', 'features', 'date:', date, 'now:', self.compute_date, '.pickle'))\n",
    "            file_loc = os.path.join(self.symbol_features_path, str(model_date), file_name)\n",
    "            with open(file_loc, 'rb') as handle:\n",
    "                ticker_features = pickle.load(handle)\n",
    "        else:\n",
    "            print('Loading Feature Date which is in-sample. Change your Model Date')\n",
    "        return ticker_features\n",
    "\n",
    "    def ticker_labels_csv(self, date):\n",
    "        file_loc = os.path.join(self.labels_path, str(date) + '.csv')\n",
    "        ticker_labels = pd.read_csv(file_loc, index_col=0)\n",
    "        return ticker_labels\n",
    "\n",
    "    @staticmethod\n",
    "    def open_pickle_file(path, pickle_file):\n",
    "        file_loc = os.path.join(path, pickle_file)\n",
    "        pickle_to_file = pickle.load(open(file_loc, \"rb\"))\n",
    "        return pickle_to_file\n",
    "\n",
    "    @staticmethod\n",
    "    def get_date_from_file(file_, numb_):\n",
    "        return os.path.splitext(file_[numb_])[0]\n",
    "\n",
    "class MarketFeatures(object):\n",
    "    # a class to be expanded that uses features for base case -market based only-indicators/features\n",
    "    \"\"\"\"Requires:\n",
    "    a dataframe that has TradedPrice And Volume columns\n",
    "    symbol - A stock symbol on which to form a strategy on.\n",
    "    short_window - Lookback period for short moving average.\n",
    "    long_window - Lookback period for long moving average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        #         self.ticker = ticker\n",
    "        self.df = df\n",
    "\n",
    "    def load_data(self):\n",
    "        pass\n",
    "\n",
    "    def ma_spread(self, short_window=5, long_window=20):\n",
    "        # function that produces the MA spread, which can be used on its own or as an input for MACD\n",
    "        short_rolling_px = self.df['TradedPrice'].rolling(window=short_window).mean()\n",
    "        long_rolling_px = self.df['TradedPrice'].rolling(window=long_window).mean()\n",
    "        px_name = \"_\".join(('px_indx', str(short_window), str(long_window)))\n",
    "        self.df[px_name] = long_rolling_px - short_rolling_px\n",
    "        return self.df\n",
    "\n",
    "    def ma_spread_duration(self, short_window=5, long_window=20):\n",
    "        # function that produces the MA spread, which can be used on its own or as an input for MACD\n",
    "        short_rolling_px = self.df['Duration'].rolling(window=short_window).mean()\n",
    "        long_rolling_px = self.df['Duration'].rolling(window=long_window).mean()\n",
    "        dur_name = \"_\".join(('dur_indx', str(short_window), str(long_window)))\n",
    "        self.df[dur_name] = long_rolling_px - short_rolling_px\n",
    "        return self.df\n",
    "\n",
    "    def obv_calc(self):\n",
    "        # on balance volume indicator\n",
    "        self.df['SignedVolume'] = self.df['Volume'] * np.sign(self.df['TradedPrice'].diff()).cumsum()\n",
    "        self.df['SignedVolume'].iat[1] = 0\n",
    "        self.df['OBV'] = self.df['SignedVolume']  # .cumsum()\n",
    "        self.df = self.df.drop(columns=['SignedVolume'])\n",
    "        return self.df\n",
    "\n",
    "    def chaikin_mf(self, period=5):\n",
    "        # Chaikin money flow indicator\n",
    "        self.df[\"MF Multiplier\"] = (self.df['TradedPrice'] - (self.df['TradedPrice'].expanding(period).min()) \\\n",
    "                                    - (self.df['TradedPrice'].expanding(period).max() \\\n",
    "                                       - self.df['TradedPrice'])) / (\n",
    "                                           self.df['TradedPrice'].expanding(period).max() - self.df[ \\\n",
    "                                       'TradedPrice'].expanding(period).min())\n",
    "        self.df[\"MF Volume\"] = self.df['MF Multiplier'] * self.df['Volume']\n",
    "        self.df['CMF_' + str(period)] = self.df['MF Volume'].sum() / self.df[\"Volume\"].rolling(period).sum()\n",
    "        self.df = self.df.drop(columns=['MF Multiplier', 'MF Volume'])\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_oos_loc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-73d30802b87c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpassport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'/media/ak/My Passport'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mexperimental_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpassport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Experiment Data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mfeatures_loc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_oos_loc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mlabels_loc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_oos_loc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodels_loc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_oos_loc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_oos_loc' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# main locations\n",
    "\n",
    "data_dir = os.getenv('FINANCE_DATA')  # main directory referenced in all the code\n",
    "# data_oos_loc= ('/media/ak/My Passport/Experiment Data')\n",
    "# passport = ('/media/ak/My Passport')\n",
    "# experimental_data = os.path.join(passport, 'Experiment Data')\n",
    "features_loc = os.path.join(data_oos_loc,'features') #\n",
    "labels_loc = os.path.join(data_oos_loc,'labels')\n",
    "models_loc = os.path.join(data_oos_loc,'models')\n",
    "features= os.listdir(features_loc)\n",
    "# os.listdir(os.path.join(data_dir,'features_models','models'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a symbol that works\n",
    "symbol ='MKS.L'\n",
    "\n",
    "syml_features_loc = os.path.join(features_loc,symbol,'MODEL_BASED') #create the symbol feature locations\n",
    "syml_models_loc = os.path.join(models_loc,symbol,'SINGLE_KERNEL') #create the symbol model locations\n",
    "syml_labels_loc = os.path.join(labels_loc,symbol,'NON_DIRECTIONAL')# create the symbol labels location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists of dates\n",
    "model_dates = os.listdir(syml_models_loc)\n",
    "labels_dates = [os.listdir(syml_labels_loc)[idx].split(\".\")[0] for idx, _ in enumerate(os.listdir(syml_labels_loc))]\n",
    "features_dates_dir = os.listdir(syml_features_loc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_date = list(common_member(labels_dates, oos_features_dates))\n",
    "common_date_features_loc = os.path.join(syml_features_loc, common_date[2])\n",
    "common_date_features = os.listdir(common_date_features_loc)\n",
    "common_dates = [os.listdir(common_date_features)[idx] for ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MKS.L_3_states_features_date:_20180413_now:_20181225_.pickle',\n",
       " 'MKS.L_3_states_features_date:_20180416_now:_20181225_.pickle',\n",
       " 'MKS.L_3_states_features_date:_20180417_now:_20181225_.pickle',\n",
       " 'MKS.L_3_states_features_date:_20180418_now:_20181225_.pickle',\n",
       " 'MKS.L_3_states_features_date:_20180419_now:_20181225_.pickle',\n",
       " 'MKS.L_3_states_features_date:_20180420_now:_20181225_.pickle']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_date_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 2] No such file or directory: '/media/ak/My Passport/Experiment Data/features/MKS.L/MODEL_BASED/MKS.L_3_states_features_date:_20180416_now:_20181225_.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-44fde4983391>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#contains the list of detailed features for the specific model date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfeatures_dates_detail\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyml_features_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommon_date_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdate_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0moos_features_dates\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_dates_detail\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_dates_detail\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory: '/media/ak/My Passport/Experiment Data/features/MKS.L/MODEL_BASED/MKS.L_3_states_features_date:_20180416_now:_20181225_.pickle'"
     ]
    }
   ],
   "source": [
    "#model date- this date corresponds to the model which was used for out of sample\n",
    "date_idx = 1 \n",
    "#contains the list of detailed features for the specific model date\n",
    "features_dates_detail = os.path.join(syml_features_loc, common_date_features[date_idx]) \n",
    "oos_features_dates =[os.listdir(features_dates_detail)[idx].split(\"_\")[5] for idx,_ in enumerate(os.listdir(features_dates_detail))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20170808',\n",
       " '20180202',\n",
       " '20170116',\n",
       " '20170117',\n",
       " '20170118',\n",
       " '20170119',\n",
       " '20170120',\n",
       " '20170123',\n",
       " '20170124',\n",
       " '20170125',\n",
       " '20170126',\n",
       " '20170127',\n",
       " '20170130',\n",
       " '20170131',\n",
       " '20170703',\n",
       " '20170704',\n",
       " '20170705',\n",
       " '20170706',\n",
       " '20170707',\n",
       " '20170710',\n",
       " '20170711',\n",
       " '20170712',\n",
       " '20170713',\n",
       " '20170714',\n",
       " '20170717',\n",
       " '20170718',\n",
       " '20170719',\n",
       " '20170720',\n",
       " '20170721',\n",
       " '20170724',\n",
       " '20170725',\n",
       " '20170726',\n",
       " '20170727',\n",
       " '20170728',\n",
       " '20170731',\n",
       " '20170801',\n",
       " '20170802',\n",
       " '20170803',\n",
       " '20170804',\n",
       " '20170807',\n",
       " '20170809',\n",
       " '20170810',\n",
       " '20170811',\n",
       " '20170814',\n",
       " '20170815',\n",
       " '20170816',\n",
       " '20170817',\n",
       " '20170818',\n",
       " '20170821',\n",
       " '20170822',\n",
       " '20170823',\n",
       " '20170824',\n",
       " '20170825',\n",
       " '20170829',\n",
       " '20170830',\n",
       " '20170831',\n",
       " '20170901',\n",
       " '20170904',\n",
       " '20170905',\n",
       " '20170906',\n",
       " '20170907',\n",
       " '20170908',\n",
       " '20170911',\n",
       " '20170912',\n",
       " '20170913',\n",
       " '20170914',\n",
       " '20170915',\n",
       " '20170918',\n",
       " '20170919',\n",
       " '20170920',\n",
       " '20170921',\n",
       " '20170922',\n",
       " '20170925',\n",
       " '20170926',\n",
       " '20170927',\n",
       " '20170928',\n",
       " '20170929',\n",
       " '20180201',\n",
       " '20180205',\n",
       " '20180206',\n",
       " '20180207',\n",
       " '20180208',\n",
       " '20180209',\n",
       " '20180212',\n",
       " '20180213',\n",
       " '20180214',\n",
       " '20180215',\n",
       " '20180216',\n",
       " '20180219',\n",
       " '20180220',\n",
       " '20180221',\n",
       " '20180222',\n",
       " '20180223',\n",
       " '20180226',\n",
       " '20180227',\n",
       " '20180228',\n",
       " '20180403',\n",
       " '20180404',\n",
       " '20180405',\n",
       " '20180406',\n",
       " '20180409',\n",
       " '20180410',\n",
       " '20180411',\n",
       " '20180412',\n",
       " '20180413',\n",
       " '20180416',\n",
       " '20180417',\n",
       " '20180418',\n",
       " '20180419',\n",
       " '20180420']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_pickle_to_svc(model_pickle):\n",
    "\n",
    "    pickle_to_file = pickle.load(open(model_pickle, \"rb\"))\n",
    "    best_estimator = pickle_to_file['SVC'].best_estimator_\n",
    "\n",
    "    return best_estimator\n",
    "\n",
    "\n",
    "# test case ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    \n",
    "    # test symbol path, which essentially produces the path where all the fitted models are.\n",
    "    # '/mnt/usb-Seagate_Expansion_Desk_NA8XEHR6-0:0-part2/Data/features_models/models/SPT.L/SINGLE_KERNEL'\n",
    "\n",
    "    symbol_labels_path = syml_labels_loc\n",
    "    \n",
    "    # specific symbol features list of directories. so this has all the model-based directories of features\n",
    "    # each date on this list corresponds to an hmm model, and each date-directory contains all the features \n",
    "    # constructed out of sample\n",
    "    \n",
    "    symbol_features_dates_path = syml_features_loc\n",
    "    \n",
    "    # we construct a list of all the hmm-model-date directories, each containing OOS features\n",
    "    list_features_dates_dirs = os.listdir(symbol_features_dates_path)\n",
    "    \n",
    "    # go into each hmm-model-generated directory and get us the location of that directory\n",
    "    def oos_date_specific_features_dir(date_dir_no): \n",
    "        return os.path.join(symbol_features_dates_path,list_features_dates_dirs[date_dir_no]) #basically \n",
    "    \n",
    "    # this is an example\n",
    "    oos_feature_file_locations_dict ={}\n",
    "    oos_labels_file_locations_dict ={}\n",
    "    for date_idx, date in enumerate(list_features_dates_dirs):\n",
    "\n",
    "        # create a directory of all the feature locations        \n",
    "        oos_feature_file_locations_dict[date] = [os.listdir(oos_date_specific_features_dir(date_idx))[loc_idx]\n",
    "                                              for loc_idx,_ in \n",
    "                                               enumerate(os.listdir(oos_date_specific_features_dir(date_idx)))]\n",
    "    \n",
    "        # create a directory of all the label locations\n",
    "        oos_labels_file_locations_dict[date]=[\".\".join((oos_feature_file_locations_dict[date][idx].split(\"_\")[5],\"csv\"))\n",
    "                                             for idx,_ in enumerate(oos_feature_file_locations_dict[date])]\n",
    "        \n",
    "       \n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the key of keys to load up the \n",
    " # go through all the keys and basically get the out of sample features and labels\n",
    "keys= oos_feature_file_locations_dict.keys()\n",
    "#so you need to iterate over idxs's and over keys'\n",
    "# for key_idx,_ in enumerate(keys):\n",
    "# print \n",
    "key_idx=1 #one loop\n",
    "\n",
    "# location= os.path.join(oos_date_specific_features_dir(date_idx),oos_feature_file_locations_dict[keys[key_idx]][1])\n",
    "# test_features=pickle.load(open(location, \"rb\")) #second loop\n",
    "# test_labels = pd.read_csv(oos_labels_file_locations_dict[keys[key_idx]][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacls = DataLoader(path_main=data_dir, ticker=symbol)  # test symbol -create class\n",
    "symbol_compute_date = datacls.compute_date\n",
    "from collections import defaultdict\n",
    "all_symbols_d = defaultdict(dict)\n",
    "symbol_model_dates = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print symbol\n",
    "test_path = symbol_fitted_models_path(symbol=symbol)\n",
    "\n",
    "# now lets take all the model directories and locations##\n",
    "model_directories = [symbol_fitted_model_date_loc(test_path, idx) for idx, date in enumerate(os.listdir(test_path))]\n",
    "models_locations = [os.path.join(model_directories[dir_idx], os.listdir(model_directories[dir_idx])[0]) for\n",
    "                    dir_idx, model_dir in enumerate(model_directories)]\n",
    "for model_idx, model_loc in enumerate(models_locations):\n",
    "    model_date = model_loc.split(\"_\")[6]\n",
    "    print ('model date is:',model_date)\n",
    "    model_pickle = model_loc\n",
    "\n",
    "    print model_pickle\n",
    "    pickle_to_file = pickle.load(open(model_pickle, \"rb\")) #load your model\n",
    "\n",
    "    best_estimator = pickle_to_file['SVC'].best_estimator_\n",
    "\n",
    "    print('Your symbol is:', symbol, 'and the model date is:' ,model_date)\n",
    "    fwd_dates_list = [i for i in keys if i > model_date]\n",
    "    # set up the dictionary for metrics #\n",
    "    M = len(fwd_dates_list)\n",
    "    T = 1\n",
    "    T_2 = 4\n",
    "    fitted_models_results = {\n",
    "            'accuracy': np.empty((M,T)),\n",
    "            'recall': np.empty((M,T)),\n",
    "            'F1-score': np.empty((M,T)),\n",
    "            'precision_recall_fscore_support': np.empty((M, T_2))\n",
    "        }\n",
    "    for key in fwd_dates_list:\n",
    "        listA=oos_feature_file_locations_dict[key] # list of out of sample features\n",
    "        listB=oos_labels_file_locations_dict[key] # list of out of sample labels\n",
    "        for a, b in zip(listA, listB):\n",
    "            if fwd_date > model_date:\n",
    "                fwd_date=b.split(\".\")[0]\n",
    "                features_date= a.split(\"_\")[5]\n",
    "                print('model date', model_date)\n",
    "                print('features_date', features_date)\n",
    "            else:\n",
    "                print('tiny problem, model date is not behind!')\n",
    "#             exists=os.path.isfile(os.path.join(symbol_features_dates_path,key,a))            \n",
    "#             if exists:\n",
    "#                 print('###-computing-###')\n",
    "#                 features_tuple=pickle.load(open(os.path.join(symbol_features_dates_path,key,a), \"rb\"))\n",
    "#                 market_data_oos= pd.read_csv(os.path.join(symbol_labels_path, b),index_col=0)\n",
    "# #                 features_df = pd.concat([features_tuple[0], features_tuple[1],\n",
    "# #                                      features_tuple[2], features_tuple[3]], axis=1, sort=False)\n",
    "# #                 df_w_market_features = MarketFeatures(df=MarketFeatures(\\\n",
    "# #                     df=MarketFeatures(\n",
    "#                         df=MarketFeatures(df=market_data_oos).obv_calc()).chaikin_mf()).ma_spread()).ma_spread_duration()\n",
    "\n",
    "#                 df_concat = pd.concat([features_df, df_w_market_features], axis=1, sort='False').dropna()\n",
    "\n",
    "#                 label_name = str(df_concat.columns[df_concat.columns.str.contains(pat='label')].values[0])\n",
    "\n",
    "#                 df_final = df_concat.drop(columns=['TradedPrice', 'Duration', 'TradedTime', 'ReturnTradedPrice', \\\n",
    "#                                                    'Volume', label_name])\n",
    "#                 if len(df_final)> 5:\n",
    "#                     X = MinMaxScaler().fit_transform(df_final)\n",
    "\n",
    "#                     y_labels = df_concat[df_concat.columns[df_concat.columns.str.contains(pat='label')]].iloc[:, 0]\n",
    "#                     y_predict = best_estimator.predict(X)\n",
    "#                     print accuracy_score(y_labels, y_predict)\n",
    "#                 else:\n",
    "#                     print('skipping...')\n",
    "\n",
    "#     #                 results_loc = str(os.path.join(metrics_loc, \"_\".join((symbol,model_date,\"results_metrics.pickle\"))))\n",
    "#     #                 fitted_models_results['accuracy'][fwd_date, :] = accuracy_score(y_labels, y_predict)\n",
    "#     #                 fitted_models_results['recall'][fwd_date, :] = recall_score(y_true=y_labels, y_pred=y_predict)\n",
    "#     #                 fitted_models_results['F1-score'][fwd_date, :] =f1_score(y_true= y_labels, y_pred=y_predict)\n",
    "#     #                 fitted_models_results['precision_recall_fscore_support'][fwd_idx, :] = precision_recall_fscore_support(y_true=  y_labels, y_pred=y_predict, average='micro')\n",
    "\n",
    "#     #                 with open(results_loc, 'wb') as f:\n",
    "#     #                     pickle.dump(fitted_models_results, f)\n",
    "#             else:\n",
    "#                 print ('problem')\n",
    "\n",
    "#             #             features_tuple = pickle.load(open(os.path.join(symbol_features_dates_path,a), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(symbol_features_dates_path,key,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwd_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import os\n",
    "    exists = os.path.isfile('/path/to/file')\n",
    "    if exists:\n",
    "        # Store configuration file values\n",
    "    else:\n",
    "        # Keep presets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwd_dates_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwd_dates_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
