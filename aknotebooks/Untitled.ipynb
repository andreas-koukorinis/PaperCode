{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsmm_core.data_utils import DataLoader, TradingHours\n",
    "from hsmm_core.feature_spaces import hmm_features\n",
    "from hsmm_core.hsmm_runner import HmmCalibration\n",
    "import time\n",
    "from hsmm_core.consts import InitialisationMethod\n",
    "from hsmm_core.data_utils import TradingHours, DataLoader\n",
    "from hsmm_core.labelling import DataLabellingSimple\n",
    "from hsmm_core.consts import ThresholdMethod, LabellingChoice\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier,  GradientBoostingClassifier\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "sc = StandardScaler()\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "##useful functions\n",
    "def fwd_dates(_dates_list, _key_date):\n",
    "    # returns a list of dates that are forward from the key_date\n",
    "    fwd_dates_list = [i for i in _dates_list if i > _key_date]\n",
    "    return fwd_dates_list\n",
    "def remove_nans(features_tuple, labels, idx=1):\n",
    "    # not the cleanest but useful\n",
    "    # function to clean up nans as I seem to use it a lot, so better to have one function\n",
    "    # combines the features and labels and removes rows with nans across so we dont lose the ordering\n",
    "    # returns features and labels\n",
    "    features_df = pd.concat([features_tuple[0], features_tuple[1], features_tuple[2], \\\n",
    "                             features_tuple[3]], axis=1, sort=False)\n",
    "    labels_only = labels.drop(columns=['ReturnTradedPrice', 'Duration', 'states', 'TradedTime',\n",
    "                                       'TradedPrice'], axis=1)\n",
    "    df_concat = pd.concat([features_df, labels_only.iloc[:, 0:idx]], axis=1, sort='False')\n",
    "    # only using 1st set of labels- but we can re-write this a bit\n",
    "    df_x_nan = df_concat.dropna()  # dropping all nans\n",
    "    label_column_loc_ = df_x_nan.shape[1] - 1  # location of labels column in the clean df\n",
    "    labels_ = df_x_nan.iloc[:, label_column_loc_:label_column_loc_ + 1]  # keep pure labels\n",
    "    features_ = df_x_nan.drop(df_x_nan.columns[label_column_loc_], axis=1)  # keeping the features only\n",
    "    return features_, labels_\n",
    "\n",
    "def prec_recall_report(y_true, y_predict):\n",
    "    # function to ge the sci-kit learn classification metrics into a pretty DF for csv!\n",
    "    report = pd.DataFrame(list(precision_recall_fscore_support(y_true, y_predict)),\n",
    "                          index=['Precision', 'Recall', 'F1-score', 'Support']).T\n",
    "    # Now add the 'Avg/Total' row\n",
    "    report.loc['Avg/Total', :] = precision_recall_fscore_support(y_true, y_predict, average='weighted')\n",
    "    report.loc['Avg/Total', 'Support'] = report['Support'].sum()\n",
    "    return report\n",
    "\n",
    "class MarketFeatures(object):\n",
    "    # a class to be expanded that uses features for base case -market based only-indicators/features\n",
    "    \"\"\"\"Requires:\n",
    "    a dataframe that has TradedPrice And Volume columns\n",
    "    symbol - A stock symbol on which to form a strategy on.\n",
    "    short_window - Lookback period for short moving average.\n",
    "    long_window - Lookback period for long moving average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        #         self.ticker = ticker\n",
    "        self.df = df\n",
    "\n",
    "    def load_data(self):\n",
    "        pass\n",
    "\n",
    "    def ma_spread(self, short_window=5, long_window=20):\n",
    "        # function that produces the MA spread, which can be used on its own or as an input for MACD\n",
    "        short_rolling_px = self.df['TradedPrice'].rolling(window=short_window).mean()\n",
    "        long_rolling_px = self.df['TradedPrice'].rolling(window=long_window).mean()\n",
    "        px_name = \"_\".join(('px_indx', str(short_window), str(long_window)))\n",
    "        self.df[px_name] = long_rolling_px - short_rolling_px\n",
    "        return self.df\n",
    "\n",
    "    def ma_spread_duration(self, short_window=5, long_window=20):\n",
    "        # function that produces the MA spread, which can be used on its own or as an input for MACD\n",
    "        short_rolling_px = self.df['Duration'].rolling(window=short_window).mean()\n",
    "        long_rolling_px = self.df['Duration'].rolling(window=long_window).mean()\n",
    "        dur_name = \"_\".join(('dur_indx', str(short_window), str(long_window)))\n",
    "        self.df[dur_name] = long_rolling_px - short_rolling_px\n",
    "        return self.df\n",
    "\n",
    "    def obv_calc(self):\n",
    "        # on balance volume indicator\n",
    "        self.df['SignedVolume'] = self.df['Volume'] * np.sign(self.df['TradedPrice'].diff()).cumsum()\n",
    "        self.df['SignedVolume'].iat[1] = 0\n",
    "        self.df['OBV'] = self.df['SignedVolume']  # .cumsum()\n",
    "        self.df = self.df.drop(columns=['SignedVolume'])\n",
    "        return self.df\n",
    "\n",
    "    def chaikin_mf(self, period=5):\n",
    "        # Chaikin money flow indicator\n",
    "        self.df[\"MF Multiplier\"] = (self.df['TradedPrice'] - (self.df['TradedPrice'].expanding(period).min()) \\\n",
    "                                    - (self.df['TradedPrice'].expanding(period).max() \\\n",
    "                                       - self.df['TradedPrice'])) / (\n",
    "                                           self.df['TradedPrice'].expanding(period).max() - self.df[ \\\n",
    "                                       'TradedPrice'].expanding(period).min())\n",
    "        self.df[\"MF Volume\"] = self.df['MF Multiplier'] * self.df['Volume']\n",
    "        self.df['CMF_' + str(period)] = self.df['MF Volume'].sum() / self.df[\"Volume\"].rolling(period).sum()\n",
    "        self.df = self.df.drop(columns=['MF Multiplier', 'MF Volume'])\n",
    "        return self.df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    ## locations\n",
    "\n",
    "    data_dir = os.getenv('FINANCE_DATA')  # main directory referenced in all the code\n",
    "    data_only_drive = '/mnt/usb-Seagate_Expansion_Desk_NA8XEHR6-0:0-part2'  # external date only drive\n",
    "\n",
    "    # this is the central location for all the features/models/predictions\n",
    "    features_models = os.path.join(data_dir,\n",
    "                                   'features_models')  # main path where all the sub-directories are (features, models, labels)\n",
    "\n",
    "    # this is the central location for all the labels\n",
    "    labels = os.path.join(features_models, 'labels')  # label subdirectory\n",
    "    # this is the central location for all the features #feature subdirectory\n",
    "    features = os.path.join(features_models, 'features')\n",
    "\n",
    "    # location to save results\n",
    "    model_save_loc = os.path.join(data_only_drive, 'Data', 'features_models',\n",
    "                                  'models')  # location where all the symbols are\n",
    "    # from the main directory select all the symbols that are finishing in .L for FTSE\n",
    "    symbols_ftse = [s for s in os.listdir(features) if s.endswith('.L')]\n",
    "    main_path = os.path.join(data_dir, 'features_models')  # main directory\n",
    "\n",
    "    features_path = os.path.join(main_path, 'features')  # all the features - same as above -redundant\n",
    "    labels_path = os.path.join(main_path, 'labels')  # all the labels\n",
    "\n",
    "  \n",
    "    # same as above- new target directory, where all the models and output is saved\n",
    "\n",
    "    model_paths = os.path.join(data_only_drive, 'Data', 'features_models',\n",
    "                               'models')\n",
    "    # using lambda to make a small function that just takes in the symbol\n",
    "    # and produces the relevant path of all fitted single kernel models\n",
    "\n",
    "    def symbol_fitted_models_path(symbol): return  os.path.join(model_paths, symbol, 'SINGLE_KERNEL')\n",
    "\n",
    "    # provides a fitted list of above path\n",
    "\n",
    "    def symbol_list_fitted_dates(symbol): return sorted(os.listdir(symbol_fitted_models_path(symbol)))\n",
    "\n",
    "    # fitted model sub-directory- the fitted model is stored in this sub-directory as a pickle\n",
    "\n",
    "    def symbol_fitted_model_date_loc(file_path, model_date_no): \n",
    "        return os.path.join(file_path, str(symbol_list_fitted_dates(symbol)[model_date_no]))\n",
    "\n",
    "\n",
    "    def symbol_model_date_loc(model_date_path): \n",
    "        return os.path.join(model_date_path, os.listdir(model_date_path)[0])\n",
    "\n",
    "    # test case ##\n",
    "    \n",
    "      # symbols to use as a starting point\n",
    "    good_symbols = [\n",
    "        'RDSa.L', 'PRU.L', 'III.L', 'REL.L', 'CNA.L', 'SHP.L', 'MKS.L',\n",
    "        'CPI.L', 'ULVR.L', 'ECM.L', 'AV.L', 'GKN.L', 'TSCO.L', 'ITV.L',\n",
    "        'BARC.L', 'CPG.L', 'AAL.L', 'LGEN.L', 'LAND.L', 'VOD.L', 'HSBA.L',\n",
    "        'RSA.L', 'DMGOa.L', 'RR.L', 'DGE.L', 'BATS.L', 'MAB.L',\n",
    "        'KGF.L', 'SPT.L', 'AZN.L'\n",
    "    ]\n",
    "\n",
    "    symbol = good_symbols[1] # picking PRU as an example\n",
    "\n",
    "    # test symbol path, which essentially produces the path where all the fitted models are.\n",
    "    # '/mnt/usb-Seagate_Expansion_Desk_NA8XEHR6-0:0-part2/Data/features_models/models/SPT.L/SINGLE_KERNEL'\n",
    "\n",
    "    test_path = symbol_fitted_models_path(symbol=symbol)\n",
    "\n",
    "    # now lets take the exact sub-directory which corresponds to date # 5 in our list of fitted models\n",
    "\n",
    "    test_fitted_model_date_loc = symbol_fitted_model_date_loc(test_path, 5)\n",
    "\n",
    "#     # now pick the first item in that list which is basically the pickle file\n",
    "#     # this is the entire path you have to load with pickle.load\n",
    "\n",
    "    model_pickle = os.path.join(test_fitted_model_date_loc, os.listdir(test_fitted_model_date_loc)[0])\n",
    "\n",
    "    # will take the input of a path (should be the models path, and a number and will produce a pickle file\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/usb-Seagate_Expansion_Desk_NA8XEHR6-0:0-part2/Data/features_models/models/PRU.L/SINGLE_KERNEL/20170124'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fitted_model_date_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/usb-Seagate_Expansion_Desk_NA8XEHR6-0:0-part2/Data/features_models/models/PRU.L/SINGLE_KERNEL/20170124/PRU.L_20170124_label_PrMov__window_5__thres_arbitrary__0.1_clf_fitted_.pickle'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/usb-Seagate_Expansion_Desk_NA8XEHR6-0:0-part2/Data/features_models/models/PRU.L/SINGLE_KERNEL/20170124/PRU.L_20170124_label_PrMov__window_5__thres_arbitrary__0.1_clf_fitted_.pickle\n",
      "('Your symbol is:', 'PRU.L', 'and the model date is:', '20170124')\n",
      "/media/ak/WorkDrive/Data/features_models/labels/PRU.L/NON_DIRECTIONAL\n",
      "/media/ak/WorkDrive/Data/features_models/features/PRU.L/MODEL_BASED\n"
     ]
    }
   ],
   "source": [
    "print model_pickle\n",
    "\n",
    "# getting the fitted model date, as this will be used in fwd dates\n",
    "model_date = test_fitted_model_date_loc.split(\"/\")[-1]\n",
    "\n",
    "print('Your symbol is:', symbol, 'and the model date is:' ,model_date)\n",
    "symbol_labels_path = os.path.join(labels_path, symbol, 'NON_DIRECTIONAL')\n",
    "print symbol_labels_path\n",
    "symbol_features_path = os.path.join(features_path, symbol, 'MODEL_BASED')\n",
    "print symbol_features_path\n",
    "\n",
    "# get all the dates of the labels from the labels path - this may be a bit redundant in the end\n",
    "\n",
    "labels_dates = sorted([os.listdir(symbol_labels_path)[idx].split(\".\")[0]\n",
    "                       for idx, _ in enumerate(os.listdir(symbol_labels_path))])\n",
    "\n",
    "# this is the location of the out of sample features\n",
    "\n",
    "oos_features_path = os.path.join(symbol_features_path, model_date)\n",
    "oos_dates_list = sorted([oos_date.split(\"_\")[5] for oos_date in\n",
    "                         sorted(os.listdir(oos_features_path))])  # list of oos features\n",
    "\n",
    "# keep only the fwd dates i.e the oos dates\n",
    "\n",
    "fwd_dates = fwd_dates(_dates_list=labels_dates, _key_date=model_date)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fnmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=os.listdir(oos_features_path)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/usb-Seagate_Expansion_Desk_NA8XEHR6-0:0-part2/Data/features_models/models/PRU.L/SINGLE_KERNEL/20170124/PRU.L_20170124_label_PrMov__window_5__thres_arbitrary__0.1_clf_fitted_.pickle'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "pat ='*20170125*'\n",
    "print fnmatch.fnmatch(model_pickle, pat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/usb-Seagate_Expansion_Desk_NA8XEHR6-0:0-part2/Data/features_models/models/PRU.L/SINGLE_KERNEL/20170124/PRU.L_20170124_label_PrMov__window_5__thres_arbitrary__0.1_clf_fitted_.pickle'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRU.L_3_states_features_date:_20170125_now:_20181229_.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for file_oos in os.listdir(oos_features_path):\n",
    "    if fnmatch.fnmatch(file_oos, pat):\n",
    "        print file_oos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_file = \"_\".join((symbol, '3_states_features_date:',oos_date,'now:_20181229_.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/usb-Seagate_Expansion_Desk_NA8XEHR6-0:0-part2/Data/features_models/models/PRU.L/SINGLE_KERNEL/accuracy_dictionary.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-4de9b742268b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_precision\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'accuracy_dictionary.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprecision_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/mnt/usb-Seagate_Expansion_Desk_NA8XEHR6-0:0-part2/Data/features_models/models/PRU.L/SINGLE_KERNEL/accuracy_dictionary.pickle'"
     ]
    }
   ],
   "source": [
    "test_precision= os.path.join(test_path, 'accuracy_dictionary.pickle')\n",
    "precision_dict=pickle.load(open(test_precision, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/usb-Seagate_Expansion_Desk_NA8XEHR6-0:0-part2/Data/features_models/models/PRU.L/SINGLE_KERNEL/recall_dictionary.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-666ef5108f58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_recall\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'recall_dictionary.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrecall_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_recall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/mnt/usb-Seagate_Expansion_Desk_NA8XEHR6-0:0-part2/Data/features_models/models/PRU.L/SINGLE_KERNEL/recall_dictionary.pickle'"
     ]
    }
   ],
   "source": [
    "test_recall= os.path.join(test_path, 'recall_dictionary.pickle')\n",
    "recall_dict=pickle.load(open(test_recall, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.8\n"
     ]
    }
   ],
   "source": [
    "G = {'E': 18.0, 'D': 17.0, 'C': 19.0, 'B': 15.0, 'A': 0}\n",
    "d = float(sum(G.values())) / len(G)\n",
    "print (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2=float(sum(precision_dict.values()))/len(precision_dict)\n",
    "d3= float(np.median(precision_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2=float(sum(recall_dict.values()))/len(recall_dict)\n",
    "r3= float(np.median(recall_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(precision_dict.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fitted_model_date_loc = symbol_fitted_model_date_loc(test_path, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fitted_model_date_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directories=[symbol_fitted_model_date_loc(test_path, idx) for idx, date in enumerate(os.listdir(test_path))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_locations =[os.path.join(model_directories[dir_idx],os.listdir(model_directories[dir_idx])[0]) for dir_idx, model_dir in enumerate(model_directories)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20170117\n",
      "20170118\n",
      "20170119\n",
      "20170120\n",
      "20170123\n",
      "20170124\n",
      "20170125\n",
      "20170126\n",
      "20170127\n",
      "20170130\n",
      "20170131\n",
      "20170301\n",
      "20170703\n",
      "20170704\n",
      "20170705\n",
      "20170706\n",
      "20170707\n",
      "20170710\n",
      "20170711\n",
      "20170712\n",
      "20170713\n",
      "20170714\n",
      "20170717\n",
      "20170718\n",
      "20170719\n",
      "20170720\n",
      "20170721\n",
      "20170724\n",
      "20170725\n",
      "20170726\n",
      "20170727\n",
      "20170728\n",
      "20170731\n",
      "20170801\n",
      "20170802\n",
      "20170803\n",
      "20170804\n",
      "20170807\n",
      "20170808\n",
      "20170809\n",
      "20170810\n",
      "20170811\n",
      "20170814\n",
      "20170815\n",
      "20170816\n",
      "20170817\n",
      "20170818\n",
      "20170821\n",
      "20170822\n",
      "20170823\n",
      "20170824\n",
      "20170825\n",
      "20170829\n",
      "20170830\n",
      "20170831\n",
      "20180201\n",
      "20180202\n",
      "20180205\n",
      "20180206\n",
      "20180207\n",
      "20180208\n",
      "20180209\n",
      "20180212\n",
      "20180213\n",
      "20180214\n",
      "20180215\n",
      "20180216\n",
      "20180219\n",
      "20180220\n",
      "20180221\n",
      "20180222\n",
      "20180223\n",
      "20180226\n",
      "20180227\n",
      "20180228\n",
      "20180403\n",
      "20180404\n",
      "20180405\n",
      "20180406\n",
      "20180409\n",
      "20180410\n",
      "20180411\n",
      "20180413\n",
      "20180418\n",
      "20180419\n",
      "20180420\n"
     ]
    }
   ],
   "source": [
    "for model_idx, model_loc in enumerate(models_locations):\n",
    "    model_date= model_loc.split(\"/\")[8]\n",
    "    model_pickle = model_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loc_specific(model_no): return  model_locations[model_no][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no = 4\n",
    "model_locations[model_no]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_pickle = os.path.join(test_fitted_model_date_loc, os.listdir(test_fitted_model_date_loc)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pickle_to_svc(model_pickle):\n",
    "    pickle_to_file = pickle.load(open(model_pickle, \"rb\"))\n",
    "    best_estimator = pickle_to_file['SVC'].best_estimator_\n",
    "    return best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_date(fitted_model_date_loc):\n",
    "    model_date = fitted_model_date_loc.split(\"/\")[-1]\n",
    "    return model_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_loc = os.path.join(data_only_drive, 'Data','features_models','metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRU.L_20170124_results_metrics.pickle']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(metrics_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_file = os.path.join(metrics_loc,os.listdir(metrics_loc)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict =pickle.load(open(metrics_file, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['recall', 'F1-score', 'accuracy']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dicamaz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/usb-Seagate_Expansion_Desk_NA8XEHR6-0:0-part2/Data/features_models/metrics/PRU.L_20170124_results_metrics.pickle'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
