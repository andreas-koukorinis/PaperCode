{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsmm_core.data_utils import DataLoader, TradingHours\n",
    "from hsmm_core.feature_spaces import hmm_features\n",
    "from hsmm_core.hsmm_runner import HmmCalibration\n",
    "import time\n",
    "from hsmm_core.consts import InitialisationMethod\n",
    "from hsmm_core.data_utils import TradingHours, DataLoader\n",
    "from hsmm_core.labelling import DataLabellingSimple\n",
    "from hsmm_core.consts import ThresholdMethod, LabellingChoice\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier,  GradientBoostingClassifier\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "sc = StandardScaler()\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "M = len(_keys) - 1  # number of models- essentially one for each day\n",
    "    T = 1\n",
    "    ###array for results###\n",
    "\n",
    "    _clfs_svm =[]\n",
    "\n",
    "    all_scores={\n",
    "        'test-mean': np.empty((M, 5)),\n",
    "        'test-std':  np.empty((M, 5)),\n",
    "        'train-mean': np.empty((M, 5)),\n",
    "        'train-std': np.empty((M, 5)),\n",
    "        }\n",
    "# 5 is from the train sizes below\n",
    "    _fitted_model_results = {\n",
    "        'clfs': np.empty((M, T)),\n",
    "        'svm_test_F1': np.empty((M, T)),\n",
    "        'svm_data_date': np.empty((M, T)),\n",
    "        'svm_test_recall': np.empty((M, T)),\n",
    "        'svm_train_recall': np.empty((M, T)),\n",
    "        'svm_test_accuracy': np.empty((M, T)),\n",
    "        'svm_train_accuracy': np.empty((M, T)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##useful functions\n",
    "def fwd_dates(_dates_list, _key_date):\n",
    "    # returns a list of dates that are forward from the key_date\n",
    "    fwd_dates_list = [i for i in _dates_list if i > _key_date]\n",
    "    return fwd_dates_list\n",
    "def remove_nans(features_tuple, labels, idx=1):\n",
    "    # not the cleanest but useful\n",
    "    # function to clean up nans as I seem to use it a lot, so better to have one function\n",
    "    # combines the features and labels and removes rows with nans across so we dont lose the ordering\n",
    "    # returns features and labels\n",
    "    features_df = pd.concat([features_tuple[0], features_tuple[1], features_tuple[2], \\\n",
    "                             features_tuple[3]], axis=1, sort=False)\n",
    "    labels_only = labels.drop(columns=['ReturnTradedPrice', 'Duration', 'states', 'TradedTime',\n",
    "                                       'TradedPrice'], axis=1)\n",
    "    df_concat = pd.concat([features_df, labels_only.iloc[:, 0:idx]], axis=1, sort='False')\n",
    "    # only using 1st set of labels- but we can re-write this a bit\n",
    "    df_x_nan = df_concat.dropna()  # dropping all nans\n",
    "    label_column_loc_ = df_x_nan.shape[1] - 1  # location of labels column in the clean df\n",
    "    labels_ = df_x_nan.iloc[:, label_column_loc_:label_column_loc_ + 1]  # keep pure labels\n",
    "    features_ = df_x_nan.drop(df_x_nan.columns[label_column_loc_], axis=1)  # keeping the features only\n",
    "    return features_, labels_ \n",
    "\n",
    "def prec_recall_report(y_true, y_predict):\n",
    "    # function to ge the sci-kit learn classification metrics into a pretty DF for csv!\n",
    "    report = pd.DataFrame(list(precision_recall_fscore_support(y_true, y_predict)),\n",
    "                          index=['Precision', 'Recall', 'F1-score', 'Support']).T\n",
    "    # Now add the 'Avg/Total' row\n",
    "    report.loc['Avg/Total', :] = precision_recall_fscore_support(y_true, y_predict, average='weighted')\n",
    "    report.loc['Avg/Total', 'Support'] = report['Support'].sum()\n",
    "    return report\n",
    "\n",
    "class MarketFeatures(object):\n",
    "    # a class to be expanded that uses features for base case -market based only-indicators/features\n",
    "    \"\"\"\"Requires:\n",
    "    a dataframe that has TradedPrice And Volume columns\n",
    "    symbol - A stock symbol on which to form a strategy on.\n",
    "    short_window - Lookback period for short moving average.\n",
    "    long_window - Lookback period for long moving average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        #         self.ticker = ticker\n",
    "        self.df = df\n",
    "\n",
    "    def load_data(self):\n",
    "        pass\n",
    "\n",
    "    def ma_spread(self, short_window=5, long_window=20):\n",
    "        # function that produces the MA spread, which can be used on its own or as an input for MACD\n",
    "        short_rolling_px = self.df['TradedPrice'].rolling(window=short_window).mean()\n",
    "        long_rolling_px = self.df['TradedPrice'].rolling(window=long_window).mean()\n",
    "        px_name = \"_\".join(('px_indx', str(short_window), str(long_window)))\n",
    "        self.df[px_name] = long_rolling_px - short_rolling_px\n",
    "        return self.df\n",
    "\n",
    "    def ma_spread_duration(self, short_window=5, long_window=20):\n",
    "        # function that produces the MA spread, which can be used on its own or as an input for MACD\n",
    "        short_rolling_px = self.df['Duration'].rolling(window=short_window).mean()\n",
    "        long_rolling_px = self.df['Duration'].rolling(window=long_window).mean()\n",
    "        dur_name = \"_\".join(('dur_indx', str(short_window), str(long_window)))\n",
    "        self.df[dur_name] = long_rolling_px - short_rolling_px\n",
    "        return self.df\n",
    "\n",
    "    def obv_calc(self):\n",
    "        # on balance volume indicator\n",
    "        self.df['SignedVolume'] = self.df['Volume'] * np.sign(self.df['TradedPrice'].diff()).cumsum()\n",
    "        self.df['SignedVolume'].iat[1] = 0\n",
    "        self.df['OBV'] = self.df['SignedVolume']  # .cumsum()\n",
    "        self.df = self.df.drop(columns=['SignedVolume'])\n",
    "        return self.df\n",
    "\n",
    "    def chaikin_mf(self, period=5):\n",
    "        # Chaikin money flow indicator\n",
    "        self.df[\"MF Multiplier\"] = (self.df['TradedPrice'] - (self.df['TradedPrice'].expanding(period).min()) \\\n",
    "                                    - (self.df['TradedPrice'].expanding(period).max() \\\n",
    "                                       - self.df['TradedPrice'])) / (\n",
    "                                           self.df['TradedPrice'].expanding(period).max() - self.df[ \\\n",
    "                                       'TradedPrice'].expanding(period).min())\n",
    "        self.df[\"MF Volume\"] = self.df['MF Multiplier'] * self.df['Volume']\n",
    "        self.df['CMF_' + str(period)] = self.df['MF Volume'].sum() / self.df[\"Volume\"].rolling(period).sum()\n",
    "        self.df = self.df.drop(columns=['MF Multiplier', 'MF Volume'])\n",
    "        return self.df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## locations ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbols to use as a starting point\n",
    "good_symbols = [\n",
    "    'RDSa.L', 'PRU.L', 'III.L', 'REL.L', 'CNA.L', 'SHP.L', 'MKS.L',\n",
    "    'CPI.L', 'ULVR.L',  'ECM.L', 'AV.L', 'GKN.L', 'TSCO.L',  'ITV.L',  \n",
    "    'BARC.L', 'CPG.L', 'AAL.L', 'LGEN.L', 'LAND.L', 'VOD.L', 'HSBA.L', \n",
    "    'RSA.L', 'DMGOa.L', 'RR.L', 'DGE.L', 'BATS.L','MAB.L',\n",
    "    'KGF.L', 'SPT.L', 'AZN.L'\n",
    "    ]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_only_drive' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-35d2d61f0d16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#simple functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_only_drive\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'features_models'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#same as above- new target directory, where all the models and output is saved\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# using lambda to make a small function that just takes in the symbol and produces the relevant path of all fitted single kernel models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msymbol_fitted_models_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0msymbol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SINGLE_KERNEL'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#provides a fitted list of above path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_only_drive' is not defined"
     ]
    }
   ],
   "source": [
    "#simple functions\n",
    "model_paths = os.path.join(data_only_drive,'Data','features_models','models') #same as above- new target directory, where all the models and output is saved\n",
    "# using lambda to make a small function that just takes in the symbol and produces the relevant path of all fitted single kernel models\n",
    "symbol_fitted_models_path = lambda symbol: os.path.join(model_paths, symbol, 'SINGLE_KERNEL') \n",
    "#provides a fitted list of above path\n",
    "symbol_list_fitted_dates= sorted(os.listdir(symbol_fitted_models_path(symbol)))\n",
    "#fitted model sub-directory- the fitted model is stored in this sub-directory as a pickle\n",
    "symbol_fitted_model_date_loc = lambda file_path, model_date_no: os.path.join(file_path,\\\n",
    "                                                              symbol_list_fitted_dates[model_date_no])\n",
    "# will take the input of a path (should be the models path, and a number and will produce a pickle file\n",
    "symbol_model_date_loc = lambda model_date_path:os.path.join(model_date_path, os.listdir(model_date_path)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol= 'SPT.L' #this is a test symbol\n",
    "#test symbol path, which essentially produces the path where all the fitted models are.\n",
    "#'/mnt/usb-Seagate_Expansion_Desk_NA8XEHR6-0:0-part2/Data/features_models/models/SPT.L/SINGLE_KERNEL'\n",
    "test_path= symbol_fitted_models_path(symbol=symbol)\n",
    "# now lets take the exact sub-directory which corresponds to date # 5 in our list of fitted models\n",
    "test_fitted_model_date_loc = symbol_fitted_model_date_loc(test_path, 5)\n",
    "# now pick the first item in that list which is basically the pickle file\n",
    "# this is the entire path you have to load with pickle.load\n",
    "model_pickle = os.path.join(test_fitted_model_date,os.listdir(test_fitted_model_date)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#again this is the pickle file that we get the location for. now it is written in the form of a function\n",
    "\n",
    "model_loc= symbol_model_date_loc(test_fitted_model_date)\n",
    "\n",
    "#getting the fitted model date, as this will be used in fwd dates\n",
    "model_date= test_fitted_model_date.split(\"/\")[-1]\n",
    "\n",
    "model_pickle = os.path.join(test_fitted_model_date,os.listdir(test_fitted_model_date)[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_to_file = pickle.load(open(model_loc, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator = pickle_to_file['SVC'].best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_labels_path = os.path.join(labels_path, symbol, 'NON_DIRECTIONAL')\n",
    "symbol_features_path = os.path.join(features_path, symbol, 'MODEL_BASED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dates = sorted([os.listdir(symbol_labels_path)[idx].split(\".\")[0]\\\n",
    "                for idx,_ in enumerate(os.listdir(symbol_labels_path))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(oos_features_path)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwd_dates =fwd_dates(_dates_list=labels_dates, _key_date=test_fitted_model_date.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'symbol_features_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f20908a59170>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moos_features_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol_features_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_date\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m oos_dates_list = sorted([oos_date.split(\"_\")[5] for oos_date in\n\u001b[1;32m      3\u001b[0m                                      sorted(os.listdir(oos_features_path))])  # list of oos features\n",
      "\u001b[0;31mNameError\u001b[0m: name 'symbol_features_path' is not defined"
     ]
    }
   ],
   "source": [
    "oos_features_path = os.path.join(symbol_features_path, model_date )\n",
    "oos_dates_list = sorted([oos_date.split(\"_\")[5] for oos_date in\n",
    "                                     sorted(os.listdir(oos_features_path))])  # list of oos features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'oos_features_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d6350fcaf94e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfnmatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0moos_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moos_features_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfnmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfnmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moos_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SPT.L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moos_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'oos_features_path' is not defined"
     ]
    }
   ],
   "source": [
    "oos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_file= \"_\".join(('SPT.L_3_states_features_date:',fwd_date,'now:_20181230_.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fwd_date in fwd_dates:\n",
    "    fwd_file= \"_\".join(('SPT.L_3_states_features_date:',fwd_date,'now:_20181230_.pickle'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_date = os.listdir(oos_features_path)[0].split(\"_\")[5]\n",
    "features_tuple= os.listdir(oos_features_path)[0]\n",
    "labels_oos= pd.read_csv(os.path.join(labels_path, symbol,'NON_DIRECTIONAL', features_date+'.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_loc = os.path.join(symbol_features_path,model_date,features_tuple)\n",
    "features_tuple= pickle.load(open(features_loc, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.concat([features_tuple[0], features_tuple[1],\\\n",
    "                         features_tuple[2], features_tuple[3]], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_market_features = MarketFeatures(df=MarketFeatures( \\\n",
    "                            df=MarketFeatures(\n",
    "                                df=MarketFeatures(df=labels_oos).obv_calc()).chaikin_mf()).ma_spread()).ma_spread_duration()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = pd.concat([features_df, df_w_market_features], axis=1, sort='False').dropna()\n",
    "\n",
    "                        # ok start putting in the magic\n",
    "                        # y_duration = np.asanyarray(df_concat['Duration'].shift(window).dropna())\n",
    "                        # y_price = np.asanyarray(df_concat['TradedPrice'].shift(window).dropna())\n",
    "\n",
    "                        # drop things we dont need: traded price, duration, traded time, labels etc!\n",
    "label_name = str(df_concat.columns[df_concat.columns.str.contains(pat='label')].values[0])\n",
    "\n",
    "df_final = df_concat.drop(columns=['TradedPrice', 'Duration', 'TradedTime', 'ReturnTradedPrice', \\\n",
    "                                   'Volume', label_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X= MinMaxScaler().fit_transform(df_final)\n",
    "\n",
    "y_labels = df_concat[df_concat.columns[df_concat.columns.str.contains(pat='label')]].iloc[:, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict =best_estimator.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _fitted_model_results['svm_test_accuracy'][_idx, :] = accuracy_score(y_test, y_predict)\n",
    "# _fitted_model_results['svm_test_recall'][_idx, :] = recall_score(y_true=y_test, y_pred=y_predict)\n",
    "# _fitted_model_results['svm_train_accuracy'][_idx, :] = accuracy_score(y_train, y_predict_train)\n",
    "# _fitted_model_results['svm_train_recall'][_idx, :] = recall_score(y_true=y_train, y_pred=y_predict_train)\n",
    "# _fitted_model_results['svm_test_F1'][_idx, :] = f1_score(y_true=y_test, y_pred=y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print accuracy_score(y_labels, y_predict)\n",
    "print recall_score(y_true=y_labels, y_pred=y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_true=y_labels, y_pred=y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
