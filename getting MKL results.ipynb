{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import sys\n",
    "import sys\n",
    "from memory_profiler import profile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sys.path.append('/home/ak/Documents/Research/PaperCode/MultiKernelLearning')\n",
    "sys.path.append('/home/ak/Documents/Research/PaperCode/singlekernelclf/')\n",
    "import jsonpickle\n",
    "from collections import defaultdict\n",
    "from sklearn.svm import SVC\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "\n",
    "\n",
    "from fileutils import new_feature_utils as nfu\n",
    "from fileutils.new_feature_utils import CreateMarketFeatures\n",
    "import multiprocessing\n",
    "import mkl_data_processing as mkldp\n",
    "from matplotlib.ticker import ScalarFormatter,AutoMinorLocator\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "styleFiles = os.listdir(mpl.get_configdir())\n",
    "styleFileIdx = 0\n",
    "\n",
    "plt.style.use(os.path.join(mpl.get_configdir(), styleFiles[styleFileIdx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_pickle_filepath(pickle_file):\n",
    "    pickle_to_file = pickle.load(open(pickle_file, \"rb\"), encoding='latin1')\n",
    "\n",
    "    return pickle_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_location = '/media/ak/DataOnly/ExperimentCommonLocs/MKLOOSPredictions/SimpleMKL/'\n",
    "symbols = sorted(os.listdir(files_location))\n",
    "#processed results path\n",
    "processed_location = '/media/ak/DataOnly/ExperimentCommonLocs/MKLOOSProcessed'\n",
    "heuristic_path = '/media/ak/DataOnly/ExperimentCommonLocs/MKLOOSPredictions/Heuristic/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, symbol in enumerate(symbols):\n",
    "#     print(idx, symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a loop that goes through all the symbols below and takes out all the data in one go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#symbols to loop over\n",
    "#9 for CPG\n",
    "#12 for KGF\n",
    "# 22 for RBS\n",
    "# 25 for REL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AAL.L',\n",
       " 'APF.L',\n",
       " 'AV.L',\n",
       " 'AZN.L',\n",
       " 'BARC.L',\n",
       " 'BATS.L',\n",
       " 'BLT.L',\n",
       " 'CCL.L',\n",
       " 'CEY.L',\n",
       " 'CPG.L',\n",
       " 'CPI.L',\n",
       " 'ITV.L',\n",
       " 'KGF.L',\n",
       " 'LAND.L',\n",
       " 'LGEN.L',\n",
       " 'LLOY.L',\n",
       " 'MAB.L',\n",
       " 'MKS.L',\n",
       " 'NG.L',\n",
       " 'PRU.L',\n",
       " 'PSON.L',\n",
       " 'RB.L',\n",
       " 'RBS.L',\n",
       " 'RDSa.L',\n",
       " 'RDSb.L',\n",
       " 'REL.L',\n",
       " 'RR.L',\n",
       " 'RSA.L',\n",
       " 'RTO.L',\n",
       " 'SDR.L',\n",
       " 'SGE.L',\n",
       " 'SHP.L',\n",
       " 'SMIN.L',\n",
       " 'SPT.L',\n",
       " 'STAN.L',\n",
       " 'TSCO.L',\n",
       " 'ULVR.L',\n",
       " 'UU.L',\n",
       " 'VOD.L',\n",
       " 'WPP.L']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # symbolIDX =25\n",
    "# # label_idx = 6 #pick a label\n",
    "# symbols_list = list()\n",
    "# symbols_label_dict = defaultdict(dict)\n",
    "# for symbolIDX in [13]:\n",
    "#     for label_idx in range(1, 7):\n",
    "#         label_string= 'Label_idx:_'+str(label_idx) # construct the label\n",
    "#         print('you are doing symbol', symbols[symbolIDX], 'and label', label_string)\n",
    "#         print(' and symbol IDX ', symbolIDX)\n",
    "#         # now we take out the files for the symbol \n",
    "#         processed_symbol_path = os.path.join(processed_location,str(sorted(symbols)[symbolIDX]))\n",
    "#         os.listdir(processed_symbol_path)\n",
    "#         chunks_file = str(sorted(symbols)[symbolIDX])+'_'+'Label_'+str(label_idx)+'_all_chunks.csv'\n",
    "#         dates_file = str(sorted(symbols)[symbolIDX])+'_'+'Label_'+str(label_idx)+'_dates.csv'\n",
    "\n",
    "\n",
    "#         chunks_path = os.path.join(processed_symbol_path, chunks_file)\n",
    "#         dates_file_path = os.path.join(processed_symbol_path, dates_file)\n",
    "#         columns_values = ['Hamming Loss', 'accuracy', 'f1- macro', 'f1- micro',\n",
    "#        'f1- weighted', 'precision', 'recall']\n",
    "\n",
    "# #         label_dict[label_idx] = pd.read_csv(chunks_path).median(axis=1)\n",
    "#         symbols_label_dict[symbols[symbolIDX]][label_idx] = pd.read_csv(chunks_path).set_index([pd.Index(columns_values)]).median(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CPG.L</th>\n",
       "      <th>KGF.L</th>\n",
       "      <th>RBS.L</th>\n",
       "      <th>REL.L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hamming Loss    0.03\n",
       "accuracy        0.97\n",
       "f1- ...</td>\n",
       "      <td>Hamming Loss    0.02\n",
       "accuracy        0.98\n",
       "f1- ...</td>\n",
       "      <td>Hamming Loss    0.03\n",
       "accuracy        0.97\n",
       "f1- ...</td>\n",
       "      <td>Hamming Loss    0.02\n",
       "accuracy        0.98\n",
       "f1- ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hamming Loss    0.18\n",
       "accuracy        0.82\n",
       "f1- ...</td>\n",
       "      <td>Hamming Loss    0.11\n",
       "accuracy        0.89\n",
       "f1- ...</td>\n",
       "      <td>Hamming Loss    0.10\n",
       "accuracy        0.90\n",
       "f1- ...</td>\n",
       "      <td>Hamming Loss    0.22\n",
       "accuracy        0.78\n",
       "f1- ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hamming Loss    0.66\n",
       "accuracy        0.34\n",
       "f1- ...</td>\n",
       "      <td>Hamming Loss    0.62\n",
       "accuracy        0.38\n",
       "f1- ...</td>\n",
       "      <td>Hamming Loss    0.62\n",
       "accuracy        0.38\n",
       "f1- ...</td>\n",
       "      <td>Hamming Loss    0.60\n",
       "accuracy        0.40\n",
       "f1- ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hamming Loss    0.23\n",
       "accuracy        0.77\n",
       "f1- ...</td>\n",
       "      <td>Hamming Loss    0.14\n",
       "accuracy        0.86\n",
       "f1- ...</td>\n",
       "      <td>Hamming Loss    0.14\n",
       "accuracy        0.86\n",
       "f1- ...</td>\n",
       "      <td>Hamming Loss    0.275\n",
       "accuracy        0.725\n",
       "f1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hamming Loss    0.545\n",
       "accuracy        0.460\n",
       "f1...</td>\n",
       "      <td>Hamming Loss    0.350\n",
       "accuracy        0.650\n",
       "f1...</td>\n",
       "      <td>Hamming Loss    0.80\n",
       "accuracy        0.20\n",
       "f1- ...</td>\n",
       "      <td>Hamming Loss    0.50\n",
       "accuracy        0.50\n",
       "f1- ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hamming Loss    0.54\n",
       "accuracy        0.46\n",
       "f1- ...</td>\n",
       "      <td>Hamming Loss    0.54\n",
       "accuracy        0.46\n",
       "f1- ...</td>\n",
       "      <td>Hamming Loss    0.53\n",
       "accuracy        0.47\n",
       "f1- ...</td>\n",
       "      <td>Hamming Loss    0.565\n",
       "accuracy        0.435\n",
       "f1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               CPG.L  \\\n",
       "1  Hamming Loss    0.03\n",
       "accuracy        0.97\n",
       "f1- ...   \n",
       "2  Hamming Loss    0.18\n",
       "accuracy        0.82\n",
       "f1- ...   \n",
       "3  Hamming Loss    0.66\n",
       "accuracy        0.34\n",
       "f1- ...   \n",
       "4  Hamming Loss    0.23\n",
       "accuracy        0.77\n",
       "f1- ...   \n",
       "5  Hamming Loss    0.545\n",
       "accuracy        0.460\n",
       "f1...   \n",
       "6  Hamming Loss    0.54\n",
       "accuracy        0.46\n",
       "f1- ...   \n",
       "\n",
       "                                               KGF.L  \\\n",
       "1  Hamming Loss    0.02\n",
       "accuracy        0.98\n",
       "f1- ...   \n",
       "2  Hamming Loss    0.11\n",
       "accuracy        0.89\n",
       "f1- ...   \n",
       "3  Hamming Loss    0.62\n",
       "accuracy        0.38\n",
       "f1- ...   \n",
       "4  Hamming Loss    0.14\n",
       "accuracy        0.86\n",
       "f1- ...   \n",
       "5  Hamming Loss    0.350\n",
       "accuracy        0.650\n",
       "f1...   \n",
       "6  Hamming Loss    0.54\n",
       "accuracy        0.46\n",
       "f1- ...   \n",
       "\n",
       "                                               RBS.L  \\\n",
       "1  Hamming Loss    0.03\n",
       "accuracy        0.97\n",
       "f1- ...   \n",
       "2  Hamming Loss    0.10\n",
       "accuracy        0.90\n",
       "f1- ...   \n",
       "3  Hamming Loss    0.62\n",
       "accuracy        0.38\n",
       "f1- ...   \n",
       "4  Hamming Loss    0.14\n",
       "accuracy        0.86\n",
       "f1- ...   \n",
       "5  Hamming Loss    0.80\n",
       "accuracy        0.20\n",
       "f1- ...   \n",
       "6  Hamming Loss    0.53\n",
       "accuracy        0.47\n",
       "f1- ...   \n",
       "\n",
       "                                               REL.L  \n",
       "1  Hamming Loss    0.02\n",
       "accuracy        0.98\n",
       "f1- ...  \n",
       "2  Hamming Loss    0.22\n",
       "accuracy        0.78\n",
       "f1- ...  \n",
       "3  Hamming Loss    0.60\n",
       "accuracy        0.40\n",
       "f1- ...  \n",
       "4  Hamming Loss    0.275\n",
       "accuracy        0.725\n",
       "f1...  \n",
       "5  Hamming Loss    0.50\n",
       "accuracy        0.50\n",
       "f1- ...  \n",
       "6  Hamming Loss    0.565\n",
       "accuracy        0.435\n",
       "f1...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(symbols_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_dict = symbols_label_dict\n",
    "columns_values = ['Hamming Loss', 'accuracy', 'f1- macro', 'f1- micro',\n",
    "       'f1- weighted', 'precision', 'recall']\n",
    "df =pd.DataFrame.from_dict({(i,j): user_dict[i][j] \n",
    "                           for i in user_dict.keys() \n",
    "                           for j in user_dict[i].keys()},\n",
    "                       orient='index')\n",
    "df.index.name = (\"Symbol, Label\")\n",
    "df.index.set_names(['Symbol', 'Label'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\centering\n",
      "\\caption{Select symbol Multi-Kernel results}\n",
      "\\begin{tabular}{llrrrrrrr}\n",
      "\\toprule\n",
      "      &   &  Hamming Loss &  accuracy &  f1- macro &  f1- micro &  f1- weighted &  precision &  recall \\\\\n",
      "Symbol & Label &               &           &            &            &               &            &         \\\\\n",
      "\\midrule\n",
      "CPG.L & 1 &         0.030 &     0.970 &       0.49 &      0.970 &         0.960 &       0.95 &   0.970 \\\\\n",
      "      & 2 &         0.180 &     0.820 &       0.45 &      0.820 &         0.740 &       0.68 &   0.820 \\\\\n",
      "      & 3 &         0.660 &     0.340 &       0.25 &      0.340 &         0.260 &       0.33 &   0.340 \\\\\n",
      "      & 4 &         0.230 &     0.770 &       0.44 &      0.770 &         0.670 &       0.60 &   0.770 \\\\\n",
      "      & 5 &         0.545 &     0.460 &       0.21 &      0.460 &         0.290 &       0.22 &   0.460 \\\\\n",
      "      & 6 &         0.540 &     0.460 &       0.24 &      0.460 &         0.340 &       0.40 &   0.460 \\\\\n",
      "KGF.L & 1 &         0.020 &     0.980 &       0.49 &      0.980 &         0.970 &       0.96 &   0.980 \\\\\n",
      "      & 2 &         0.110 &     0.890 &       0.47 &      0.890 &         0.850 &       0.80 &   0.890 \\\\\n",
      "      & 3 &         0.620 &     0.380 &       0.26 &      0.380 &         0.300 &       0.30 &   0.380 \\\\\n",
      "      & 4 &         0.140 &     0.860 &       0.46 &      0.860 &         0.800 &       0.74 &   0.860 \\\\\n",
      "      & 5 &         0.350 &     0.650 &       0.26 &      0.650 &         0.515 &       0.43 &   0.650 \\\\\n",
      "      & 6 &         0.540 &     0.460 &       0.22 &      0.460 &         0.320 &       0.30 &   0.460 \\\\\n",
      "RBS.L & 1 &         0.030 &     0.970 &       0.49 &      0.970 &         0.950 &       0.94 &   0.970 \\\\\n",
      "      & 2 &         0.100 &     0.900 &       0.47 &      0.900 &         0.850 &       0.80 &   0.900 \\\\\n",
      "      & 3 &         0.620 &     0.380 &       0.26 &      0.380 &         0.300 &       0.30 &   0.380 \\\\\n",
      "      & 4 &         0.140 &     0.860 &       0.46 &      0.860 &         0.790 &       0.73 &   0.860 \\\\\n",
      "      & 5 &         0.800 &     0.200 &       0.17 &      0.200 &         0.100 &       0.08 &   0.200 \\\\\n",
      "      & 6 &         0.530 &     0.470 &       0.21 &      0.470 &         0.300 &       0.22 &   0.470 \\\\\n",
      "REL.L & 1 &         0.020 &     0.980 &       0.50 &      0.980 &         0.980 &       0.97 &   0.980 \\\\\n",
      "      & 2 &         0.220 &     0.780 &       0.44 &      0.780 &         0.690 &       0.61 &   0.780 \\\\\n",
      "      & 3 &         0.600 &     0.400 &       0.21 &      0.400 &         0.260 &       0.22 &   0.400 \\\\\n",
      "      & 4 &         0.275 &     0.725 &       0.42 &      0.725 &         0.610 &       0.53 &   0.725 \\\\\n",
      "      & 5 &         0.500 &     0.500 &       0.22 &      0.500 &         0.340 &       0.25 &   0.500 \\\\\n",
      "      & 6 &         0.565 &     0.435 &       0.20 &      0.435 &         0.260 &       0.19 &   0.435 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.to_latex( caption=\"Select symbol Multi-Kernel results\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.to_latex?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated = pd.DataFrame.from_dict(symbols_label_dict)\n",
    "columns_values = ['Hamming Loss', 'accuracy', 'f1- macro', 'f1- micro',\n",
    "       'f1- weighted', 'precision', 'recall']\n",
    "print(consolidated.set_index([pd.Index(columns_values)]).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pick a symbol\n",
    "for symbolIDX in [22]: \n",
    "    #range(0,40):\n",
    "    label_idx = 1 #pick a label\n",
    "    label_string= 'Label_idx:_'+str(label_idx) # construct the label\n",
    "    print('you are doing symbol', symbols[symbolIDX], 'and label', label_string)\n",
    "    print(' and symbol IDX ', symbolIDX)\n",
    "    # now we take out the files for the symbol \n",
    "    symbol_path = os.path.join(files_location,str(sorted(symbols)[symbolIDX]))\n",
    "    # now take the symbol path and connect it to get all the files \n",
    "    symbolFiles = os.listdir(os.path.join(files_location,str(sorted(symbols)[symbolIDX])))\n",
    "    # now isolate all the label+ symbol combos only\n",
    "    symbolLabelFiles = [f for f in symbolFiles if str(label_string) in f]\n",
    "    # this is the path you will store something \n",
    "    path_to_store = os.path.join(processed_location, symbols[symbolIDX])\n",
    "    # here is the number of files that have the specific combination\n",
    "    number_of_files = len(symbolLabelFiles) \n",
    "\n",
    "    # this is the number of files that correspond to the label/symbol combo\n",
    "\n",
    "    print('total files for the symbol:', len(symbolFiles)) # total how many you have and how many for a specific label\n",
    "    print('and for this label')\n",
    "    print(number_of_files)\n",
    "\n",
    "    dates_chunks_keys = dict()\n",
    "    chunks_list =list()\n",
    "    keys_list = list()\n",
    "\n",
    "    for file_to_load_idx in range(0,number_of_files):\n",
    "        # go through all of the files that are a combination of symbols and labels\n",
    "        file_to_load_path = os.path.join(os.path.join(files_location, str(symbols[symbolIDX])), symbolLabelFiles[file_to_load_idx])\n",
    "\n",
    "        if os.path.getsize(file_to_load_path)> 0:\n",
    "            # extract the key - these are the symbol feature path keys in the  main code\n",
    "            file_keys = list(open_pickle_filepath(file_to_load_path).keys()) # keys\n",
    "            keys_list.append(file_keys[0])\n",
    "            dates_chunks_keys[file_to_load_idx] = file_keys\n",
    "            chunks_list.append(open_pickle_filepath(file_to_load_path)[file_keys[0]])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    dates_df = pd.DataFrame((np.unique(list(dates_chunks_keys.values())))) # strip out all the dates I have chunks for\n",
    "\n",
    "    list_of_dfs_across_days = list()\n",
    "\n",
    "    for chunk_list_item in range(0,len(chunks_list)-1): # go through all the chunks\n",
    "        chunk_list_item_dict = chunks_list[chunk_list_item]\n",
    "        chunk_list_item_keys = list(chunk_list_item_dict.keys())\n",
    "        list_of_dicts=list()\n",
    "        for key in chunk_list_item_keys:\n",
    "            list_of_dicts.append(pd.DataFrame(chunk_list_item_dict[key]))\n",
    "        list_of_dfs_across_days.append(pd.concat(list_of_dicts,axis=1).median(axis=1))\n",
    "\n",
    "    ###\n",
    "    try:\n",
    "        df =pd.concat(list_of_dfs_across_days, axis =1)\n",
    "        df.median(axis =1)\n",
    "        #df.to_csv('file_name.csv\n",
    "        location_for_df = os.path.join(path_to_store, symbols[symbolIDX] +\"_Label_\"+str(label_idx)+\"_all_chunks.csv\")\n",
    "        df.to_csv(location_for_df, index= True)\n",
    "        location_for_dates_df = os.path.join(path_to_store, symbols[symbolIDX] +\"_Label_\"+str(label_idx)+\"_dates.csv\")\n",
    "        dates_df.to_csv(location_for_dates_df, index = True)\n",
    "    except(ValueError):\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_idx = 2\n",
    "label_string= 'Label_idx:_'+str(label_idx) # construct the label\n",
    "symbols_label = dict()\n",
    "symbols_label_list = []\n",
    "for root, dirs, files in os.walk(files_location):\n",
    "    for name in files:\n",
    "        if name.__contains__(str(label_string)):\n",
    "            print(name.split(\"_\")[0])\n",
    "            symbols_label_list.append(name.split(\"_\")[0])\n",
    "#         symbols_label[name.split(\"_\")[0]] = np.unique(symbols_label_list)\n",
    "symbols_label[label_idx] = np.unique(symbols_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.isdir('/media/ak/DataOnly/SymbolFeatureDirectories/KGF.L/MODEL_BASED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(['APF.L', 'AV.L', 'BLT.L', 'CCL.L', 'CEY.L', 'ITV.L', 'KGF.L',\n",
    "        'LAND.L', 'LGEN.L', 'MAB.L', 'MKS.L', 'NG.L', 'PRU.L', 'PSON.L',\n",
    "        'RBS.L', 'REL.L', 'RR.L', 'SHP.L', 'SMIN.L', 'SPT.L', 'ULVR.L'])\n",
    "len(label_2_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {1: array(['APF.L', 'AV.L', 'AZN.L', 'BARC.L', 'BLT.L', 'CCL.L', 'CPG.L',\n",
    "#         'ITV.L', 'KGF.L', 'LLOY.L', 'RDSa.L', 'REL.L', 'RR.L', 'RSA.L',\n",
    "#         'TSCO.L', 'ULVR.L'], dtype='<U6')}\n",
    "\n",
    "# {2: array(['APF.L', 'AZN.L', 'BARC.L', 'BLT.L', 'CCL.L', 'CEY.L', 'CPG.L',\n",
    "#         'ITV.L', 'KGF.L', 'LAND.L', 'LGEN.L', 'LLOY.L', 'MAB.L', 'MKS.L',\n",
    "#         'REL.L', 'RR.L', 'RSA.L', 'TSCO.L', 'ULVR.L', 'VOD.L'], dtype='<U6')}\n",
    "\n",
    "# {3: array(['APF.L', 'AV.L', 'BLT.L', 'CCL.L', 'CEY.L', 'ITV.L', 'KGF.L',\n",
    "#         'LAND.L', 'LGEN.L', 'MAB.L', 'MKS.L', 'NG.L', 'PRU.L', 'PSON.L',\n",
    "#         'RBS.L', 'REL.L', 'RR.L', 'SHP.L', 'SMIN.L', 'SPT.L', 'ULVR.L'],\n",
    "#        dtype='<U6')}\n",
    "\n",
    "# {4: array(['APF.L', 'AV.L', 'AZN.L', 'BARC.L', 'BLT.L', 'CCL.L', 'CEY.L',\n",
    "#         'CPG.L', 'ITV.L', 'KGF.L', 'LAND.L', 'LGEN.L', 'LLOY.L', 'MAB.L',\n",
    "#         'MKS.L', 'NG.L', 'PRU.L', 'PSON.L', 'RB.L', 'RBS.L', 'RDSa.L',\n",
    "#         'RDSb.L', 'REL.L', 'RR.L', 'RSA.L', 'RTO.L', 'SDR.L', 'SGE.L',\n",
    "#         'SHP.L', 'SMIN.L', 'SPT.L', 'STAN.L', 'TSCO.L', 'ULVR.L', 'UU.L',\n",
    "#         'VOD.L', 'WPP.L'], dtype='<U6')}\n",
    "\n",
    "# {5: array(['APF.L', 'AV.L', 'BLT.L', 'CPG.L', 'ITV.L', 'KGF.L', 'LAND.L',\n",
    "#         'LGEN.L', 'MAB.L', 'MKS.L', 'PRU.L', 'PSON.L', 'RBS.L', 'RDSa.L',\n",
    "#         'REL.L', 'RR.L', 'RSA.L', 'SDR.L', 'SGE.L', 'SHP.L', 'SMIN.L',\n",
    "#         'SPT.L', 'UU.L'], dtype='<U6')}\n",
    "\n",
    "\n",
    "\n",
    "# {6: array(['APF.L', 'CCL.L', 'CEY.L', 'CPG.L', 'ITV.L', 'KGF.L', 'LAND.L',\n",
    "#         'MKS.L', 'REL.L', 'RR.L', 'RSA.L', 'SPT.L', 'STAN.L', 'WPP.L'],\n",
    "#        dtype='<U6')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_1_files = ['BARC.L_Label_1_all_chunks.csv', \n",
    "                 'APF.L_Label_1_all_chunks.csv', \n",
    "                 'AV.L_Label_1_all_chunks.csv',\n",
    "                 'RSA.L_Label_1_all_chunks.csv',\n",
    "                 'ITV.L_Label_1_all_chunks.csv',\n",
    "                 'TSCO.L_Label_1_all_chunks.csv',\n",
    "                 'CCL.L_Label_1_all_chunks.csv',\n",
    "                 'AV.L_Label_1_all_chunks.csv',\n",
    "                'BLT.L_Label_1_all_chunks.csv',\n",
    "                'AZN.L_Label_1_all_chunks.csv',\n",
    "                'KGF.L_Label_1_all_chunks.csv',\n",
    "                'CPG.L_Label_1_all_chunks.csv',\n",
    "                'RDSa.L_Label_1_all_chunks.csv',\n",
    "                'REL.L_Label_1_all_chunks.csv',\n",
    "                'LLOY.L_Label_1_all_chunks.csv', \n",
    "                 'ULVR.L_Label_1_all_chunks.csv'\n",
    "                ]\n",
    "\n",
    "label_2_files = [ 'APF.L_Label_2_all_chunks.csv', \n",
    "                 'AV.L_Label_2_all_chunks.csv',\n",
    "                 'BARC.L_Label_2_all_chunks.csv'\n",
    "                'BLT.L_Label_2_all_chunks.csv', \n",
    "                 'KGF.L_Label_2_all_chunks.csv',\n",
    "                 'CCL.L_Label_2_all_chunks.csv',\n",
    "                'CEY.L_Label_2_all_chunks.csv',\n",
    "                 'CPG.L_Label_2_all_chunks.csv',\n",
    "                'ITV.L_Label_2_all_chunks.csv',\n",
    "                 'APF.L_Label_2_all_chunks.csv' ,\n",
    "                'LAND.L_Label_2_all_chunks.csv',\n",
    "                'LGEN.L_Label_2_all_chunks.csv',\n",
    "                 'MAB.L_Label_2_all_chunks.csv',\n",
    "                  'MKS.L_Label_2_all_chunks.csv',\n",
    "                  'RBS.L_Label_2_all_chunks.csv',\n",
    "                  'REL.L_Label_2_all_chunks.csv',\n",
    "                  'TSCO.L_Label_2_all_chunks.csv',\n",
    "                  'ULVR.L_Label_2_all_chunks.csv',\n",
    "                  'VOD.L_Label_2_all_chunks.csv',\n",
    "                  'SHP.L_Label_2_all_chunks.csv',\n",
    "                 'SMIN.L_Label_2_all_chunks.csv',\n",
    "                 'SPT.L_Label_2_all_chunks.csv'\n",
    "                ]\n",
    "\n",
    "label_3_files =[  'APF.L_Label_3_all_chunks.csv',\n",
    "                'AV.L_Label_3_all_chunks.csv',\n",
    "                'BLT.L_Label_3_all_chunks.csv',\n",
    "                'CCL.L_Label_3_all_chunks.csv',\n",
    "                'CEY.L_Label_3_all_chunks.csv',\n",
    "                'CPG.L_Label_3_all_chunks.csv',\n",
    "                'ITV.L_Label_3_all_chunks.csv',                \n",
    "                'LAND.L_Label_3_all_chunks.csv',\n",
    "               'LLOY.L_Label_3_all_chunks.csv',\n",
    "                'LGEN.L_Label_3_all_chunks.csv',\n",
    "                'MAB.L_Label_3_all_chunks.csv',\n",
    "                'MKS.L_Label_3_all_chunks.csv',\n",
    "               'RBS.L_Label_3_all_chunks.csv',\n",
    "               'PRU.L_Label_3_all_chunks.csv',\n",
    "                'PSON.L_Label_3_all_chunks.csv',\n",
    "                'LGEN.L_Label_3_all_chunks.csv',\n",
    "                'REL.L_Label_3_all_chunks.csv',\n",
    "                'RR.L_Label_3_all_chunks.csv',\n",
    "                'SHP.L_Label_3_all_chunks.csv',\n",
    "                'SMIN.L_Label_3_all_chunks.csv',\n",
    "                'SPT.L_Label_3_all_chunks.csv',\n",
    "                'ULVR.L_Label_3_all_chunks.csv',\n",
    "                'TSCO.L_Label_3_all_chunks.csv'\n",
    "                \n",
    "               ]\n",
    "label_4_files =  ['APF.L_Label_4_all_chunks.csv',\n",
    "                  'AZN.L_Label_4_all_chunks.csv',\n",
    " 'AV.L_Label_4_all_chunks.csv',\n",
    "  'BLT.L_Label_4_all_chunks.csv',\n",
    "'BARC.L_Label_4_all_chunks.csv',\n",
    "'SHP.L_Label_4_all_chunks.csv',\n",
    " 'RSA.L_Label_4_all_chunks.csv',\n",
    "'CCL.L_Label_4_all_chunks.csv',\n",
    "'CEY.L_Label_4_all_chunks.csv',\n",
    "'CPG.L_Label_4_all_chunks.csv',\n",
    "'ITV.L_Label_4_all_chunks.csv',\n",
    "'KGF.L_Label_4_all_chunks.csv', \n",
    "'LAND.L_Label_4_all_chunks.csv', \n",
    "'LGEN.L_Label_4_all_chunks.csv',\n",
    "'LLOY.L_Label_4_all_chunks.csv', \n",
    "'MAB.L_Label_4_all_chunks.csv',\n",
    "'MKS.L_Label_4_all_chunks.csv',\n",
    "'NG.L_Label_4_all_chunks.csv',                  \n",
    "'PSON.L_Label_4_all_chunks.csv',\n",
    "'PRU.L_Label_4_all_chunks.csv',\n",
    "'REL.L_Label_4_all_chunks.csv', \n",
    "'RB.L_Label_4_all_chunks.csv',\n",
    "'RBS.L_Label_4_all_chunks.csv',\n",
    "'RDSa.L_Label_4_all_chunks.csv',\n",
    "'RDSb.L_Label_4_all_chunks.csv',\n",
    "'RTO.L_Label_4_all_chunks.csv',\n",
    "'RR.L_Label_4_all_chunks.csv',\n",
    "'SDR.L_Label_4_all_chunks.csv',  \n",
    "'SGE.L_Label_4_all_chunks.csv', \n",
    "'SHP.L_Label_4_all_chunks.csv', \n",
    "'SMIN.L_Label_4_all_chunks.csv', \n",
    "'SPT.L_Label_4_all_chunks.csv', \n",
    "'STAN.L_Label_4_all_chunks.csv', \n",
    "'TSCO.L_Label_4_all_chunks.csv', \n",
    "'ULVR.L_Label_4_all_chunks.csv', \n",
    "'UU.L_Label_4_all_chunks.csv', \n",
    "'VOD.L_Label_4_all_chunks.csv', \n",
    "'WPP.L_Label_4_all_chunks.csv', \n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "label_5_files = ['AV.L_Label_5_all_chunks.csv', \n",
    "                 'APF.L_Label_5_all_chunks.csv',\n",
    "                 'BLT.L_Label_5_all_chunks.csv', \n",
    "                  'CPG.L_Label_5_all_chunks.csv', \n",
    "                'ITV.L_Label_5_all_chunks.csv',                             \n",
    "                 'KGF.L_Label_5_all_chunks.csv', \n",
    "                'LAND.L_Label_5_all_chunks.csv',\n",
    "                 'LGEN.L_Label_5_all_chunks.csv',\n",
    "                 'MAB.L_Label_5_all_chunks.csv',                \n",
    "                 'MKS.L_Label_5_all_chunks.csv',\n",
    "                'PSON.L_Label_5_all_chunks.csv', \n",
    "                 'PRU.L_Label_5_all_chunks.csv',\n",
    "                 'RR.L_Label_5_all_chunks.csv',\n",
    "                'RSA.L_Label_5_all_chunks.csv',\n",
    "                'RBS.L_Label_5_all_chunks.csv',\n",
    "                'RDSa.L_Label_5_all_chunks.csv',\n",
    "                 'REL.L_Label_5_all_chunks.csv',\n",
    "                'SGE.L_Label_5_all_chunks.csv',\n",
    "                 'SHP.L_Label_5_all_chunks.csv',\n",
    "                'SMIN.L_Label_5_all_chunks.csv',\n",
    "                'SDR.L_Label_5_all_chunks.csv', \n",
    "                'SPT.L_Label_5_all_chunks.csv',\n",
    "                'UU.L_Label_5_all_chunks.csv',\n",
    "                 'VOD.L_Label_5_all_chunks.csv'\n",
    "                ]\n",
    "\n",
    "label_6_files = ['APF.L_Label_6_all_chunks.csv',\n",
    "                 'CPG.L_Label_6_all_chunks.csv',\n",
    "    'RSA.L_Label_6_all_chunks.csv',\n",
    "                'WPP.L_Label_6_all_chunks.csv',\n",
    "                'MKS.L_Label_6_all_chunks.csv',\n",
    "                'LAND.L_Label_6_all_chunks.csv',\n",
    "                 'ITV.L_Label_6_all_chunks.csv',\n",
    "                 'REL.L_Label_6_all_chunks.csv',\n",
    "                 'RR.L_Label_6_all_chunks.csv',\n",
    "                 'RSA.L_Label_6_all_chunks.csv',\n",
    "                'CEY.L_Label_6_all_chunks.csv',\n",
    "                 'CCL.L_Label_6_all_chunks.csv',\n",
    "                 'KGF.L_Label_6_all_chunks.csv',\n",
    "                 'PSON.L_Label_6_all_chunks.csv',\n",
    "                 'SPT.L_Label_6_all_chunks.csv',\n",
    "                 'STAN.L_Label_6_all_chunks.csv',  \n",
    "                 'VOD.L__Label_6_all_chunks.csv'\n",
    "                ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label_6_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.median(axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dfs_across_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(location_for_df, index_col=0)\n",
    "df_test.median(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to do:\n",
    "1. write a loop to go through all the labels for each symbol and store them in a seperate file\n",
    "2. concentrate only on the symbols we know we have labels\n",
    "3. pull the file from Single Kernel and compare them.\n",
    "4. re-do the labels and the plots for the symbols that you are picking and describe them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row_labels = ['accuracy','precision','recall','f1- weighted','f1- micro','f1- macro',\n",
    "#               'Hamming Loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path\n",
    "def create_path(parent_dir, symbol):\n",
    "    path = os.path.join(parent_dir, symbol)\n",
    "    if os.path.isdir(path)==False:\n",
    "        # Create the directory\n",
    "    \n",
    "        os.mkdir(path)\n",
    "        print(\"Directory '%s' created\" %path)\n",
    "    else:\n",
    "        print(\"Directory Exist\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_to_load = os.listdir(processed_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files = list()\n",
    "string = str('_Label_1')\n",
    "for idx in range(0,40):\n",
    "    files =os.listdir(os.path.join(processed_location, symbols_to_load[idx]))\n",
    "    list_files.append([f for f in files if string in f])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more efficient way to do this\n",
    "label_1_dfs = list()\n",
    "label_1_dicts = dict()\n",
    "\n",
    "for idx in range(0, len(label_1_files)):\n",
    "    symbol = label_1_files[idx].split(\"_\")[0]\n",
    "    try:\n",
    "        file_to_load = \\\n",
    "        pd.read_csv(os.path.join(processed_location, symbol, label_1_files[idx])).median(axis=1)\n",
    "        label_1_dfs.append(file_to_load)\n",
    "        file_to_load.index = df_test.index.values #row_labels\n",
    "        label_1_dicts[symbol] = file_to_load\n",
    "    except (ValueError, TypeError,PermissionError, EOFError, IndexError, FileNotFoundError, OSError, RuntimeError):\n",
    "        \n",
    "        continue\n",
    "# more efficient way to do this\n",
    "label_2_dfs = list()\n",
    "label_2_dicts = dict()\n",
    "\n",
    "for idx in range(0, len(label_2_files)):\n",
    "    symbol = label_2_files[idx].split(\"_\")[0]\n",
    "    try:\n",
    "        file_to_load = \\\n",
    "        pd.read_csv(os.path.join(processed_location, symbol, label_2_files[idx])).median(axis=1)\n",
    "        label_2_dfs.append(file_to_load)\n",
    "        file_to_load.index = df_test.index.values\n",
    "        label_2_dicts[symbol] = file_to_load\n",
    "    \n",
    "    except (ValueError, TypeError,PermissionError, EOFError, IndexError, FileNotFoundError, OSError, RuntimeError):\n",
    "        \n",
    "        continue\n",
    "# mo\n",
    "# more efficient way to do this\n",
    "label_3_dfs = list()\n",
    "label_3_dicts = dict()\n",
    "\n",
    "for idx in range(0, len(label_3_files)):\n",
    "    symbol = label_3_files[idx].split(\"_\")[0]\n",
    "    try:\n",
    "    \n",
    "        file_to_load = \\\n",
    "        pd.read_csv(os.path.join(processed_location, symbol, label_3_files[idx])).median(axis=1)\n",
    "        label_3_dfs.append(file_to_load)\n",
    "        file_to_load.index = df_test.index.values#     file_to_load.index = row_labels\n",
    "        label_3_dicts[symbol] = file_to_load\n",
    "    except (ValueError, TypeError,PermissionError, EOFError, IndexError, FileNotFoundError, OSError, RuntimeError):\n",
    "\n",
    "        continue\n",
    "    \n",
    "# more efficient way to do this\n",
    "label_4_dfs = list()\n",
    "label_4_dicts = dict()\n",
    "\n",
    "for idx in range(0, len(label_4_files)):\n",
    "    symbol = label_4_files[idx].split(\"_\")[0]\n",
    "    try:\n",
    "        file_to_load = \\\n",
    "        pd.read_csv(os.path.join(processed_location, symbol, label_4_files[idx])).median(axis=1)\n",
    "        label_4_dfs.append(file_to_load)\n",
    "        file_to_load.index = df_test.index.values\n",
    "        label_4_dicts[symbol] = file_to_load\n",
    "    except (ValueError, TypeError,PermissionError, EOFError, IndexError, FileNotFoundError, OSError, RuntimeError):\n",
    "        continue\n",
    "# more efficient way to do this\n",
    "label_5_dfs = list()\n",
    "label_5_dicts = dict()\n",
    "\n",
    "for idx in range(0, len(label_5_files)):\n",
    "    symbol = label_5_files[idx].split(\"_\")[0]\n",
    "    try:\n",
    "    \n",
    "        file_to_load = \\\n",
    "        pd.read_csv(os.path.join(processed_location, symbol, label_5_files[idx])).median(axis=1)\n",
    "        label_5_dfs.append(file_to_load)\n",
    "        file_to_load.index = df_test.index.values\n",
    "        label_5_dicts[symbol] = file_to_load\n",
    "    except (ValueError, TypeError,PermissionError, EOFError, IndexError, FileNotFoundError, OSError, RuntimeError):\n",
    "\n",
    "        continue\n",
    "    \n",
    "\n",
    "# more efficient way to do this\n",
    "label_6_dfs = list()\n",
    "label_6_dicts = dict()\n",
    "\n",
    "for idx in range(0, len(label_6_files)):\n",
    "    symbol = label_6_files[idx].split(\"_\")[0]\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        file_to_load = \\\n",
    "        pd.read_csv(os.path.join(processed_location, symbol, label_6_files[idx])).median(axis=1)\n",
    "        label_6_dfs.append(file_to_load)\n",
    "        file_to_load.index = df_test.index.values\n",
    "        label_6_dicts[symbol] = file_to_load\n",
    "    except (ValueError, TypeError,PermissionError, EOFError, IndexError, FileNotFoundError, OSError, RuntimeError):\n",
    "\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(label_1_dicts, axis =1).median(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(label_1_dfs, axis =1).median(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(label_5_dfs, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(label_3_dfs, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "label_dfs = {'1':label_1_dfs,\n",
    "             '2':label_2_dfs,\n",
    "             '3':label_3_dfs,\n",
    "             '4':label_4_dfs,\n",
    "             '5':label_5_dfs,\n",
    "             '6':label_6_dfs\n",
    "            }\n",
    "\n",
    "label_dicts = {'1':label_1_dicts,\n",
    "             '2':label_2_dicts,\n",
    "             '3':label_3_dicts,\n",
    "             '4':label_4_dicts,\n",
    "             '5':label_5_dicts,\n",
    "             '6':label_6_dicts\n",
    "            }\n",
    "\n",
    "label_files ={'1':label_1_files,\n",
    "              '2':label_2_files,\n",
    "              '3':label_3_files,\n",
    "              '4':label_4_files,\n",
    "              '5':label_5_files,\n",
    "              '6':label_6_files\n",
    "             }\n",
    "\n",
    "labelIds = list(label_dfs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_dfs_across_all_symbols = dict()\n",
    "for idx in labelIds:\n",
    "    median_dfs_across_all_symbols['Label_'+str(idx)] = pd.DataFrame(label_dfs[str(idx)]).T.median(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame.from_dict(median_dfs_across_all_symbols).T.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIdx = str(1)\n",
    "label_Idx_files = label_files[labelIdx]\n",
    "label_Idx_dfs = label_dfs[labelIdx]\n",
    "label_Idx_dicts = label_dicts[labelIdx]\n",
    "\n",
    "\n",
    "\n",
    "for idx in range(0, len(label_Idx_files)):\n",
    "    symbol = label_Idx_files[idx].split(\"_\")[0]\n",
    "    \n",
    "    file_to_load = \\\n",
    "    pd.read_csv(os.path.join(processed_location, symbol, label_Idx_files[idx])).median(axis=1)\n",
    "    label_Idx_dfs.append(file_to_load)\n",
    "    file_to_load.index = df_test.index.values\n",
    "    label_Idx_dicts[symbol] = file_to_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(label_dicts['4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dicts_processed = {'One': pd.DataFrame.from_dict(label_dicts['1']),\n",
    "               'Two':  pd.DataFrame.from_dict(label_dicts['2']),\n",
    "               'Three':  pd.DataFrame.from_dict(label_dicts['3']),\n",
    "               'Four':  pd.DataFrame.from_dict(label_dicts['4']),\n",
    "               'Five':  pd.DataFrame.from_dict(label_dicts['5']),\n",
    "               'Six':  pd.DataFrame.from_dict(label_dicts['6']),}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_for_label_one = list(labels_dicts_processed['One'].columns.values)\n",
    "symbols_for_label_two = list(labels_dicts_processed['Two'].columns.values)\n",
    "symbols_for_label_three = list(labels_dicts_processed['Three'].columns.values)\n",
    "symbols_for_label_four = list(labels_dicts_processed['Four'].columns.values)\n",
    "symbols_for_label_five = list(labels_dicts_processed['Five'].columns.values)\n",
    "symbols_for_label_six = list(labels_dicts_processed['Six'].columns.values)\n",
    "\n",
    "common_symbols = set(symbols_for_label_one)  - set(symbols_for_label_five) - \\\n",
    "set(symbols_for_label_four)-set(symbols_for_label_three)- set(symbols_for_label_two)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_list =list(common_symbols)\n",
    "\n",
    "#labels_dicts_processed['One'][common_list].median(axis=1)\n",
    "# labels_dicts_processed['Three'][common_list].median(axis=1)\n",
    "common_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for key in list(labels_dicts_processed.keys()):\n",
    "    df[key] =labels_dicts_processed[str(key)].median(axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
