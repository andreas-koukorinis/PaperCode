{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import fileutils as fileutils\n",
    "import gc\n",
    "import psutil\n",
    "import pickle\n",
    "import fnmatch\n",
    "import os\n",
    "import clfutils\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, recall_score, precision_score\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "dataDrive = '/media/ak/My Passport/Data/FinDataReal/'  # also labels location folder\n",
    "jointLocationsPickleFolder = os.path.join(dataDrive, 'JointLocationsDicts')\n",
    "extPath = '/media/ak/My Passport/ExperimentData'\n",
    "featuresPath = \"/\".join((extPath, 'features'))  # path with features\n",
    "\n",
    "# Labels\n",
    "labels_location_folder = fileutils.data_path  # this is the folder where all the labels are saved\n",
    "\n",
    "labels_pickle_files = [s for s in os.listdir(labels_location_folder) if ('LabelsAlternate') in s if\n",
    "                       not ('.pkl') in s]  # these are all the dicts that we have alternate labels for.\n",
    "# labels_pickle_files: these are all the dicts that we have alternate labels for.\n",
    "\n",
    "symbols = [f for f in [s for s in os.listdir(labels_location_folder) if '.L' in s if '_Features' not in s] if\n",
    "           ('.L_A' or '_Features') not in f]  # from all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logmemoryusage(msg):\n",
    "    # function to log memory usage\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print('Memory usage at %s is %smb.' % (msg, process.memory_info().rss / 1000 / 1000))\n",
    "\n",
    "\n",
    "def unpickle_csv(pickled_csv):\n",
    "    with open(pickled_csv, 'rb') as f:\n",
    "        u = pickle._Unpickler(f)\n",
    "        u.encoding = 'latin1'\n",
    "        p = u.load()\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "def ticker_features_labels(file_joint_locations):\n",
    "    # input a joint location file that contains both features and labels and returns one\n",
    "    labels = pd.read_csv(file_joint_locations[1])\n",
    "    features = unpickle_csv(file_joint_locations[0])\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def hmm_features_df(features_tuple):\n",
    "    return pd.concat([features_tuple[0], features_tuple[1], \\\n",
    "                      features_tuple[2], features_tuple[3]], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AlternateLabelFeaturesLoader(object):\n",
    "    \"\"\"\n",
    "    takes in a main path, a symbol, an alternate label index (from 0 to 4) and returns, the pickled dict file name\n",
    "    and path for the common locations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path_main, symbol, alternate_label_idx=None,\n",
    "                 jointLocationsPickleInput=jointLocationsPickleFolder):\n",
    "        self.main_path = path_main\n",
    "        self.symbol = symbol\n",
    "        self.LabelsAlternateName = ['LabelsAlternateFive', 'LabelsAlternateFour', 'LabelsAlternateOne',\n",
    "                                    'LabelsAlternateThree', 'LabelsAlternateTwo']\n",
    "        self.alternate_label_idx = alternate_label_idx\n",
    "        self.jointLocationsPickleFolder = jointLocationsPickleInput\n",
    "        self.jointLocsSymbols = list(np.unique([f.split(\"_\")[0] for f in os.listdir(self.jointLocationsPickleFolder)]))\n",
    "\n",
    "    def return_pickled_dict(self):\n",
    "        # returns the filename of the joint features and labels file\n",
    "        # the features file is a dictionary that has keys\n",
    "#         if self.symbol in self.jointLocsSymbols:\n",
    "        if self.alternate_label_idx < 4:\n",
    "            pickle_in_filename_local = os.path.join(self.jointLocationsPickleFolder, \"_\".join(\n",
    "                (self.symbol, self.LabelsAlternateName[self.alternate_label_idx], 'FeaturesLocations.pkl')))\n",
    "        else:\n",
    "            print('Error in the alternate label index: value between 0 and 4')\n",
    "#         else:\n",
    "#             print('Symbol is not in the folder')\n",
    "        return pickle_in_filename_local\n",
    "\n",
    "    @staticmethod\n",
    "    def load_pickled_in_filename(file):\n",
    "        # load a simple pickled file and return it. its a bit different to the method used for the dictionary as this\n",
    "        # is pure Python 3.x\n",
    "        pickle_in = open(file, 'rb')\n",
    "        return pickle.load(pickle_in)\n",
    "\n",
    "    @staticmethod\n",
    "    def joint_loc_pickle_keys(inputFile):\n",
    "        # returns keys of joint locations from labels and features\n",
    "        return list(AlternateLabelFeaturesLoader.load_pickled_in_filename(inputFile).keys())\n",
    "\n",
    "    @staticmethod\n",
    "    def forwardDates(list_of_keys, current_date):\n",
    "        \"\"\" return all the forward looking dates for each idxKey we use for training\"\"\"\n",
    "        lookAheadKeys = sorted(i for i in list_of_keys if i > current_date)\n",
    "        return lookAheadKeys\n",
    "\n",
    "\n",
    "class CreateMarketFeatures(object):\n",
    "    # a class to be expanded that uses features for base case -market based only-indicators/features\n",
    "    \"\"\"\"Requires:\n",
    "    a dataframe that has TradedPrice And Volume columns\n",
    "    symbol - A stock symbol on which to form a strategy on.\n",
    "    short_window - Lookback period for short moving average.\n",
    "    long_window - Lookback period for long moving average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        #         self.ticker = ticker\n",
    "        self.df = df\n",
    "\n",
    "    def load_data(self):\n",
    "        pass\n",
    "\n",
    "    def ma_spread(self, short_window=3, long_window=6):\n",
    "        # function that produces the MA spread, which can be used on its own or as an input for MACD\n",
    "        short_rolling_px = self.df['TradedPrice'].rolling(window=short_window).mean()\n",
    "        long_rolling_px = self.df['TradedPrice'].rolling(window=long_window).mean()\n",
    "        px_name = \"_\".join(('px_indx', str(short_window), str(long_window)))\n",
    "        self.df[px_name] = long_rolling_px - short_rolling_px\n",
    "        return self.df\n",
    "\n",
    "    def ma_spread_duration(self, short_window=2, long_window=4):\n",
    "        # function that produces the MA spread, which can be used on its own or as an input for MACD\n",
    "        short_rolling_px = self.df['Duration'].rolling(window=short_window).mean()\n",
    "        long_rolling_px = self.df['Duration'].rolling(window=long_window).mean()\n",
    "        dur_name = \"_\".join(('dur_indx', str(short_window), str(long_window)))\n",
    "        self.df[dur_name] = long_rolling_px - short_rolling_px\n",
    "        return self.df\n",
    "\n",
    "    def obv_calc(self):\n",
    "        # on balance volume indicator\n",
    "        self.df['SignedVolume'] = self.df['Volume'] * np.sign(self.df['TradedPrice'].diff()).cumsum()\n",
    "        self.df['SignedVolume'].iat[1] = 0\n",
    "        self.df['OBV'] = self.df['SignedVolume']  # .cumsum()\n",
    "        self.df = self.df.drop(columns=['SignedVolume'])\n",
    "        return self.df\n",
    "\n",
    "    def chaikin_mf(self, period=3):\n",
    "        # Chaikin money flow indicator\n",
    "        self.df[\"MF Multiplier\"] = (self.df['TradedPrice'] - (self.df['TradedPrice'].expanding(period).min()) \\\n",
    "                                    - (self.df['TradedPrice'].expanding(period).max() \\\n",
    "                                       - self.df['TradedPrice'])) / (\n",
    "                                           self.df['TradedPrice'].expanding(period).max() - self.df[ \\\n",
    "                                       'TradedPrice'].expanding(period).min())\n",
    "        self.df[\"MF Volume\"] = self.df['MF Multiplier'] * self.df['Volume']\n",
    "        self.df['CMF_' + str(period)] = self.df['MF Volume'].sum() / self.df[\"Volume\"].rolling(period).sum()\n",
    "        self.df = self.df.drop(columns=['MF Multiplier', 'MF Volume'])\n",
    "        return self.df\n",
    "\n",
    "\n",
    "#         # forward_dates_keys = data_cls.forwardDates(joint_keys, joint_keys[joint_key_idx])  # forward dates for this date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LabelsAlternateOne'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just pick symbols I have joint locations\n",
    "jointLocsSymbols = list(np.unique([f.split(\"_\")[0] for f in os.listdir(jointLocationsPickleFolder)]))\n",
    "symbol= 'AAL.L'\n",
    "alternate_label_idx = 2\n",
    "symbol_idx = jointLocsSymbols.index(symbol)\n",
    "jointLocsSymbols[symbol_idx]\n",
    "labels_pickle_files[alternate_label_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit code per symbol - see if this works! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok to go\n",
      "AAL.L  and labels  LabelsAlternateOne\n",
      "Memory usage at Before garbage collect is 147.570688mb.\n",
      "Memory usage at Before feature creation is 147.570688mb.\n",
      "20170117\n",
      "Fitting 11 folds for each of 32 candidates, totalling 352 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:  1.1min\n"
     ]
    }
   ],
   "source": [
    "symbol = 'AAL.L'\n",
    "best_svc_dict =defaultdict(dict)\n",
    "if symbol in jointLocsSymbols:\n",
    "    print('ok to go')\n",
    "    alternate_label_idx = 2  # pick a label indexprint(jointLocsSymbols[symbol_idx], ' and labels ', labels_pickle_files[alternate_label_idx])\n",
    "    symbol_idx = jointLocsSymbols.index(symbol) # dont particularly need this!\n",
    "    print(symbol, ' and labels ', labels_pickle_files[alternate_label_idx])\n",
    "    data_cls = AlternateLabelFeaturesLoader(path_main=dataDrive, symbol=jointLocsSymbols[symbol_idx],\n",
    "                                            alternate_label_idx=alternate_label_idx,\n",
    "                                            jointLocationsPickleInput=jointLocationsPickleFolder)\n",
    "    jointLocationsDictionary = (data_cls.load_pickled_in_filename(data_cls.return_pickled_dict()))\n",
    "    joint_keys = data_cls.joint_loc_pickle_keys(data_cls.return_pickled_dict())\n",
    "    logmemoryusage(\"Before garbage collect\")\n",
    "    gc.collect()  # continue\n",
    "    for joint_key_idx, joint_key_date in enumerate(joint_keys):\n",
    "        # this is a date - and we will enumerate through the keys\n",
    "        # getting features and labels\n",
    "        logmemoryusage(\"Before feature creation\")\n",
    "        features, labels = ticker_features_labels(jointLocationsDictionary[joint_keys[joint_key_idx]])\n",
    "        print(joint_key_date)\n",
    "        label_name = str(labels.columns[labels.columns.str.contains(pat='label')].values[0])\n",
    "        features_df = hmm_features_df(features)  # features data-frame - this just unbundles the features into a dataframe\n",
    "        # lets get all the features in order now#\n",
    "        market_features_df = CreateMarketFeatures(\n",
    "                CreateMarketFeatures(CreateMarketFeatures(df=CreateMarketFeatures(df=labels).ma_spread_duration())\n",
    "                                     .ma_spread()).chaikin_mf()).obv_calc()  # market features dataframe\n",
    "        df_concat = pd.DataFrame(pd.concat([features_df, market_features_df], axis=1, sort='False').dropna())\n",
    "        df = df_concat[df_concat[label_name].notna()]\n",
    "        df_final = df.drop(columns=['TradedPrice', 'Duration', 'TradedTime', 'ReturnTradedPrice', \\\n",
    "                                                           'Volume', label_name])\n",
    "        y_labels_train = df[df.columns[df.columns.str.contains(pat='label')]].iloc[:, 0]\n",
    "        if df_final.shape[0] < 10:\n",
    "            print(' the ratio of classes is too low. try another label permutation')\n",
    "            continue\n",
    "        else:\n",
    "            X_train = MinMaxScaler().fit_transform(df_final)\n",
    "            models_cls =  clfutils.FitModels(X_train, y_labels_train)\n",
    "            best_svc_dict[symbol][joint_key_date] = {'SVC': models_cls.best_svc_clf()}\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clfs['SVC'].fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clfs['RF_clf'].fit(X_train, y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_cls.best_gradient_boost_clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_cls.best_gradient_boost_clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_candidates = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classifier object with the classifier and parameter candidates\n",
    "clf = GridSearchCV(estimator=SVC(), param_grid=parameter_candidates, n_jobs=-1)\n",
    "\n",
    "# Train the classifier on data1's feature and target data\n",
    "clf.fit(X_train, y_labels)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clfs.['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
