{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Compare the effect of different scalers on data with outliers\n",
    "\n",
    "\n",
    "Feature 0 (median income in a block) and feature 5 (number of households) of\n",
    "the `California housing dataset\n",
    "<https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html>`_ have very\n",
    "different scales and contain some very large outliers. These two\n",
    "characteristics lead to difficulties to visualize the data and, more\n",
    "importantly, they can degrade the predictive performance of many machine\n",
    "learning algorithms. Unscaled data can also slow down or even prevent the\n",
    "convergence of many gradient-based estimators.\n",
    "\n",
    "Indeed many estimators are designed with the assumption that each feature takes\n",
    "values close to zero or more importantly that all features vary on comparable\n",
    "scales. In particular, metric-based and gradient-based estimators often assume\n",
    "approximately standardized data (centered features with unit variances). A\n",
    "notable exception are decision tree-based estimators that are robust to\n",
    "arbitrary scaling of the data.\n",
    "\n",
    "This example uses different scalers, transformers, and normalizers to bring the\n",
    "data within a pre-defined range.\n",
    "\n",
    "Scalers are linear (or more precisely affine) transformers and differ from each\n",
    "other in the way to estimate the parameters used to shift and scale each\n",
    "feature.\n",
    "\n",
    "``QuantileTransformer`` provides non-linear transformations in which distances\n",
    "between marginal outliers and inliers are shrunk. ``PowerTransformer`` provides\n",
    "non-linear transformations in which data is mapped to a normal distribution to\n",
    "stabilize variance and minimize skewness.\n",
    "\n",
    "Unlike the previous transformations, normalization refers to a per sample\n",
    "transformation instead of a per feature transformation.\n",
    "\n",
    "The following code is a bit verbose, feel free to jump directly to the analysis\n",
    "of the results_.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Exterrnal Files\n"
     ]
    }
   ],
   "source": [
    "# Author:  Raghav RV <rvraghav93@gmail.com>\n",
    "#          Guillaume Lemaitre <g.lemaitre58@gmail.com>\n",
    "#          Thomas Unterthiner\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "print(__doc__)\n",
    "def open_pickle_file(path, pickle_file):\n",
    "    file_loc = os.path.join(path, pickle_file)\n",
    "    pickle_to_file = pickle.load(open(file_loc, \"rb\"))\n",
    "    return pickle_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = fetch_california_housing()\n",
    "# X_full, y_full = dataset.data, dataset.target\n",
    "\n",
    "# # Take only 2 features to make visualization easier\n",
    "# # Feature of 0 has a long tail distribution.\n",
    "# # Feature 5 has a few but very large outliers.\n",
    "\n",
    "# X = X_full[:, [0, 5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "##locations ##\n",
    "dataOnlyDrive = ('/media/ak/WorkDrive/Data')\n",
    "''' Exterrnal Files'''\n",
    "extPath = '/media/ak/My Passport/Experiment Data'\n",
    "barketData = '/media/ak/My Passport/Barket Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetDrive = barketData\n",
    "bmrg_folders=[s for s in os.listdir(targetDrive ) if s.endswith('Comdty')]\n",
    "bmrg_trades=sorted([s for s in os.listdir(targetDrive ) if s.endswith('y_trades')])\n",
    "bmrg_quotes=sorted([s for s in os.listdir(targetDrive ) if s.endswith('y_quotes')])\n",
    "bmrg_tickers=sorted([bmrg_trades[idx].split('_t')[0] for idx,_ in enumerate(bmrg_trades)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbolIdx= 0\n",
    "# get dates and files\n",
    "symbol_quotes = os.path.join(targetDrive,str(bmrg_quotes[symbolIdx]))\n",
    "symbol_trades = os.path.join(targetDrive,str(bmrg_trades[symbolIdx]))\n",
    "symbolQuoteDates = [quoteFile.split(\".\")[0] for quoteFile in os.listdir(symbol_quotes)] \n",
    "symbolTradeDates = [tradeFile.split(\".\")[0] for tradeFile in os.listdir(symbol_trades)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAllQuotesName = \"\".join(('AllQuotes',bmrg_quotes[symbolIdx],'Comdty.pkl'))\n",
    "dfAllTradesName = \"\".join(('AllTrades',bmrg_trades[symbolIdx],'Comdty.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dQuoteKeys = list(allQuotesDict.keys()) #one common set of keys at the moment\n",
    "dTradeKeys = list(allTradesDict.keys())\n",
    "commonDates =list(set(dQuoteKeys).intersection(set(dTradeKeys)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try and load the below just once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.isfile(\"/\".join((symbol_quotes, dfAllQuotesName)))\n",
    "allQuotesDict =open_pickle_file(symbol_quotes, dfAllQuotesName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTradesDict = open_pickle_file(symbol_trades, dfAllTradesName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# list of common keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dQuoteKeys = list(allQuotesDict.keys()) #one common set of keys at the moment\n",
    "dTradeKeys = list(allTradesDict.keys())\n",
    "commonDates =list(set(dQuoteKeys).intersection(set(dTradeKeys)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dKeys= commonDates\n",
    "# for idx, dateKey in enumerate(commonDates):\n",
    "def reshapeDF(idx, allQuotesDic, AllTradesDict):\n",
    "    allQuotesDict[dKeys[idx]]['TimeStamp']= pd.to_datetime(allQuotesDict[dKeys[idx]]['time'])\n",
    "    allTradesDict[dKeys[idx]]['TradeTimeStamp']= pd.to_datetime(allTradesDict[dKeys[idx]]['time'])\n",
    "    allTradesDict[dQuoteKeys[idx]]['TradedPrice']= allTradesDict[dKeys[idx]]['value']\n",
    "    allTradesDict[dKeys[idx]]['TradedSize']= allTradesDict[dKeys[idx]]['size']\n",
    "    allTradesDict[dKeys[idx]]['Duration']=allTradesDict[dKeys[idx]].TradeTimeStamp.diff()/np.timedelta64(1, 'ms')\n",
    "    allTradesDict[dKeys[idx]].rename(columns = {'type':'QuotedSide','value':'bestPrice','size':'QuoteSize','time':'QuoteTimeStamp'}, inplace = True) \n",
    "    allQuotesDict[dKeys[idx]].reset_index(level=0, inplace=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshapeDF(4,allQuotesDict,allTradesDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dKeys= commonDates\n",
    "# for idx, dateKey in enumerate(commonDates):\n",
    "#     #dfAllQuotes[dKeys[idx]].reset_index(level=0, inplace=True)\n",
    "#     allQuotesDict[dKeys[idx]]['TimeStamp']= pd.to_datetime(allQuotesDict[dKeys[idx]]['time'])\n",
    "# #     allTradesDict[dKeys[idx]]['TradeTimeStamp']= pd.to_datetime(allTradesDict[dKeys[idx]]['time'])\n",
    "# #     allTradesDict[dKeys[idx]]['TradedPrice']= allTradesDict[dKeys[idx]]['value']\n",
    "# #     allTradesDict[dKeys[idx]]['TradedSize']= allTradesDict[dKeys[idx]]['size']\n",
    "#     allTradesDict[dKeys[idx]].rename(columns = {'type':'QuotedSide','value':'bestPrice','size':'QuoteSize','time':'QuoteTimeStamp'}, inplace = True) \n",
    "#     allTradesDict[dKeys[idx]]['Duration']=allTradesDict[dKeys[idx]].TradeTimeStamp.diff()/np.timedelta64(1, 'ms')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx =4 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, _ in enumerate(commonDates):\n",
    "        allTradesDict[dKeys[idx]] =allTradesDict[dKeys[idx]].rename(index=str).drop(columns=['Unnamed: 0'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allQuotesDict[dKeys[1]].head(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfBID={}\n",
    "dfASK={}\n",
    "dfLOB ={}\n",
    "def LOB(idx):\n",
    "    dfBID[dKeys[idx]] =allQuotesDict[commonDates[idx]][allQuotesDict[commonDates[idx]]['type']=='BID'].rename(columns={'type':'BidSide','value':'bestBidPrice','size':'bestBidSize','time':'TimeStampS'})#.drop(QuoteColumns, inplace=True, axis=1)\n",
    "    dfASK[dKeys[idx]] =allQuotesDict[commonDates[idx]][allQuotesDict[commonDates[idx]]['type']=='ASK'].rename(columns={'type':'AskSide','value':'bestAskPrice','size':'bestAskSize','time':'TimeStampS'})\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 100):\n",
    "    LOB(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=3\n",
    "keys= list(dfASK.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx =1\n",
    "# dfLOB[dKeys[idx]] =dfBID[dKeys[idx]]\n",
    "dfASK[dKeys[idx]] =dfASK[dKeys[idx]].drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfASK[dKeys[idx]].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfBID[dKeys[idx]] =dfBID[dKeys[idx]].drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>bestBidSize</th>\n",
       "      <th>TimeStampS</th>\n",
       "      <th>BidSide</th>\n",
       "      <th>bestBidPrice</th>\n",
       "      <th>TimeStamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>266</td>\n",
       "      <td>2018-08-20 00:00:02</td>\n",
       "      <td>BID</td>\n",
       "      <td>120.296875</td>\n",
       "      <td>2018-08-20 00:00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>270</td>\n",
       "      <td>2018-08-20 00:00:10</td>\n",
       "      <td>BID</td>\n",
       "      <td>120.296875</td>\n",
       "      <td>2018-08-20 00:00:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Unnamed: 0  bestBidSize           TimeStampS BidSide  bestBidPrice  \\\n",
       "2      2           2          266  2018-08-20 00:00:02     BID    120.296875   \n",
       "3      3           3          270  2018-08-20 00:00:10     BID    120.296875   \n",
       "\n",
       "            TimeStamp  \n",
       "2 2018-08-20 00:00:02  \n",
       "3 2018-08-20 00:00:10  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfBID[dKeys[idx]].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "No matching signature found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-c18aa1ac6086>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdfBID\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfASK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_asof\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'TimeStampS'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_exact_matches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Envs/resrPyth3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge_asof\u001b[0;34m(left, right, on, left_on, right_on, left_index, right_index, by, left_by, right_by, suffixes, tolerance, allow_exact_matches, direction)\u001b[0m\n\u001b[1;32m    460\u001b[0m                     \u001b[0mallow_exact_matches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_exact_matches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m                     direction=direction)\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/resrPyth3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m         \u001b[0mjoin_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_join_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         \u001b[0;31m# this is a bit kludgy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/resrPyth3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_join_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m             (left_indexer,\n\u001b[0;32m--> 756\u001b[0;31m              right_indexer) = self._get_join_indexers()\n\u001b[0m\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/resrPyth3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_join_indexers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                         \u001b[0mright_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_exact_matches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1527\u001b[0;31m                         tolerance)\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/join.pyx\u001b[0m in \u001b[0;36mpandas._libs.join.__pyx_fused_cpdef\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: No matching signature found"
     ]
    }
   ],
   "source": [
    "left= dfBID[keys[1]]\n",
    "right = dfASK[keys[1]]\n",
    "pd.merge_asof( left, right, on='TimeStampS', allow_exact_matches=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the output between 0 and 1 for the colorbar\n",
    "y = minmax_scale(y_full)\n",
    "\n",
    "# plasma does not exist in matplotlib < 1.5\n",
    "cmap = getattr(cm, 'plasma_r', cm.hot_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def distributions(X):\n",
    "    \n",
    "    distributions = [\n",
    "        ('Unscaled data', X),\n",
    "        ('Data after standard scaling',\n",
    "            StandardScaler().fit_transform(X)),\n",
    "        ('Data after min-max scaling',\n",
    "            MinMaxScaler().fit_transform(X)),\n",
    "        ('Data after max-abs scaling',\n",
    "            MaxAbsScaler().fit_transform(X)),\n",
    "        ('Data after robust scaling',\n",
    "            RobustScaler(quantile_range=(25, 75)).fit_transform(X)),\n",
    "        ('Data after power transformation (Yeo-Johnson)',\n",
    "         PowerTransformer(method='yeo-johnson').fit_transform(X)),\n",
    "        ('Data after power transformation (Box-Cox)',\n",
    "         PowerTransformer(method='box-cox').fit_transform(X)),\n",
    "        ('Data after quantile transformation (gaussian pdf)',\n",
    "            QuantileTransformer(output_distribution='normal')\n",
    "            .fit_transform(X)),\n",
    "        ('Data after quantile transformation (uniform pdf)',\n",
    "            QuantileTransformer(output_distribution='uniform')\n",
    "            .fit_transform(X)),\n",
    "        ('Data after sample-wise L2 normalizing',\n",
    "            Normalizer().fit_transform(X)),\n",
    "    ]\n",
    "    return distributions\n",
    "\n",
    "\n",
    "\n",
    "def create_axes(title, figsize=(16, 6)):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    # define the axis for the first plot\n",
    "    left, width = 0.1, 0.22\n",
    "    bottom, height = 0.1, 0.7\n",
    "    bottom_h = height + 0.15\n",
    "    left_h = left + width + 0.02\n",
    "\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom_h, width, 0.1]\n",
    "    rect_histy = [left_h, bottom, 0.05, height]\n",
    "\n",
    "    ax_scatter = plt.axes(rect_scatter)\n",
    "    ax_histx = plt.axes(rect_histx)\n",
    "    ax_histy = plt.axes(rect_histy)\n",
    "\n",
    "    # define the axis for the zoomed-in plot\n",
    "    left = width + left + 0.2\n",
    "    left_h = left + width + 0.02\n",
    "\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom_h, width, 0.1]\n",
    "    rect_histy = [left_h, bottom, 0.05, height]\n",
    "\n",
    "    ax_scatter_zoom = plt.axes(rect_scatter)\n",
    "    ax_histx_zoom = plt.axes(rect_histx)\n",
    "    ax_histy_zoom = plt.axes(rect_histy)\n",
    "\n",
    "    # define the axis for the colorbar\n",
    "    left, width = width + left + 0.13, 0.01\n",
    "\n",
    "    rect_colorbar = [left, bottom, width, height]\n",
    "    ax_colorbar = plt.axes(rect_colorbar)\n",
    "\n",
    "    return ((ax_scatter, ax_histy, ax_histx),\n",
    "            (ax_scatter_zoom, ax_histy_zoom, ax_histx_zoom),\n",
    "            ax_colorbar)\n",
    "\n",
    "\n",
    "def plot_distribution(axes, X, y, hist_nbins=50, title=\"\",\n",
    "                      x0_label=\"\", x1_label=\"\"):\n",
    "    ax, hist_X1, hist_X0 = axes\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(x0_label)\n",
    "    ax.set_ylabel(x1_label)\n",
    "\n",
    "    # The scatter plot\n",
    "    colors = cmap(y)\n",
    "    ax.scatter(X[:, 0], X[:, 1], alpha=0.5, marker='o', s=5, lw=0, c=colors)\n",
    "\n",
    "    # Removing the top and the right spine for aesthetics\n",
    "    # make nice axis layout\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    ax.spines['left'].set_position(('outward', 10))\n",
    "    ax.spines['bottom'].set_position(('outward', 10))\n",
    "\n",
    "    # Histogram for axis X1 (feature 5)\n",
    "    hist_X1.set_ylim(ax.get_ylim())\n",
    "    hist_X1.hist(X[:, 1], bins=hist_nbins, orientation='horizontal',\n",
    "                 color='grey', ec='grey')\n",
    "    hist_X1.axis('off')\n",
    "\n",
    "    # Histogram for axis X0 (feature 0)\n",
    "    hist_X0.set_xlim(ax.get_xlim())\n",
    "    hist_X0.hist(X[:, 0], bins=hist_nbins, orientation='vertical',\n",
    "                 color='grey', ec='grey')\n",
    "    hist_X0.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two plots will be shown for each scaler/normalizer/transformer. The left\n",
    "figure will show a scatter plot of the full data set while the right figure\n",
    "will exclude the extreme values considering only 99 % of the data set,\n",
    "excluding marginal outliers. In addition, the marginal distributions for each\n",
    "feature will be shown on the side of the scatter plot.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(item_idx):\n",
    "    title, X = distributions[item_idx]\n",
    "    ax_zoom_out, ax_zoom_in, ax_colorbar = create_axes(title)\n",
    "    axarr = (ax_zoom_out, ax_zoom_in)\n",
    "    plot_distribution(axarr[0], X, y, hist_nbins=200,\n",
    "                      x0_label=\"Median Income\",\n",
    "                      x1_label=\"Number of households\",\n",
    "                      title=\"Full data\")\n",
    "\n",
    "    # zoom-in\n",
    "    zoom_in_percentile_range = (0, 99)\n",
    "    cutoffs_X0 = np.percentile(X[:, 0], zoom_in_percentile_range)\n",
    "    cutoffs_X1 = np.percentile(X[:, 1], zoom_in_percentile_range)\n",
    "\n",
    "    non_outliers_mask = (\n",
    "        np.all(X > [cutoffs_X0[0], cutoffs_X1[0]], axis=1) &\n",
    "        np.all(X < [cutoffs_X0[1], cutoffs_X1[1]], axis=1))\n",
    "    plot_distribution(axarr[1], X[non_outliers_mask], y[non_outliers_mask],\n",
    "                      hist_nbins=50,\n",
    "                      x0_label=\"Median Income\",\n",
    "                      x1_label=\"Number of households\",\n",
    "                      title=\"Zoom-in\")\n",
    "\n",
    "    norm = mpl.colors.Normalize(y_full.min(), y_full.max())\n",
    "    mpl.colorbar.ColorbarBase(ax_colorbar, cmap=cmap,\n",
    "                              norm=norm, orientation='vertical',\n",
    "                              label='Color mapping for values of y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Original data\n",
    "-------------\n",
    "\n",
    "Each transformation is plotted showing two transformed features, with the\n",
    "left plot showing the entire dataset, and the right zoomed-in to show the\n",
    "dataset without the marginal outliers. A large majority of the samples are\n",
    "compacted to a specific range, [0, 10] for the median income and [0, 6] for\n",
    "the number of households. Note that there are some marginal outliers (some\n",
    "blocks have more than 1200 households). Therefore, a specific pre-processing\n",
    "can be very beneficial depending of the application. In the following, we\n",
    "present some insights and behaviors of those pre-processing methods in the\n",
    "presence of marginal outliers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StandardScaler\n",
    "--------------\n",
    "\n",
    "``StandardScaler`` removes the mean and scales the data to unit variance.\n",
    "However, the outliers have an influence when computing the empirical mean and\n",
    "standard deviation which shrink the range of the feature values as shown in\n",
    "the left figure below. Note in particular that because the outliers on each\n",
    "feature have different magnitudes, the spread of the transformed data on\n",
    "each feature is very different: most of the data lie in the [-2, 4] range for\n",
    "the transformed median income feature while the same data is squeezed in the\n",
    "smaller [-0.2, 0.2] range for the transformed number of households.\n",
    "\n",
    "``StandardScaler`` therefore cannot guarantee balanced feature scales in the\n",
    "presence of outliers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MinMaxScaler\n",
    "------------\n",
    "\n",
    "``MinMaxScaler`` rescales the data set such that all feature values are in\n",
    "the range [0, 1] as shown in the right panel below. However, this scaling\n",
    "compress all inliers in the narrow range [0, 0.005] for the transformed\n",
    "number of households.\n",
    "\n",
    "As ``StandardScaler``, ``MinMaxScaler`` is very sensitive to the presence of\n",
    "outliers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MaxAbsScaler\n",
    "------------\n",
    "\n",
    "``MaxAbsScaler`` differs from the previous scaler such that the absolute\n",
    "values are mapped in the range [0, 1]. On positive only data, this scaler\n",
    "behaves similarly to ``MinMaxScaler`` and therefore also suffers from the\n",
    "presence of large outliers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RobustScaler\n",
    "------------\n",
    "\n",
    "Unlike the previous scalers, the centering and scaling statistics of this\n",
    "scaler are based on percentiles and are therefore not influenced by a few\n",
    "number of very large marginal outliers. Consequently, the resulting range of\n",
    "the transformed feature values is larger than for the previous scalers and,\n",
    "more importantly, are approximately similar: for both features most of the\n",
    "transformed values lie in a [-2, 3] range as seen in the zoomed-in figure.\n",
    "Note that the outliers themselves are still present in the transformed data.\n",
    "If a separate outlier clipping is desirable, a non-linear transformation is\n",
    "required (see below).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PowerTransformer\n",
    "----------------\n",
    "\n",
    "``PowerTransformer`` applies a power transformation to each feature to make\n",
    "the data more Gaussian-like. Currently, ``PowerTransformer`` implements the\n",
    "Yeo-Johnson and Box-Cox transforms. The power transform finds the optimal\n",
    "scaling factor to stabilize variance and mimimize skewness through maximum\n",
    "likelihood estimation. By default, ``PowerTransformer`` also applies\n",
    "zero-mean, unit variance normalization to the transformed output. Note that\n",
    "Box-Cox can only be applied to strictly positive data. Income and number of\n",
    "households happen to be strictly positive, but if negative values are present\n",
    "the Yeo-Johnson transformed is to be preferred.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot(5)\n",
    "make_plot(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QuantileTransformer (Gaussian output)\n",
    "-------------------------------------\n",
    "\n",
    "``QuantileTransformer`` has an additional ``output_distribution`` parameter\n",
    "allowing to match a Gaussian distribution instead of a uniform distribution.\n",
    "Note that this non-parametetric transformer introduces saturation artifacts\n",
    "for extreme values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QuantileTransformer (uniform output)\n",
    "------------------------------------\n",
    "\n",
    "``QuantileTransformer`` applies a non-linear transformation such that the\n",
    "probability density function of each feature will be mapped to a uniform\n",
    "distribution. In this case, all the data will be mapped in the range [0, 1],\n",
    "even the outliers which cannot be distinguished anymore from the inliers.\n",
    "\n",
    "As ``RobustScaler``, ``QuantileTransformer`` is robust to outliers in the\n",
    "sense that adding or removing outliers in the training set will yield\n",
    "approximately the same transformation on held out data. But contrary to\n",
    "``RobustScaler``, ``QuantileTransformer`` will also automatically collapse\n",
    "any outlier by setting them to the a priori defined range boundaries (0 and\n",
    "1).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizer\n",
    "----------\n",
    "\n",
    "The ``Normalizer`` rescales the vector for each sample to have unit norm,\n",
    "independently of the distribution of the samples. It can be seen on both\n",
    "figures below where all samples are mapped onto the unit circle. In our\n",
    "example the two selected features have only positive values; therefore the\n",
    "transformed data only lie in the positive quadrant. This would not be the\n",
    "case if some original features had a mix of positive and negative values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot(9)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
