{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%matplotlib inline\n",
    "#To import all shogun classes\n",
    "from shogun import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "sc = StandardScaler()\n",
    "import os\n",
    "import pickle\n",
    "import fnmatch\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, recall_score, precision_score\n",
    "from collections import defaultdict\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##useful functions\n",
    "\n",
    "def fwd_dates(_dates_list, _key_date):\n",
    "    # returns a list of dates that are forward from the key_date\n",
    "    fwd_dates_list = [i for i in _dates_list if i > _key_date]\n",
    "    return fwd_dates_list\n",
    "\n",
    "def common_member(a, b): \n",
    "      \n",
    "    a_set = set(a) \n",
    "    b_set = set(b) \n",
    "      \n",
    "    # check length  \n",
    "    if len(a_set.intersection(b_set)) > 0: \n",
    "        return(a_set.intersection(b_set))   \n",
    "    else: \n",
    "        return(\"no common elements\") \n",
    "\n",
    "\n",
    "def remove_nans(features_tuple, labels, idx=1):\n",
    "    # not the cleanest but useful\n",
    "    # function to clean up nans as I seem to use it a lot, so better to have one function\n",
    "    # combines the features and labels and removes rows with nans across so we dont lose the ordering\n",
    "    # returns features and labels\n",
    "    features_df = pd.concat([features_tuple[0], features_tuple[1], features_tuple[2], \\\n",
    "                             features_tuple[3]], axis=1, sort=False)\n",
    "    labels_only = labels.drop(columns=['ReturnTradedPrice', 'Duration', 'states', 'TradedTime',\n",
    "                                       'TradedPrice'], axis=1)\n",
    "    df_concat = pd.concat([features_df, labels_only.iloc[:, 0:idx]], axis=1, sort='False')\n",
    "    # only using 1st set of labels- but we can re-write this a bit\n",
    "    df_x_nan = df_concat.dropna()  # dropping all nans\n",
    "    label_column_loc_ = df_x_nan.shape[1] - 1  # location of labels column in the clean df\n",
    "    labels_ = df_x_nan.iloc[:, label_column_loc_:label_column_loc_ + 1]  # keep pure labels\n",
    "    features_ = df_x_nan.drop(df_x_nan.columns[label_column_loc_], axis=1)  # keeping the features only\n",
    "    return features_, labels_\n",
    "\n",
    "\n",
    "def prec_recall_report(y_true, y_predict):\n",
    "    # function to ge the sci-kit learn classification metrics into a pretty DF for csv!\n",
    "    report = pd.DataFrame(list(precision_recall_fscore_support(y_true, y_predict)),\n",
    "                          index=['Precision', 'Recall', 'F1-score', 'Support']).T\n",
    "    # Now add the 'Avg/Total' row\n",
    "    report.loc['Avg/Total', :] = precision_recall_fscore_support(y_true, y_predict, average='weighted')\n",
    "    report.loc['Avg/Total', 'Support'] = report['Support'].sum()\n",
    "    return report\n",
    "\n",
    "\n",
    "class DataLoader(object):\n",
    "    def __init__(self, path_main, ticker):\n",
    "        self.main_path = path_main\n",
    "        self.ticker = ticker\n",
    "\n",
    "        self.features_labels_path = os.path.join(self.main_path, 'features_models')\n",
    "        self.features_path = os.path.join(self.features_labels_path, 'features')\n",
    "        # collection of per symbol non directional labels\n",
    "        self.labels_path = os.path.join(self.features_labels_path, 'labels', self.ticker, 'NON_DIRECTIONAL')\n",
    "        self.symbol_features_path = os.path.join(self.features_labels_path, 'features', self.ticker, 'MODEL_BASED')\n",
    "        # list of all the model -oos hmm feature dates - each folder is a collection of oos feature dates\n",
    "        self.hmm_dates_list = os.listdir(self.symbol_features_path)  # each folder are the OOS features from each HMM\n",
    "        self.compute_date = os.listdir(os.path.join( \\\n",
    "            self.symbol_features_path, \\\n",
    "            os.listdir(self.symbol_features_path)[1]))[1].split(\"_\")[7]\n",
    "\n",
    "    def ticker_features(self, model_date, date):\n",
    "        # need to make this a lot more flexible with number of states\n",
    "        if model_date < date:\n",
    "            file_name = \"_\".join(\n",
    "                (self.ticker, '3', 'states', 'features', 'date:', date, 'now:', self.compute_date, '.pickle'))\n",
    "            file_loc = os.path.join(self.symbol_features_path, str(model_date), file_name)\n",
    "            with open(file_loc, 'rb') as handle:\n",
    "                ticker_features = pickle.load(handle)\n",
    "        else:\n",
    "            print('Loading Feature Date which is in-sample. Change your Model Date')\n",
    "        return ticker_features\n",
    "\n",
    "    def ticker_labels_csv(self, date):\n",
    "        file_loc = os.path.join(self.labels_path, str(date) + '.csv')\n",
    "        ticker_labels = pd.read_csv(file_loc, index_col=0)\n",
    "        return ticker_labels\n",
    "\n",
    "    @staticmethod\n",
    "    def open_pickle_file(path, pickle_file):\n",
    "        file_loc = os.path.join(path, pickle_file)\n",
    "        pickle_to_file = pickle.load(open(file_loc, \"rb\"))\n",
    "        return pickle_to_file\n",
    "\n",
    "    @staticmethod\n",
    "    def get_date_from_file(file_, numb_):\n",
    "        return os.path.splitext(file_[numb_])[0]\n",
    "\n",
    "class MarketFeatures(object):\n",
    "    # a class to be expanded that uses features for base case -market based only-indicators/features\n",
    "    \"\"\"\"Requires:\n",
    "    a dataframe that has TradedPrice And Volume columns\n",
    "    symbol - A stock symbol on which to form a strategy on.\n",
    "    short_window - Lookback period for short moving average.\n",
    "    long_window - Lookback period for long moving average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        #         self.ticker = ticker\n",
    "        self.df = df\n",
    "\n",
    "    def load_data(self):\n",
    "        pass\n",
    "\n",
    "    def ma_spread(self, short_window=5, long_window=20):\n",
    "        # function that produces the MA spread, which can be used on its own or as an input for MACD\n",
    "        short_rolling_px = self.df['TradedPrice'].rolling(window=short_window).mean()\n",
    "        long_rolling_px = self.df['TradedPrice'].rolling(window=long_window).mean()\n",
    "        px_name = \"_\".join(('px_indx', str(short_window), str(long_window)))\n",
    "        self.df[px_name] = long_rolling_px - short_rolling_px\n",
    "        return self.df\n",
    "\n",
    "    def ma_spread_duration(self, short_window=5, long_window=20):\n",
    "        # function that produces the MA spread, which can be used on its own or as an input for MACD\n",
    "        short_rolling_px = self.df['Duration'].rolling(window=short_window).mean()\n",
    "        long_rolling_px = self.df['Duration'].rolling(window=long_window).mean()\n",
    "        dur_name = \"_\".join(('dur_indx', str(short_window), str(long_window)))\n",
    "        self.df[dur_name] = long_rolling_px - short_rolling_px\n",
    "        return self.df\n",
    "\n",
    "    def obv_calc(self):\n",
    "        # on balance volume indicator\n",
    "        self.df['SignedVolume'] = self.df['Volume'] * np.sign(self.df['TradedPrice'].diff()).cumsum()\n",
    "        self.df['SignedVolume'].iat[1] = 0\n",
    "        self.df['OBV'] = self.df['SignedVolume']  # .cumsum()\n",
    "        self.df = self.df.drop(columns=['SignedVolume'])\n",
    "        return self.df\n",
    "\n",
    "    def chaikin_mf(self, period=5):\n",
    "        # Chaikin money flow indicator\n",
    "        self.df[\"MF Multiplier\"] = (self.df['TradedPrice'] - (self.df['TradedPrice'].expanding(period).min()) \\\n",
    "                                    - (self.df['TradedPrice'].expanding(period).max() \\\n",
    "                                       - self.df['TradedPrice'])) / (\n",
    "                                           self.df['TradedPrice'].expanding(period).max() - self.df[ \\\n",
    "                                       'TradedPrice'].expanding(period).min())\n",
    "        self.df[\"MF Volume\"] = self.df['MF Multiplier'] * self.df['Volume']\n",
    "        self.df['CMF_' + str(period)] = self.df['MF Volume'].sum() / self.df[\"Volume\"].rolling(period).sum()\n",
    "        self.df = self.df.drop(columns=['MF Multiplier', 'MF Volume'])\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureCreation(idxKey, locDict):\n",
    "    \"\"\" gives out clean features and labels for a given locDict and a idxKey \"\"\"\n",
    "    keys = list(locDict.keys())\n",
    "    featuresIdxDirFileLoc = locDict[keys[idxKey]][0]\n",
    "    labelsIdxDirFileLoc = locDict[keys[idxKey]][1]\n",
    "    ''' read the features file'''\n",
    "    featuresTupleFile = pkl.load(open(featuresIdxDirFileLoc, \"rb\"), encoding='latin1')\n",
    "    dfFeatures = pd.concat([featuresTupleFile[0], featuresTupleFile[1], \\\n",
    "                            featuresTupleFile[2], featuresTupleFile[3]], axis=1, sort=False).fillna(0)\n",
    "    ''' read the labels file'''\n",
    "    labelsDf = pd.read_csv(labelsIdxDirFileLoc)\n",
    "    ''' pop the labels out'''\n",
    "    labels = labelsDf['label_PrMov__window_5__thres_arbitrary__0.1']\n",
    "    '''dataframe of Features and Labels - X and Y'''\n",
    "    dfXY = pd.concat([dfFeatures, labels], axis=1, sort='False').dropna()\n",
    "    labelName = str(dfXY.columns[dfXY.columns.str.contains(pat='label')].values[0])\n",
    "    ''' drop the labels from the features'''\n",
    "    dfX = dfXY.drop(columns=[labelName])\n",
    "    arrX = np.array(dfX)\n",
    "    ''' feature normalisation'''\n",
    "    # feature scaling in [0,1] - X = rescale_01(arrX)\n",
    "    X = normalization(rescale_01(arrX))\n",
    "    y = dfXY[dfXY.columns[dfXY.columns.str.contains(pat='label')]].iloc[:, 0]\n",
    "    ''' returns features, labels'''\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "passport_fin_data_real_data = '/media/ak/My Passport/Data/FinDataReal'\n",
    "passport_data_drive = '/media/ak/My Passport/Data/'\n",
    "features_models = '/media/ak/WorkDrive/Data/features_models/'\n",
    "experiment_data = '/media/ak/My Passport/Experiment Data/'\n",
    "alternate_label_results = '/media/ak/My Passport/Experiment Data/Alt_Label_Results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Symbols we can look at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_ftse = [s for s in os.listdir(os.path.join(experiment_data,os.listdir(experiment_data)[-1])) if s.endswith('.L')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels and Symbols "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Paths #\n",
    "labelPaths = ['LabelsAlternateOne','LabelsAlternateTwo','LabelsAlternateThree','LabelsAlternateFour','LabelsAlternateFive']\n",
    "\n",
    "passportData = '/media/ak/My Passport/Data/FinDataReal/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set symbol "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbolIdx =1\n",
    "symbolChoice = symbols_ftse[symbolIdx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check that the alternate labels exist - if not move to the next symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_path_creation(symbol,passport_drive):\n",
    "    labelPaths = ['LabelsAlternateOne','LabelsAlternateTwo','LabelsAlternateThree','LabelsAlternateFour','LabelsAlternateFive']\n",
    "    # [x for b in a for x in b] # Works fine\n",
    "    labelSymbolPaths = [os.path.join(passport_drive, label, symbol) for label in labelPaths for symbol in symbols]\n",
    "    return labelSymbolPaths\n",
    "def check_label_path_exists(symbol, passport_drive, label):\n",
    "    label_path_to_check =  os.path.join(os.path.join(passport_drive, label, symbol))\n",
    "    \n",
    "    if os.path.exists(label_path_to_check) and len(os.listdir(label_path_to_check)) ==0:\n",
    "        print('Directory exists but is empty') \n",
    "    elif os.path.exists(label_path_to_check) and len(os.listdir(label_path_to_check)) >0:\n",
    "        print('Directory exists and is not empty!')\n",
    "    return print('good to go')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check that the path exists and is not empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory exists and is not empty!\n",
      "good to go\n",
      "None\n",
      "Directory exists and is not empty!\n",
      "good to go\n",
      "None\n",
      "Directory exists and is not empty!\n",
      "good to go\n",
      "None\n",
      "Directory exists and is not empty!\n",
      "good to go\n",
      "None\n",
      "Directory exists and is not empty!\n",
      "good to go\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for label in labelPaths:\n",
    "    print(check_label_path_exists(symbolChoice, passport_drive, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelPaths = ['LabelsAlternateOne','LabelsAlternateTwo','LabelsAlternateThree','LabelsAlternateFour','LabelsAlternateFive']\n",
    "symbols = os.listdir(passport_drive+'/Labels')\n",
    "# [x for b in a for x in b] # Works fine\n",
    "labelSymbolPaths = [os.path.join(passport_drive, label, symbol) for label in labelPaths for symbol in symbols]\n",
    "# double list comprehenstion that creates a directory in the Passport file for each label/symbol\n",
    "# labelSymbolPaths\n",
    "def specific_Symbol_Labe_lPaths(labelSymbolPaths, symbolChoice):\n",
    "    return [s for s in labelSymbolPaths if symbolChoice in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many and the dates in each Path\n",
    "alternateDateDates = dict()\n",
    "for label in labelSymbolPaths:\n",
    "    alternateDateDates[label] =   os.listdir(specific_Symbol_Labe_lPaths(labelSymbolPaths, symbolChoice)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx =1\n",
    "#file that has all the features from experiments\n",
    "experiments_features_path  = os.path.join(experiment_data,os.listdir(experiment_data)[-1])\n",
    "# for symbol with idx above, create the specific features path\n",
    "## where all teh features are saved: experiments_features_path "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### which symbol are you using?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AAL.L'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols_ftse[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this is the old models dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_feature_path = os.path.join(experiments_features_path,symbols_ftse[idx],'MODEL_BASED')\n",
    "# the list dir above has all the models that were used to create features\n",
    "symbol_feature_dates = os.listdir(symbol_feature_path)\n",
    "### <--- this is the old hmm models dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## and these are the label dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "altnerateLabelKeys  = list(alternateDateDates.keys())\n",
    "label_dates = alternateDateDates[altnerateLabelKeys[0]]\n",
    "# 0 here needs to be indexed by the key we want, and the key corresponds to a label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/media/ak/My Passport/Data/FinDataReal/LabelsAlternateOne/AAL.L',\n",
       " '/media/ak/My Passport/Data/FinDataReal/LabelsAlternateTwo/AAL.L',\n",
       " '/media/ak/My Passport/Data/FinDataReal/LabelsAlternateThree/AAL.L',\n",
       " '/media/ak/My Passport/Data/FinDataReal/LabelsAlternateFour/AAL.L',\n",
       " '/media/ak/My Passport/Data/FinDataReal/LabelsAlternateFive/AAL.L']"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path = os.path.join(features_models, 'features')\n",
    "symbol_features_path = os.path.join(features_path, symbolChoice, 'MODEL_BASED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_feature_idex =1  ## this is an HMM model date -or HMMModelDate in the other code base\n",
    "## next create a path for the HMM model date\n",
    "symbol_feature_files_paths = os.path.join(symbol_feature_path, symbol_feature_dates[symbol_feature_idex])\n",
    "## get all the model date feature files\n",
    "symbol_features_files_idex = os.listdir(symbol_feature_files_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extracting feature dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbolEachModelFeaturesDates = [file.split(\"_\")[5] for file in symbol_features_files_idex] \n",
    "#this needs to be indexed by symbol_feature_idex or hmmDateIdx date in the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/media/ak/My Passport/Data/FinDataReal/LabelsAlternateOne/AAL.L/20170127.csv',\n",
       " '/media/ak/My Passport/Data/FinDataReal/LabelsAlternateTwo/AAL.L/20170127.csv',\n",
       " '/media/ak/My Passport/Data/FinDataReal/LabelsAlternateThree/AAL.L/20170127.csv',\n",
       " '/media/ak/My Passport/Data/FinDataReal/LabelsAlternateFour/AAL.L/20170127.csv',\n",
       " '/media/ak/My Passport/Data/FinDataReal/LabelsAlternateFive/AAL.L/20170127.csv']"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbolAlternateLabelLocs = [x for x in altnerateLabelKeys if str(symbols_ftse[idx]) in x]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get common dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonDates = list(set(symbolEachModelFeaturesDates) & set(label_dates))\n",
    "##common dates based on the attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20170829\n",
      "20170816\n",
      "20170809\n",
      "20170814\n",
      "20170810\n",
      "20170824\n",
      "20170126\n",
      "20170125\n",
      "20170803\n",
      "20170830\n",
      "20170117\n",
      "20170801\n",
      "20170807\n",
      "20170120\n",
      "20170815\n",
      "20170118\n",
      "20170823\n",
      "20170808\n",
      "20170822\n",
      "20170123\n",
      "20170131\n",
      "20170811\n",
      "20170124\n",
      "20170825\n",
      "20170821\n",
      "20170818\n",
      "20170804\n",
      "20170817\n",
      "20170831\n",
      "20170802\n",
      "20170119\n",
      "20170130\n",
      "20170127\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for commonDate in commonDates:\n",
    "    print(commonDate)\n",
    "    FeatureFileLoc = os.path.join(symbol_feature_files_paths,\"\".join((symbols_ftse[idx],'_3_states_features_',\n",
    " 'date:_',commonDate, '_now:_',str('20181227'),'_.pickle')))\n",
    "    alternateLabels  = [os.path.join(x, commonDate+'.csv') for x in symbolAlternateLabelLocs]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for i,_ in enumerate(alternateLabels):\n",
    "    print(os.path.exists(alternateLabels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #conditions= [os.path.exists(FeatureFileLoc), [os.path.exists(alternateLabels[i]) for i in [0,1,2,3,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_symbols_d = defaultdict(dict)\n",
    "symbol_model_dates = dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_model_dates = os.listdir(symbol_feature_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.listdir(os.path.join(symbol_feature_path,symbol_model_dates[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLoca = os.path.join('/media/ak/My Passport/Experiment Data/MKLExpPath', symbolChoice)\n",
    "common_loc = os.path.join(testLoca,'CommonLocationsDicts.pkl' )\n",
    "loc_dists_loc = os.path.join(testLoca,'LocDictsListCorrect.pkl' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(loc_dists_loc, 'rb') as f:\n",
    "    test=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
