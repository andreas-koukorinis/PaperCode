{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ak/Envs/DataAnalysis/local/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/ak/Envs/DataAnalysis/local/lib/python2.7/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime\n",
    "import sys\n",
    "import sklearn\n",
    "import scipy.stats as stats\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import linear_model, decomposition\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import decomposition\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "# Added version check for recent scikit-learn 0.18 checksok ca\n",
    "from distutils.version import LooseVersion as Version\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "from sklearn.svm import SVC # \"Support Vector Classifier\"\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "from math import sqrt\n",
    "from sklearn.datasets import make_circles\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "\n",
    "###ticker###\n",
    "ticker='VOD' #enter symbol you want - this will be made for various\n",
    "\n",
    "# paths\n",
    "_pkl_path =('/home/ak/Documents/Data/features_models/')#pkl path\n",
    "_graphs_path= os.path.join(_pkl_path, 'graphs')\n",
    "_models_path = '/home/ak/Documents/features_test/models/'\n",
    "\n",
    "###predicitions path\n",
    "_predictions_path ='/home/ak/Documents/features_models/predictions/'\n",
    "\n",
    "sys.path.append('/home/ak/Documents/Research/hsmm/hsmm/')\n",
    "\n",
    "sys.path.append('/home/ak/Documents/Research/hsmm/hsmm/')\n",
    "sys.path.append('/home/ak/Documents/Research/QFPaper/aknotebooks/classification/convenience_functions/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_list= ['RB','CPI','AAL','IOG','CNA','VOD'] #can expand this after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##useful functions##\n",
    "##re-write some of these###\n",
    "def off_set(array_1, array_2):\n",
    "    off_set= array_1.__len__()- array_2.__len__()\n",
    "    off_set_abs =np.abs(off_set)\n",
    "    return off_set_abs\n",
    "\n",
    "def simpleInputs(_X,_y):\n",
    "    _offset =np.abs(off_set(_X,_y))\n",
    "    if _offset==0:\n",
    "        X = _X.as_matrix().astype(np.float)\n",
    "        y=_y.astype(np.int)\n",
    "    else:\n",
    "        X= _X[:-_offset].as_matrix().astype(np.float)\n",
    "        y = _y.astype(np.int)\n",
    "    return X, y\n",
    "\n",
    "def ensure_dir(file_path): #ensure a dictory exists otherwise create it\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print('New Directory Created',file_path)\n",
    "    else:\n",
    "        print('Directory Exists!check')\n",
    "\n",
    "def symbol_path_(_main_path, _symb): #create directory for each symbol!\n",
    "    _main_path = _ftse\n",
    "    _symbol_path = os.path.join(_main_path,_symb+'/')\n",
    "    print(_symbol_path)\n",
    "    ensure_dir(_symbol_path)\n",
    "    return _symbol_path\n",
    "\n",
    "def symbol_features_path(_main_path, _symb):\n",
    "    _symb_features_path = os.path.join(_main_path,_symb+'/')\n",
    "    print(_symb_features_path)\n",
    "    ensure_dir(symbol_features_path)\n",
    "    return symbol_features_path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###main code###\n",
    "\n",
    "##load data\n",
    "with open(_pkl_path + ticker+'_features.pickle', 'rb') as handle:\n",
    "    ticker_features_ = pickle.load(handle)\n",
    "with open(_pkl_path + ticker+'_labelled_data.pickle', 'rb') as handle:\n",
    "    ticker_labelled_data_ = pickle.load(handle)\n",
    "\n",
    "###the following bit is a bit redundant###\n",
    "_data = ticker_labelled_data_  # data\n",
    "_features = ticker_features_  # features\n",
    "_keys = _data.keys()  # dates\n",
    "\n",
    "# ###scalers###\n",
    "# mms = MinMaxScaler()\n",
    "# stdsc = StandardScaler()\n",
    "# _clfs=[]\n",
    "# _xr=len(_keys)-1\n",
    "# print(\"#of models:\",_xr)\n",
    "# print(\"for ticker:\", ticker)\n",
    "# for _idx in xrange(0, _xr):\n",
    "#     # various types of labels\n",
    "#     y_1 = _data[_keys[_idx]].iloc[:, 4].dropna().astype(int).values  # label_control_chart_q_30_q_70_window_5\n",
    "#     y_2 = _data[_keys[_idx]].iloc[:, 5].dropna().astype(\n",
    "#         int).values  # label_simple_ep_window_5_thresh_0.05- Last 4 are NaN\n",
    "#     y_3 = _data[_keys[_idx]].iloc[:, 6].dropna().astype(\n",
    "#         int).values  # label_simple_ep_window_5_thresh_0.00-last 4 are NaN\n",
    "#     duration = _data[_keys[_idx]].iloc[:, 3].dropna().astype(int).values  # can be used for regression\n",
    "\n",
    "#     # feature set\n",
    "#     _gamma = _features[_keys[_idx]][2]\n",
    "#     _csi = _features[_keys[_idx]][3]\n",
    "#     _info = _features[_keys[_idx]][1]\n",
    "#     _fischer = _features[_keys[_idx]][0]\n",
    "#     short_rolling = _data[_keys[_idx]]['TradedPrice'].rolling(window=5).mean()\n",
    "#     long_rolling = _data[_keys[_idx]]['TradedPrice'].rolling(window=15).mean()\n",
    "#     ma_signal = (long_rolling - short_rolling).fillna(0)\n",
    "#     df_new = pd.concat([_csi, _gamma, _fischer, short_rolling], axis=1).dropna()\n",
    "\n",
    "#     X, y = simpleInputs(df_new, y_3)\n",
    "\n",
    "#     print(\"number of classes:\",len(np.unique(y)))  # of classes\n",
    "\n",
    "#     X_mms = X = mms.fit_transform(X)\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X_mms, y, test_size=.1, random_state=0)\n",
    "\n",
    "#     #     # # Train a SVM classification model\n",
    "#     param_grid = dict(kernel=[\"rbf\"],\n",
    "#                       C=[1,5,10,25,50],\n",
    "#                       gamma=[0.0001,0.001,0.01])\n",
    "\n",
    "#     clf = GridSearchCV(SVC(class_weight='balanced'), param_grid, verbose=1,n_jobs=-1,cv=5)\n",
    "\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     filename = _models_path + str(ticker) + '_' + str(_keys[_idx]) + '_clf' + '.joblib.pkl'\n",
    "#     pickle.dump(clf, open(filename, 'wb'))\n",
    "#     print('SVM Best Params & Score:')\n",
    "#     print clf.best_params_, clf.best_score_\n",
    "#     _clfs.append(clf)\n",
    "#     clf_files = _models_path + str(ticker) + '_clfs_'  + '.pkl'\n",
    "#     pickle.dump(_clfs, open(clf_files, 'wb'))\n",
    "#     print(\"#of models   left:\",_xr -_idx\n",
    "    # X_1_offset= _set_1[:-_offset].as_matrix().astype(np.float)\n",
    "    # #define train and test labels - redundant\n",
    "    # X_train_std = stdsc.fit_transform(X_1_offset)\n",
    "    # y_train =y_1\n",
    "    #\n",
    "    #\n",
    "    # n_components= [2,4,6]\n",
    "    # parameters = {\n",
    "    # \"estimator__C\": [1,2,4,8],\n",
    "    # \"estimator__kernel\": [\"poly\",\"rbf\"],\n",
    "    # \"estimator__degree\":[0.001, 0.01, 1, 3, 4],\n",
    "    # \"pca__n_components\": n_components}\n",
    "    #\n",
    "    # logistic = linear_model.LogisticRegression()\n",
    "    # pca = decomposition.PCA()\n",
    "    # pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n",
    "    # pca.fit(X_train_std)\n",
    "    #\n",
    "    # n_components = [2,4]\n",
    "    # Cs = np.logspace(-4, 4, 3)\n",
    "    # estimator = GridSearchCV(pipe, dict(pca__n_components=n_components,logistic__C=Cs))\n",
    "    # estimator.fit(X_train_std, y_train)\n",
    "    # print('Logistic Best Score:')\n",
    "    # print(estimator.best_score_)\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    # pca = decomposition.RandomizedPCA(n_components=3, whiten=True).fit(X_train_std)\n",
    "    #\n",
    "    # X_train_pca = pca.transform(X_train_std)\n",
    "    #\n",
    "    # # Train a SVM classification model\n",
    "    # param_grid = dict(kernel =[\"rbf\",\"linear\"],\n",
    "    #                   C=[1, 5, 10, 50, 100],\n",
    "    #                   gamma=[0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1])\n",
    "    # # clf = GridSearchCV(SVC(kernel='rbf'), param_grid,\n",
    "    # #                verbose=1)\n",
    "    # clf = GridSearchCV(SVC(class_weight='balanced'), param_grid, verbose=1)\n",
    "    #\n",
    "    # clf.fit(X_train_pca, y_train)\n",
    "    # print('SVM Best Params & Score:')\n",
    "    # print clf.best_params_, clf.best_score_\n",
    "    #\n",
    "    # filename = _models_path+str(ticker)+'_'+str(_keys[1])+'_classifier'+'.joblib.pkl'\n",
    "    # pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20180302',\n",
       " '20180315',\n",
       " '20180316',\n",
       " '20180306',\n",
       " '20180329',\n",
       " '20180313',\n",
       " '20180312',\n",
       " '20180320',\n",
       " '20180308',\n",
       " '20180327',\n",
       " '20180319',\n",
       " '20180321',\n",
       " '20180322',\n",
       " '20180323',\n",
       " '20180314',\n",
       " '20180301',\n",
       " '20180309',\n",
       " '20180328',\n",
       " '20180305',\n",
       " '20180326',\n",
       " '20180307']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7744\n",
      "7743\n"
     ]
    }
   ],
   "source": [
    "_keys[1]\n",
    "#print np.shape(_features[_keys[1]])\n",
    "print _data[_keys[1]].__len__()\n",
    "_lengths_=[_features[_keys[1]][1].__len__(), _features[_keys[1]][2].__len__(),_features[_keys[1]][3].__len__()]\n",
    "min_length_=min(_lengths_)\n",
    "print min_length_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data[_keys[1]].head(10)\n",
    "_data_columns_list=_data[_keys[1]].head(10).columns.values.tolist()\n",
    "_data_columns_list[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 20180302\n",
      "10745\n",
      "1 20180315\n",
      "7725\n",
      "2 20180316\n",
      "9238\n",
      "3 20180306\n",
      "9075\n",
      "4 20180329\n",
      "9251\n",
      "5 20180313\n",
      "12316\n",
      "6 20180312\n",
      "6959\n",
      "7 20180320\n",
      "13000\n",
      "8 20180308\n",
      "9425\n",
      "9 20180327\n",
      "10753\n",
      "10 20180319\n",
      "11058\n",
      "11 20180321\n",
      "13972\n",
      "12 20180322\n",
      "22116\n",
      "13 20180323\n",
      "21913\n",
      "14 20180314\n",
      "10258\n",
      "15 20180301\n",
      "10378\n",
      "16 20180309\n",
      "10352\n",
      "17 20180328\n",
      "14413\n",
      "18 20180305\n",
      "9000\n",
      "19 20180326\n",
      "11979\n",
      "20 20180307\n",
      "9623\n"
     ]
    }
   ],
   "source": [
    "for _idx, _ in enumerate(_keys):\n",
    "    print _idx, _\n",
    "    test=_data[_keys[_idx]].iloc[:, 6].dropna().astype(int).values\n",
    "    print len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mms = MinMaxScaler()\n",
    "stdsc = StandardScaler()\n",
    "_clfs = []\n",
    "_reports=[]\n",
    "_idx=1\n",
    "# various types of labels\n",
    "y_1 = _data[_keys[_idx]].iloc[:, 4].dropna().astype(int).values  # label_control_chart_q_30_q_70_window_5\n",
    "y_2 = _data[_keys[_idx]].iloc[:, 5].dropna().astype(int).values  # label_simple_ep_window_5_thresh_0.05- Last 4 are NaN\n",
    "y_3 = _data[_keys[_idx]].iloc[:, 6].dropna().astype(int).values  # label_simple_ep_window_5_thresh_0.00-last 4 are NaN\n",
    "duration = _data[_keys[_idx]].iloc[:, 3].dropna().astype(int).values  # can be used for regression\n",
    "\n",
    "# feature set\n",
    "_gamma = _features[_keys[_idx]][2]\n",
    "_csi = _features[_keys[_idx]][3]\n",
    "_info = _features[_keys[_idx]][1]\n",
    "_fischer = _features[_keys[_idx]][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   33.8s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:   59.2s finished\n"
     ]
    }
   ],
   "source": [
    "#label\n",
    "y = y_3\n",
    "X, y = simpleInputs(_csi,y)\n",
    "X_mms = X = mms.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mms, y, test_size=.1, random_state=0)\n",
    "## Train a SVM classification model\n",
    "param_grid = dict(kernel=[\"rbf\"], C=[1,5,10,25,50], gamma=[0.0001,0.001,0.01])\n",
    "clf = GridSearchCV(SVC(class_weight='balanced'), param_grid, verbose=1,n_jobs=-1,cv=5)            \n",
    "clf.fit(X_train, y_train)\n",
    "y_pred= clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Best Params & Score:\n",
      "{'kernel': 'rbf', 'C': 25, 'gamma': 0.0001} 0.63247986191\n"
     ]
    }
   ],
   "source": [
    "print('SVM Best Params & Score:')\n",
    "print clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.00      0.00      0.00       497\n",
      "          1       0.36      1.00      0.53       276\n",
      "\n",
      "avg / total       0.13      0.36      0.19       773\n",
      "\n",
      "0.35705045278137126\n"
     ]
    }
   ],
   "source": [
    "print classification_report(y_test, y_pred)\n",
    "print accuracy_score(y_test, y_pred)\n",
    "# _reports.append(classification_report(y_test, y_pred))\n",
    "# _clfs.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model(_data, _features,_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_idx=1\n",
    "_data[_keys[_idx]].iloc[:,6].dropna().astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset(X, y, axes):\n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n",
    "    plt.axis(axes)\n",
    "    plt.grid(True, which='both')\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=20)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=20, rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "class DataFuncs():\n",
    "    def __init__(self, _pkl_path='/home/ak/Documents/features_test/',_graphs_path='/home/ak/Documents/features_test/graphs/',_models_path='/home/ak/Documents/features_test/models/',_predictions_path='/home/ak/Documents/features_test/predictions/', ticker='C.N'):\n",
    "        # Where to save the figures\n",
    "        self._pkl_path = _pkl_path\n",
    "        self.ticker = ticker  # enter symbol you want - this will be made for variou\n",
    "        self._graphs_path = _graphs_path\n",
    "        self._models_path = _models_path\n",
    "        self._predictions_path = _predictions_path\n",
    "#     def __init__(self,_pkl_path,_graphs_path,_models_path,_predictions_path, ticker='C.N'):\n",
    "#         # Where to save the figures\n",
    "#         self._pkl_path = ('/home/ak/Documents/features_test/')\n",
    "#         self.ticker = ticker  # enter symbol you want - this will be made for variou\n",
    "#         self._graphs_path = '/home/ak/Documents/features_test/graphs/'\n",
    "#         self._models_path = '/home/ak/Documents/features_test/models/'\n",
    "#         self._predictions_path = '/home/ak/Documents/features_test/predictions/'\n",
    "\n",
    "    def off_set(self, array_1, array_2):\n",
    "        off_set= len(array_1)-len(array_2)\n",
    "        return off_set\n",
    "\n",
    "    def simpleInputs(self,_X,_y):\n",
    "        _offset =np.abs(self.off_set(_X,_y))\n",
    "        if _offset==0:\n",
    "            X = _X.as_matrix().astype(np.float)\n",
    "            y=_y.astype(np.int)\n",
    "        else:\n",
    "            X= _X[:-_offset].as_matrix().astype(np.float)\n",
    "            y = _y.astype(np.int)\n",
    "        return X, y\n",
    "##load data\n",
    "    def load_data(self):\n",
    "        with open(self._pkl_path + self.ticker+'_features.pickle', 'rb') as handle:\n",
    "            ticker_features_ = pickle.load(handle)\n",
    "        with open(self._pkl_path + self.ticker+'_labelled_data.pickle', 'rb') as handle:\n",
    "            ticker_labelled_data_ = pickle.load(handle)\n",
    "\n",
    "        return ticker_features_, ticker_labelled_data_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "gamma1, gamma2 = 0.1, 5\n",
    "C1, C2 = 0.001, 1000\n",
    "hyperparams = (gamma1, C1), (gamma1, C2), (gamma2, C1), (gamma2, C2)\n",
    "X= X_train\n",
    "y= y_train\n",
    "svm_clfs = []\n",
    "for gamma, C in hyperparams:\n",
    "    rbf_kernel_clf=svm.SVC(kernel=\"rbf\", gamma=gamma, C=C)\n",
    "#     rbf_kernel_svm_clf = Pipeline((\n",
    "#             (\"scaler\", StandardScaler()),\n",
    "#             (\"svm_clf\", SVC(kernel=\"rbf\", gamma=gamma, C=C))\n",
    "#         ))\n",
    "    rbf_kernel_clf.fit(X, y)\n",
    "    svm_clfs.append(rbf_kernel_clf)\n",
    "\n",
    "plt.figure(figsize=(11, 7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes= [-2.5, 2.5, -1, 1.5]\n",
    "for i, svm_clf in enumerate(svm_clfs):\n",
    "    plt.subplot(221 + i)\n",
    "    #plot_predictions(svm_clf, [-1.5, 2.5, -1, 1.5])\n",
    "    plot_dataset(X, y,axes)\n",
    "    gamma, C = hyperparams[i]\n",
    "    plt.title(r\"$\\gamma = {}, C = {}$\".format(gamma, C), fontsize=8)\n",
    "\n",
    "# save_fig(\"moons_rbf_svc_plot\")\n",
    "y_decision=rbf_kernel_clf.decision_function(X)\n",
    "x0s = np.linspace(axes[0], axes[1]).reshape(len(y_decision),1)\n",
    "x1s = np.linspace(axes[2], axes[3]).reshape(len(y_decision),1)\n",
    "x0, x1 = np.meshgrid(x0s, x1s)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_decision=rbf_kernel_clf.decision_function(X).reshape(8089,1)\n",
    "plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
