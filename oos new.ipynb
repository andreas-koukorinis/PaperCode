{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "sc = StandardScaler()\n",
    "import os\n",
    "import pickle\n",
    "import fnmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##useful functions\n",
    "def fwd_dates(_dates_list, _key_date):\n",
    "    # returns a list of dates that are forward from the key_date\n",
    "    fwd_dates_list = [i for i in _dates_list if i > _key_date]\n",
    "    return fwd_dates_list\n",
    "def remove_nans(features_tuple, labels, idx=1):\n",
    "    # not the cleanest but useful\n",
    "    # function to clean up nans as I seem to use it a lot, so better to have one function\n",
    "    # combines the features and labels and removes rows with nans across so we dont lose the ordering\n",
    "    # returns features and labels\n",
    "    features_df = pd.concat([features_tuple[0], features_tuple[1], features_tuple[2], \\\n",
    "                             features_tuple[3]], axis=1, sort=False)\n",
    "    labels_only = labels.drop(columns=['ReturnTradedPrice', 'Duration', 'states', 'TradedTime',\n",
    "                                       'TradedPrice'], axis=1)\n",
    "    df_concat = pd.concat([features_df, labels_only.iloc[:, 0:idx]], axis=1, sort='False')\n",
    "    # only using 1st set of labels- but we can re-write this a bit\n",
    "    df_x_nan = df_concat.dropna()  # dropping all nans\n",
    "    label_column_loc_ = df_x_nan.shape[1] - 1  # location of labels column in the clean df\n",
    "    labels_ = df_x_nan.iloc[:, label_column_loc_:label_column_loc_ + 1]  # keep pure labels\n",
    "    features_ = df_x_nan.drop(df_x_nan.columns[label_column_loc_], axis=1)  # keeping the features only\n",
    "    return features_, labels_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataLoader(object):\n",
    "    def __init__(self, path_main, ticker):\n",
    "        self.main_path = path_main\n",
    "        self.ticker = ticker\n",
    "\n",
    "        self.features_labels_path = os.path.join(self.main_path, 'features_models')\n",
    "        self.features_path = os.path.join(self.features_labels_path, 'features')\n",
    "        # collection of per symbol non directional labels\n",
    "        self.labels_path = os.path.join(self.features_labels_path, 'labels', self.ticker, 'NON_DIRECTIONAL')\n",
    "        self.symbol_features_path = os.path.join(self.features_labels_path, 'features', self.ticker, 'MODEL_BASED')\n",
    "        # list of all the model -oos hmm feature dates - each folder is a collection of oos feature dates\n",
    "        self.hmm_dates_list = os.listdir(self.symbol_features_path) #each folder are the OOS features from each HMM\n",
    "        self.compute_date= os.listdir(os.path.join( \\\n",
    "                                                   self.symbol_features_path, \\\n",
    "                                                   os.listdir(self.symbol_features_path)[1]))[1].split(\"_\")[7]\n",
    "\n",
    "    def ticker_features(self, model_date, date):\n",
    "        # need to make this a lot more flexible with number of states\n",
    "        if model_date < date:\n",
    "            file_name = \"_\".join((self.ticker, '3', 'states', 'features', 'date:', date, 'now:', self.compute_date, '.pickle'))\n",
    "            file_loc = os.path.join(self.symbol_features_path, str(model_date), file_name)\n",
    "            with open(file_loc, 'rb') as handle:\n",
    "                ticker_features = pickle.load(handle)\n",
    "        else:\n",
    "            print('Loading Feature Date which is in-sample. Change your Model Date')\n",
    "        return ticker_features\n",
    "\n",
    "    def ticker_labels_csv(self, date):\n",
    "        file_loc = os.path.join(self.labels_path, str(date) + '.csv')\n",
    "        ticker_labels = pd.read_csv(file_loc, index_col=0)\n",
    "        return ticker_labels\n",
    "\n",
    "    @staticmethod\n",
    "    def open_pickle_file(path, pickle_file):\n",
    "        file_loc = os.path.join(path, pickle_file)\n",
    "        pickle_to_file = pickle.load(open(file_loc, \"rb\"))\n",
    "        return pickle_to_file\n",
    "\n",
    "    @staticmethod\n",
    "    def get_date_from_file(file_, numb_):\n",
    "        return os.path.splitext(file_[numb_])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_member(a, b): \n",
    "    \n",
    "        a_set = set(a)\n",
    "        b_set = set(b)\n",
    "    \n",
    "        # check length \n",
    "        if len(a_set.intersection(b_set)) > 0:\n",
    "            return(a_set.intersection(b_set)) \n",
    "        else:\n",
    "            return(\"no common elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def prec_recall_report(y_true, y_predict):\n",
    "    # function to ge the sci-kit learn classification metrics into a pretty DF for csv!\n",
    "    report = pd.DataFrame(list(precision_recall_fscore_support(y_true, y_predict)),\n",
    "                          index=['Precision', 'Recall', 'F1-score', 'Support']).T\n",
    "    # Now add the 'Avg/Total' row\n",
    "    report.loc['Avg/Total', :] = precision_recall_fscore_support(y_true, y_predict, average='weighted')\n",
    "    report.loc['Avg/Total', 'Support'] = report['Support'].sum()\n",
    "    return report\n",
    "\n",
    "class MarketFeatures(object):\n",
    "    # a class to be expanded that uses features for base case -market based only-indicators/features\n",
    "    \"\"\"\"Requires:\n",
    "    a dataframe that has TradedPrice And Volume columns\n",
    "    symbol - A stock symbol on which to form a strategy on.\n",
    "    short_window - Lookback period for short moving average.\n",
    "    long_window - Lookback period for long moving average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        #         self.ticker = ticker\n",
    "        self.df = df\n",
    "\n",
    "    def load_data(self):\n",
    "        pass\n",
    "\n",
    "    def ma_spread(self, short_window=5, long_window=20):\n",
    "        # function that produces the MA spread, which can be used on its own or as an input for MACD\n",
    "        short_rolling_px = self.df['TradedPrice'].rolling(window=short_window).mean()\n",
    "        long_rolling_px = self.df['TradedPrice'].rolling(window=long_window).mean()\n",
    "        px_name = \"_\".join(('px_indx', str(short_window), str(long_window)))\n",
    "        self.df[px_name] = long_rolling_px - short_rolling_px\n",
    "        return self.df\n",
    "\n",
    "    def ma_spread_duration(self, short_window=5, long_window=20):\n",
    "        # function that produces the MA spread, which can be used on its own or as an input for MACD\n",
    "        short_rolling_px = self.df['Duration'].rolling(window=short_window).mean()\n",
    "        long_rolling_px = self.df['Duration'].rolling(window=long_window).mean()\n",
    "        dur_name = \"_\".join(('dur_indx', str(short_window), str(long_window)))\n",
    "        self.df[dur_name] = long_rolling_px - short_rolling_px\n",
    "        return self.df\n",
    "\n",
    "    def obv_calc(self):\n",
    "        # on balance volume indicator\n",
    "        self.df['SignedVolume'] = self.df['Volume'] * np.sign(self.df['TradedPrice'].diff()).cumsum()\n",
    "        self.df['SignedVolume'].iat[1] = 0\n",
    "        self.df['OBV'] = self.df['SignedVolume']  # .cumsum()\n",
    "        self.df = self.df.drop(columns=['SignedVolume'])\n",
    "        return self.df\n",
    "\n",
    "    def chaikin_mf(self, period=5):\n",
    "        # Chaikin money flow indicator\n",
    "        self.df[\"MF Multiplier\"] = (self.df['TradedPrice'] - (self.df['TradedPrice'].expanding(period).min()) \\\n",
    "                                    - (self.df['TradedPrice'].expanding(period).max() \\\n",
    "                                       - self.df['TradedPrice'])) / (\n",
    "                                           self.df['TradedPrice'].expanding(period).max() - self.df[ \\\n",
    "                                       'TradedPrice'].expanding(period).min())\n",
    "        self.df[\"MF Volume\"] = self.df['MF Multiplier'] * self.df['Volume']\n",
    "        self.df['CMF_' + str(period)] = self.df['MF Volume'].sum() / self.df[\"Volume\"].rolling(period).sum()\n",
    "        self.df = self.df.drop(columns=['MF Multiplier', 'MF Volume'])\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getenv('FINANCE_DATA')  # main directory referenced in all the code\\n\",\n",
    "data_only_drive = '/mnt/usb-Seagate_Expansion_Desk_NA8XEHR6-0:0-part2'  # external date only drive\\n\",\n",
    "\n",
    "\"# location to save results\\n\",\n",
    "model_save_loc = os.path.join(data_only_drive, 'Data', 'features_models', 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and produces the relevant path of all fitted single kernel models\\n\",\n",
    "# this path is from the Data Only Drive\\n\",\n",
    "def symbol_fitted_models_path(symbol): return os.path.join(model_paths, symbol, 'SINGLE_KERNEL')\n",
    "# provides a list of the above path\n",
    "def symbol_list_fitted_dates(symbol): return sorted(os.listdir(symbol_fitted_models_path(symbol)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def symbol_fitted_model_date_loc(file_path, model_date_no):\n",
    "    return os.path.join(file_path, str(symbol_list_fitted_dates(symbol)[model_date_no]))\n",
    "\n",
    "def symbol_fitted_models_path(symbol): return os.path.join(model_paths, symbol, 'SINGLE_KERNEL')\n",
    "def symbol_list_fitted_dates(symbol): return sorted(os.listdir(symbol_fitted_models_path(symbol)))\n",
    "def symbol_fitted_model_date_loc(file_path, model_date_no):\n",
    "    return os.path.join(file_path, str(symbol_list_fitted_dates(symbol)[model_date_no]))\n",
    "\n",
    "def symbol_labels_path(symbol): \n",
    "    return os.path.join(labels,symbol,'NON_DIRECTIONAL')\n",
    "\n",
    "def symbol_labels_list(symbol):\n",
    "    return os.listdir(symbol_labels_path(symbol))\n",
    "    \n",
    "def symbol_hmm_dates_list(symbol):\n",
    "    return os.listdir(os.path.join(features,symbol,'MODEL_BASED'))\n",
    "\n",
    "def symbol_hmm_dates_path(symbol):\n",
    "    return os.path.join(features,symbol,'MODEL_BASED')\n",
    "def symbol_features_oos_path(symbol, idx):\n",
    "    return os.path.join(symbol_hmm_dates_path(symbol),os.listdir(symbol_hmm_dates_path(symbol))[idx])\n",
    "\n",
    "def fitted_model(idx):\n",
    "    fitted_model = os.path.join(ticker_fitted_models,os.listdir(ticker_fitted_models)[idx])\n",
    "    return fitted_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pickle_to_svc(model_pickle):\n",
    "    pickle_to_file = pickle.load(open(model_pickle, \"rb\"))\n",
    "    best_estimator = pickle_to_file['SVC'].best_estimator_\n",
    "    return best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make list of symbols with models:\n",
    "\n",
    "features_models = os.path.join(data_dir,'features_models')\n",
    "features = os.path.join(data_dir,'features_models','features')\n",
    "labels = os.path.join(data_dir,'features_models','labels')\n",
    "features_models_dod = os.path.join(data_only_drive,'Data', 'features_models')\n",
    "model_paths = os.path.join(data_only_drive,'Data', 'features_models','models')\n",
    "\n",
    "main_path = os.path.join(data_dir, 'features_models')  # main directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_w_models = [s for s in os.listdir(model_save_loc) if s.endswith('.L')]\n",
    "symbols_w_features = os.listdir(features)\n",
    "symbols_w_labels = os.listdir('/media/ak/WorkDrive/Data/features_models/labels/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols_with_features_labels= list(common_member(symbols_w_features, symbols_w_labels))\n",
    "all_good_symbols =list(common_member(symbols_w_models, symbols_with_features_labels))\n",
    "len(all_good_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, symbol in enumerate(all_good_symbols):\n",
    "#     length = len(os.listdir(os.path.join(models, all_good_symbols[idx],'SINGLE_KERNEL')))\n",
    "#     if  length <=0:\n",
    "#         print symbol\n",
    "#         print (\"not ok\", length)\n",
    "#     else:\n",
    "#         continue\n",
    "#         print (\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'KGF.L'\n",
    "ticker_fitted_models = symbol_fitted_models_path(ticker)\n",
    "ticker_labels_path = symbol_labels_path(ticker)\n",
    "ticker_labels_list = [os.listdir(ticker_labels_path)[idx].split(\".\")[0]\n",
    "                      for idx,_ in enumerate(os.listdir(ticker_labels_path))]\n",
    "ticker_features_list = symbol_hmm_dates_list(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'/mnt/usb-Seagate_Expansion_Desk_NA8XEHR6-0:0-part2/Data/features_models/models/SPT.L/SINGLE_KERNEL'\n",
    "test_path= symbol_fitted_models_path(symbol=symbol)\n",
    "# now lets take the exact sub-directory which corresponds to date # 5 in our list of fitted models\n",
    "symb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fcd119a438fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdatacls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_main\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#test symbol -create class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msymbol_compute_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatacls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "datacls = DataLoader(path_main=data_dir, ticker=ticker) #test symbol -create class\n",
    "symbol_compute_date = datacls.compute_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dates = [os.listdir(symbol_labels_path(ticker))[idx].split(\".\")[0] for idx,_ in enumerate(symbol_labels_path(ticker))]\n",
    "common_dates_dict={}\n",
    "for idx, date in enumerate(ticker_features_list):\n",
    "    specific_date_features_path = os.path.join(symbol_hmm_dates_path(ticker), str(date))\n",
    "    features_dates=[os.listdir(specific_date_features_path)[idx].split(\"_\")[5] \n",
    "                    for idx,_ in enumerate(os.listdir(specific_date_features_path))]\n",
    "    common_dates_dict[date] = sorted(list(common_member(labels_dates,features_dates)))\n",
    "\n",
    "#     for specific_idx, specific_date in enumerate(os.listdir(specific_date_features_path)):\n",
    "#         print os.path.join(specific_date_features_path, specific_date)\n",
    "#         features_date= specific_date.split(\"_\")[5]\n",
    "#         print os.path.join(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "legend\n"
     ]
    }
   ],
   "source": [
    "common_keys = common_dates_dict.keys()\n",
    "common_date= common_dates_dict[common_keys[1]][1]\n",
    "labels_file = os.path.join(symbol_labels_path(ticker),\".\".join((str(common_date),'csv')))\n",
    "data_df =pd.read_csv(labels_file)\n",
    "if data_df.shape[0]>10:\n",
    "    print('legend')\n",
    "else:\n",
    "    print('problemo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dates =[os.listdir(symbol_labels_path(ticker))[idx].split(\".\")[0] for idx,_ in enumerate(os.listdir(symbol_labels_path(ticker)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, features_file in enumerate(os.listdir(specific_date_features_path)):\n",
    "    file_loc= os.path.join(specific_date_features_path,features_file)\n",
    "    pickle.load(open(file_loc, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
